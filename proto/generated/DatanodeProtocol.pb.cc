// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: DatanodeProtocol.proto

#include "DatanodeProtocol.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/stubs/port.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// This is a temporary google only hack
#ifdef GOOGLE_PROTOBUF_ENFORCE_UNIQUENESS
#include "third_party/protobuf/version.h"
#endif
// @@protoc_insertion_point(includes)

namespace protobuf_DatanodeProtocol_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_BalancerBandwidthCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_BlockIdCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_BlockReportContextProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_FinalizeCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_RegisterCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_SlowDiskReportProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_SlowPeerReportProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_VolumeFailureSummaryProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_BlockECReconstructionCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_BlockRecoveryCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_KeyUpdateCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_ReceivedDeletedBlockInfoProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_StorageBlockReportProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<2> scc_info_StorageReceivedDeletedBlocksProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<3> scc_info_DatanodeRegistrationProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<4> scc_info_BlockCommandProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto ::google::protobuf::internal::SCCInfo<8> scc_info_DatanodeCommandProto;
}  // namespace protobuf_DatanodeProtocol_2eproto
namespace protobuf_HdfsServer_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_HdfsServer_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_NNHAStatusHeartbeatProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_HdfsServer_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_StorageInfoProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_HdfsServer_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_ExportedBlockKeysProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_HdfsServer_2eproto ::google::protobuf::internal::SCCInfo<3> scc_info_RecoveringBlockProto;
}  // namespace protobuf_HdfsServer_2eproto
namespace protobuf_erasurecoding_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_erasurecoding_2eproto ::google::protobuf::internal::SCCInfo<5> scc_info_BlockECReconstructionInfoProto;
}  // namespace protobuf_erasurecoding_2eproto
namespace protobuf_hdfs_2eproto {
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_BlockProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_DatanodeIDProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_DatanodeStorageProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_ExtendedBlockProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_RollingUpgradeStatusProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_StorageTypesProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<0> scc_info_StorageUuidsProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_DatanodeInfosProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_StorageReportProto;
extern PROTOBUF_INTERNAL_EXPORT_protobuf_hdfs_2eproto ::google::protobuf::internal::SCCInfo<3> scc_info_LocatedBlockProto;
}  // namespace protobuf_hdfs_2eproto
namespace hadoop {
namespace hdfs {
namespace datanode {
class DatanodeRegistrationProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<DatanodeRegistrationProto>
      _instance;
} _DatanodeRegistrationProto_default_instance_;
class DatanodeCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<DatanodeCommandProto>
      _instance;
} _DatanodeCommandProto_default_instance_;
class BalancerBandwidthCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BalancerBandwidthCommandProto>
      _instance;
} _BalancerBandwidthCommandProto_default_instance_;
class BlockCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockCommandProto>
      _instance;
} _BlockCommandProto_default_instance_;
class BlockIdCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockIdCommandProto>
      _instance;
} _BlockIdCommandProto_default_instance_;
class BlockRecoveryCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockRecoveryCommandProto>
      _instance;
} _BlockRecoveryCommandProto_default_instance_;
class FinalizeCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<FinalizeCommandProto>
      _instance;
} _FinalizeCommandProto_default_instance_;
class KeyUpdateCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<KeyUpdateCommandProto>
      _instance;
} _KeyUpdateCommandProto_default_instance_;
class RegisterCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<RegisterCommandProto>
      _instance;
} _RegisterCommandProto_default_instance_;
class BlockECReconstructionCommandProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockECReconstructionCommandProto>
      _instance;
} _BlockECReconstructionCommandProto_default_instance_;
class RegisterDatanodeRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<RegisterDatanodeRequestProto>
      _instance;
} _RegisterDatanodeRequestProto_default_instance_;
class RegisterDatanodeResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<RegisterDatanodeResponseProto>
      _instance;
} _RegisterDatanodeResponseProto_default_instance_;
class VolumeFailureSummaryProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<VolumeFailureSummaryProto>
      _instance;
} _VolumeFailureSummaryProto_default_instance_;
class HeartbeatRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<HeartbeatRequestProto>
      _instance;
} _HeartbeatRequestProto_default_instance_;
class HeartbeatResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<HeartbeatResponseProto>
      _instance;
} _HeartbeatResponseProto_default_instance_;
class BlockReportRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockReportRequestProto>
      _instance;
} _BlockReportRequestProto_default_instance_;
class BlockReportContextProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockReportContextProto>
      _instance;
} _BlockReportContextProto_default_instance_;
class StorageBlockReportProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<StorageBlockReportProto>
      _instance;
} _StorageBlockReportProto_default_instance_;
class BlockReportResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockReportResponseProto>
      _instance;
} _BlockReportResponseProto_default_instance_;
class CacheReportRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<CacheReportRequestProto>
      _instance;
} _CacheReportRequestProto_default_instance_;
class CacheReportResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<CacheReportResponseProto>
      _instance;
} _CacheReportResponseProto_default_instance_;
class ReceivedDeletedBlockInfoProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<ReceivedDeletedBlockInfoProto>
      _instance;
} _ReceivedDeletedBlockInfoProto_default_instance_;
class StorageReceivedDeletedBlocksProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<StorageReceivedDeletedBlocksProto>
      _instance;
} _StorageReceivedDeletedBlocksProto_default_instance_;
class BlockReceivedAndDeletedRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockReceivedAndDeletedRequestProto>
      _instance;
} _BlockReceivedAndDeletedRequestProto_default_instance_;
class BlockReceivedAndDeletedResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<BlockReceivedAndDeletedResponseProto>
      _instance;
} _BlockReceivedAndDeletedResponseProto_default_instance_;
class ErrorReportRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<ErrorReportRequestProto>
      _instance;
} _ErrorReportRequestProto_default_instance_;
class ErrorReportResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<ErrorReportResponseProto>
      _instance;
} _ErrorReportResponseProto_default_instance_;
class ReportBadBlocksRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<ReportBadBlocksRequestProto>
      _instance;
} _ReportBadBlocksRequestProto_default_instance_;
class ReportBadBlocksResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<ReportBadBlocksResponseProto>
      _instance;
} _ReportBadBlocksResponseProto_default_instance_;
class CommitBlockSynchronizationRequestProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<CommitBlockSynchronizationRequestProto>
      _instance;
} _CommitBlockSynchronizationRequestProto_default_instance_;
class CommitBlockSynchronizationResponseProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<CommitBlockSynchronizationResponseProto>
      _instance;
} _CommitBlockSynchronizationResponseProto_default_instance_;
class SlowPeerReportProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SlowPeerReportProto>
      _instance;
} _SlowPeerReportProto_default_instance_;
class SlowDiskReportProtoDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<SlowDiskReportProto>
      _instance;
} _SlowDiskReportProto_default_instance_;
}  // namespace datanode
}  // namespace hdfs
}  // namespace hadoop
namespace protobuf_DatanodeProtocol_2eproto {
static void InitDefaultsDatanodeRegistrationProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::DatanodeRegistrationProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<3> scc_info_DatanodeRegistrationProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 3, InitDefaultsDatanodeRegistrationProto}, {
      &protobuf_hdfs_2eproto::scc_info_DatanodeIDProto.base,
      &protobuf_HdfsServer_2eproto::scc_info_StorageInfoProto.base,
      &protobuf_HdfsServer_2eproto::scc_info_ExportedBlockKeysProto.base,}};

static void InitDefaultsDatanodeCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::DatanodeCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::DatanodeCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<8> scc_info_DatanodeCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 8, InitDefaultsDatanodeCommandProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_BalancerBandwidthCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockRecoveryCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_FinalizeCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_KeyUpdateCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_RegisterCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockIdCommandProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockECReconstructionCommandProto.base,}};

static void InitDefaultsBalancerBandwidthCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BalancerBandwidthCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_BalancerBandwidthCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsBalancerBandwidthCommandProto}, {}};

static void InitDefaultsBlockCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<4> scc_info_BlockCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 4, InitDefaultsBlockCommandProto}, {
      &protobuf_hdfs_2eproto::scc_info_BlockProto.base,
      &protobuf_hdfs_2eproto::scc_info_DatanodeInfosProto.base,
      &protobuf_hdfs_2eproto::scc_info_StorageUuidsProto.base,
      &protobuf_hdfs_2eproto::scc_info_StorageTypesProto.base,}};

static void InitDefaultsBlockIdCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockIdCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockIdCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockIdCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_BlockIdCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsBlockIdCommandProto}, {}};

static void InitDefaultsBlockRecoveryCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockRecoveryCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockRecoveryCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockRecoveryCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BlockRecoveryCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBlockRecoveryCommandProto}, {
      &protobuf_HdfsServer_2eproto::scc_info_RecoveringBlockProto.base,}};

static void InitDefaultsFinalizeCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_FinalizeCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::FinalizeCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::FinalizeCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_FinalizeCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsFinalizeCommandProto}, {}};

static void InitDefaultsKeyUpdateCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_KeyUpdateCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::KeyUpdateCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::KeyUpdateCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_KeyUpdateCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsKeyUpdateCommandProto}, {
      &protobuf_HdfsServer_2eproto::scc_info_ExportedBlockKeysProto.base,}};

static void InitDefaultsRegisterCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_RegisterCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::RegisterCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::RegisterCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_RegisterCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsRegisterCommandProto}, {}};

static void InitDefaultsBlockECReconstructionCommandProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockECReconstructionCommandProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BlockECReconstructionCommandProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBlockECReconstructionCommandProto}, {
      &protobuf_erasurecoding_2eproto::scc_info_BlockECReconstructionInfoProto.base,}};

static void InitDefaultsRegisterDatanodeRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_RegisterDatanodeRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_RegisterDatanodeRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsRegisterDatanodeRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,}};

static void InitDefaultsRegisterDatanodeResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_RegisterDatanodeResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_RegisterDatanodeResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsRegisterDatanodeResponseProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,}};

static void InitDefaultsVolumeFailureSummaryProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_VolumeFailureSummaryProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::VolumeFailureSummaryProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::VolumeFailureSummaryProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_VolumeFailureSummaryProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsVolumeFailureSummaryProto}, {}};

static void InitDefaultsHeartbeatRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_HeartbeatRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::HeartbeatRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::HeartbeatRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<5> scc_info_HeartbeatRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 5, InitDefaultsHeartbeatRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,
      &protobuf_hdfs_2eproto::scc_info_StorageReportProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_VolumeFailureSummaryProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_SlowPeerReportProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_SlowDiskReportProto.base,}};

static void InitDefaultsHeartbeatResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_HeartbeatResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::HeartbeatResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::HeartbeatResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<3> scc_info_HeartbeatResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 3, InitDefaultsHeartbeatResponseProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeCommandProto.base,
      &protobuf_HdfsServer_2eproto::scc_info_NNHAStatusHeartbeatProto.base,
      &protobuf_hdfs_2eproto::scc_info_RollingUpgradeStatusProto.base,}};

static void InitDefaultsBlockReportRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockReportRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockReportRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockReportRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<3> scc_info_BlockReportRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 3, InitDefaultsBlockReportRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_StorageBlockReportProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportContextProto.base,}};

static void InitDefaultsBlockReportContextProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockReportContextProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockReportContextProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockReportContextProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_BlockReportContextProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsBlockReportContextProto}, {}};

static void InitDefaultsStorageBlockReportProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_StorageBlockReportProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::StorageBlockReportProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::StorageBlockReportProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_StorageBlockReportProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsStorageBlockReportProto}, {
      &protobuf_hdfs_2eproto::scc_info_DatanodeStorageProto.base,}};

static void InitDefaultsBlockReportResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockReportResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockReportResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockReportResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_BlockReportResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsBlockReportResponseProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeCommandProto.base,}};

static void InitDefaultsCacheReportRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_CacheReportRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::CacheReportRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::CacheReportRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_CacheReportRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsCacheReportRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,}};

static void InitDefaultsCacheReportResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_CacheReportResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::CacheReportResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::CacheReportResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_CacheReportResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsCacheReportResponseProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeCommandProto.base,}};

static void InitDefaultsReceivedDeletedBlockInfoProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_ReceivedDeletedBlockInfoProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_ReceivedDeletedBlockInfoProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsReceivedDeletedBlockInfoProto}, {
      &protobuf_hdfs_2eproto::scc_info_BlockProto.base,}};

static void InitDefaultsStorageReceivedDeletedBlocksProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_StorageReceivedDeletedBlocksProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_StorageReceivedDeletedBlocksProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsStorageReceivedDeletedBlocksProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_ReceivedDeletedBlockInfoProto.base,
      &protobuf_hdfs_2eproto::scc_info_DatanodeStorageProto.base,}};

static void InitDefaultsBlockReceivedAndDeletedRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockReceivedAndDeletedRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_BlockReceivedAndDeletedRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsBlockReceivedAndDeletedRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,
      &protobuf_DatanodeProtocol_2eproto::scc_info_StorageReceivedDeletedBlocksProto.base,}};

static void InitDefaultsBlockReceivedAndDeletedResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_BlockReceivedAndDeletedResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_BlockReceivedAndDeletedResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsBlockReceivedAndDeletedResponseProto}, {}};

static void InitDefaultsErrorReportRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_ErrorReportRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::ErrorReportRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::ErrorReportRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_ErrorReportRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsErrorReportRequestProto}, {
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base,}};

static void InitDefaultsErrorReportResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_ErrorReportResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::ErrorReportResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::ErrorReportResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_ErrorReportResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsErrorReportResponseProto}, {}};

static void InitDefaultsReportBadBlocksRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_ReportBadBlocksRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_ReportBadBlocksRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsReportBadBlocksRequestProto}, {
      &protobuf_hdfs_2eproto::scc_info_LocatedBlockProto.base,}};

static void InitDefaultsReportBadBlocksResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_ReportBadBlocksResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_ReportBadBlocksResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsReportBadBlocksResponseProto}, {}};

static void InitDefaultsCommitBlockSynchronizationRequestProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_CommitBlockSynchronizationRequestProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_CommitBlockSynchronizationRequestProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsCommitBlockSynchronizationRequestProto}, {
      &protobuf_hdfs_2eproto::scc_info_ExtendedBlockProto.base,
      &protobuf_hdfs_2eproto::scc_info_DatanodeIDProto.base,}};

static void InitDefaultsCommitBlockSynchronizationResponseProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_CommitBlockSynchronizationResponseProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_CommitBlockSynchronizationResponseProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsCommitBlockSynchronizationResponseProto}, {}};

static void InitDefaultsSlowPeerReportProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_SlowPeerReportProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::SlowPeerReportProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::SlowPeerReportProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_SlowPeerReportProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsSlowPeerReportProto}, {}};

static void InitDefaultsSlowDiskReportProto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::hadoop::hdfs::datanode::_SlowDiskReportProto_default_instance_;
    new (ptr) ::hadoop::hdfs::datanode::SlowDiskReportProto();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::hadoop::hdfs::datanode::SlowDiskReportProto::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<0> scc_info_SlowDiskReportProto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 0, InitDefaultsSlowDiskReportProto}, {}};

void InitDefaults() {
  ::google::protobuf::internal::InitSCC(&scc_info_DatanodeRegistrationProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_DatanodeCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BalancerBandwidthCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockIdCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockRecoveryCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_FinalizeCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_KeyUpdateCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_RegisterCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockECReconstructionCommandProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_RegisterDatanodeRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_RegisterDatanodeResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_VolumeFailureSummaryProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_HeartbeatRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_HeartbeatResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockReportRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockReportContextProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_StorageBlockReportProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockReportResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_CacheReportRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_CacheReportResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_ReceivedDeletedBlockInfoProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_StorageReceivedDeletedBlocksProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockReceivedAndDeletedRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_BlockReceivedAndDeletedResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_ErrorReportRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_ErrorReportResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_ReportBadBlocksRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_ReportBadBlocksResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_CommitBlockSynchronizationRequestProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_CommitBlockSynchronizationResponseProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_SlowPeerReportProto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_SlowDiskReportProto.base);
}

::google::protobuf::Metadata file_level_metadata[33];
const ::google::protobuf::EnumDescriptor* file_level_enum_descriptors[5];

const ::google::protobuf::uint32 TableStruct::offsets[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, datanodeid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, storageinfo_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, keys_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeRegistrationProto, softwareversion_),
  1,
  2,
  3,
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, cmdtype_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, balancercmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, blkcmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, recoverycmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, finalizecmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, keyupdatecmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, registercmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, blkidcmd_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::DatanodeCommandProto, blkecreconstructioncmd_),
  8,
  0,
  1,
  2,
  3,
  4,
  5,
  6,
  7,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto, bandwidth_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, action_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, blockpoolid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, blocks_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, targets_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, targetstorageuuids_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockCommandProto, targetstoragetypes_),
  1,
  0,
  ~0u,
  ~0u,
  ~0u,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockIdCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockIdCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockIdCommandProto, action_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockIdCommandProto, blockpoolid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockIdCommandProto, blockids_),
  1,
  0,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockRecoveryCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockRecoveryCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockRecoveryCommandProto, blocks_),
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::FinalizeCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::FinalizeCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::FinalizeCommandProto, blockpoolid_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::KeyUpdateCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::KeyUpdateCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::KeyUpdateCommandProto, keys_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto, blockecreconstructioninfo_),
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeRequestProto, registration_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::RegisterDatanodeResponseProto, registration_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::VolumeFailureSummaryProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::VolumeFailureSummaryProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::VolumeFailureSummaryProto, failedstoragelocations_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::VolumeFailureSummaryProto, lastvolumefailuredate_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::VolumeFailureSummaryProto, estimatedcapacitylosttotal_),
  ~0u,
  0,
  1,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, registration_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, reports_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, xmitsinprogress_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, xceivercount_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, failedvolumes_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, cachecapacity_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, cacheused_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, volumefailuresummary_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, requestfullblockreportlease_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, slowpeers_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatRequestProto, slowdisks_),
  0,
  ~0u,
  2,
  3,
  6,
  4,
  5,
  1,
  7,
  ~0u,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, cmds_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, hastatus_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, rollingupgradestatus_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, rollingupgradestatusv2_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::HeartbeatResponseProto, fullblockreportleaseid_),
  ~0u,
  0,
  1,
  2,
  3,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, registration_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, blockpoolid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, reports_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportRequestProto, context_),
  1,
  0,
  ~0u,
  2,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, totalrpcs_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, currpc_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, id_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportContextProto, leaseid_),
  0,
  1,
  2,
  3,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, storage_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, blocks_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, numberofblocks_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageBlockReportProto, blocksbuffers_),
  0,
  ~0u,
  1,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReportResponseProto, cmd_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportRequestProto, registration_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportRequestProto, blockpoolid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportRequestProto, blocks_),
  1,
  0,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CacheReportResponseProto, cmd_),
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto, block_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto, status_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto, deletehint_),
  1,
  2,
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto, storageuuid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto, blocks_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto, storage_),
  0,
  ~0u,
  1,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto, registration_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto, blockpoolid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto, blocks_),
  1,
  0,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportRequestProto, registartion_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportRequestProto, errorcode_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportRequestProto, msg_),
  1,
  2,
  0,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ErrorReportResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReportBadBlocksRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReportBadBlocksRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReportBadBlocksRequestProto, blocks_),
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReportBadBlocksResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::ReportBadBlocksResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, block_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, newgenstamp_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, newlength_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, closefile_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, deleteblock_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, newtaragets_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto, newtargetstorages_),
  0,
  1,
  2,
  3,
  4,
  ~0u,
  ~0u,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, datanodeid_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, aggregatelatency_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, median_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, mad_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowPeerReportProto, upperlimitlatency_),
  0,
  1,
  2,
  3,
  4,
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, _has_bits_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, basepath_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, meanmetadataoplatency_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, meanreadiolatency_),
  GOOGLE_PROTOBUF_GENERATED_MESSAGE_FIELD_OFFSET(::hadoop::hdfs::datanode::SlowDiskReportProto, meanwriteiolatency_),
  0,
  1,
  2,
  3,
};
static const ::google::protobuf::internal::MigrationSchema schemas[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
  { 0, 9, sizeof(::hadoop::hdfs::datanode::DatanodeRegistrationProto)},
  { 13, 27, sizeof(::hadoop::hdfs::datanode::DatanodeCommandProto)},
  { 36, 42, sizeof(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto)},
  { 43, 54, sizeof(::hadoop::hdfs::datanode::BlockCommandProto)},
  { 60, 68, sizeof(::hadoop::hdfs::datanode::BlockIdCommandProto)},
  { 71, 77, sizeof(::hadoop::hdfs::datanode::BlockRecoveryCommandProto)},
  { 78, 84, sizeof(::hadoop::hdfs::datanode::FinalizeCommandProto)},
  { 85, 91, sizeof(::hadoop::hdfs::datanode::KeyUpdateCommandProto)},
  { 92, 97, sizeof(::hadoop::hdfs::datanode::RegisterCommandProto)},
  { 97, 103, sizeof(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto)},
  { 104, 110, sizeof(::hadoop::hdfs::datanode::RegisterDatanodeRequestProto)},
  { 111, 117, sizeof(::hadoop::hdfs::datanode::RegisterDatanodeResponseProto)},
  { 118, 126, sizeof(::hadoop::hdfs::datanode::VolumeFailureSummaryProto)},
  { 129, 145, sizeof(::hadoop::hdfs::datanode::HeartbeatRequestProto)},
  { 156, 166, sizeof(::hadoop::hdfs::datanode::HeartbeatResponseProto)},
  { 171, 180, sizeof(::hadoop::hdfs::datanode::BlockReportRequestProto)},
  { 184, 193, sizeof(::hadoop::hdfs::datanode::BlockReportContextProto)},
  { 197, 206, sizeof(::hadoop::hdfs::datanode::StorageBlockReportProto)},
  { 210, 216, sizeof(::hadoop::hdfs::datanode::BlockReportResponseProto)},
  { 217, 225, sizeof(::hadoop::hdfs::datanode::CacheReportRequestProto)},
  { 228, 234, sizeof(::hadoop::hdfs::datanode::CacheReportResponseProto)},
  { 235, 243, sizeof(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto)},
  { 246, 254, sizeof(::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto)},
  { 257, 265, sizeof(::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto)},
  { 268, 273, sizeof(::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto)},
  { 273, 281, sizeof(::hadoop::hdfs::datanode::ErrorReportRequestProto)},
  { 284, 289, sizeof(::hadoop::hdfs::datanode::ErrorReportResponseProto)},
  { 289, 295, sizeof(::hadoop::hdfs::datanode::ReportBadBlocksRequestProto)},
  { 296, 301, sizeof(::hadoop::hdfs::datanode::ReportBadBlocksResponseProto)},
  { 301, 313, sizeof(::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto)},
  { 320, 325, sizeof(::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto)},
  { 325, 335, sizeof(::hadoop::hdfs::datanode::SlowPeerReportProto)},
  { 340, 349, sizeof(::hadoop::hdfs::datanode::SlowDiskReportProto)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BalancerBandwidthCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockIdCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockRecoveryCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_FinalizeCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_KeyUpdateCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_RegisterCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockECReconstructionCommandProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_RegisterDatanodeRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_RegisterDatanodeResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_VolumeFailureSummaryProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_HeartbeatRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_HeartbeatResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockReportRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockReportContextProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_StorageBlockReportProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockReportResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_CacheReportRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_CacheReportResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_ReceivedDeletedBlockInfoProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_StorageReceivedDeletedBlocksProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockReceivedAndDeletedRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_BlockReceivedAndDeletedResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_ErrorReportRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_ErrorReportResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_ReportBadBlocksRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_ReportBadBlocksResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_CommitBlockSynchronizationRequestProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_CommitBlockSynchronizationResponseProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_SlowPeerReportProto_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::hadoop::hdfs::datanode::_SlowDiskReportProto_default_instance_),
};

void protobuf_AssignDescriptors() {
  AddDescriptors();
  AssignDescriptors(
      "DatanodeProtocol.proto", schemas, file_default_instances, TableStruct::offsets,
      file_level_metadata, file_level_enum_descriptors, NULL);
}

void protobuf_AssignDescriptorsOnce() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, protobuf_AssignDescriptors);
}

void protobuf_RegisterTypes(const ::std::string&) GOOGLE_PROTOBUF_ATTRIBUTE_COLD;
void protobuf_RegisterTypes(const ::std::string&) {
  protobuf_AssignDescriptorsOnce();
  ::google::protobuf::internal::RegisterAllTypes(file_level_metadata, 33);
}

void AddDescriptorsImpl() {
  InitDefaults();
  static const char descriptor[] GOOGLE_PROTOBUF_ATTRIBUTE_SECTION_VARIABLE(protodesc_cold) = {
      "\n\026DatanodeProtocol.proto\022\024hadoop.hdfs.da"
      "tanode\032\nhdfs.proto\032\023erasurecoding.proto\032"
      "\020HdfsServer.proto\"\315\001\n\031DatanodeRegistrati"
      "onProto\0220\n\ndatanodeID\030\001 \002(\0132\034.hadoop.hdf"
      "s.DatanodeIDProto\0222\n\013storageInfo\030\002 \002(\0132\035"
      ".hadoop.hdfs.StorageInfoProto\0221\n\004keys\030\003 "
      "\002(\0132#.hadoop.hdfs.ExportedBlockKeysProto"
      "\022\027\n\017softwareVersion\030\004 \002(\t\"\370\006\n\024DatanodeCo"
      "mmandProto\022@\n\007cmdType\030\001 \002(\0162/.hadoop.hdf"
      "s.datanode.DatanodeCommandProto.Type\022H\n\013"
      "balancerCmd\030\002 \001(\01323.hadoop.hdfs.datanode"
      ".BalancerBandwidthCommandProto\0227\n\006blkCmd"
      "\030\003 \001(\0132\'.hadoop.hdfs.datanode.BlockComma"
      "ndProto\022D\n\013recoveryCmd\030\004 \001(\0132/.hadoop.hd"
      "fs.datanode.BlockRecoveryCommandProto\022\?\n"
      "\013finalizeCmd\030\005 \001(\0132*.hadoop.hdfs.datanod"
      "e.FinalizeCommandProto\022A\n\014keyUpdateCmd\030\006"
      " \001(\0132+.hadoop.hdfs.datanode.KeyUpdateCom"
      "mandProto\022\?\n\013registerCmd\030\007 \001(\0132*.hadoop."
      "hdfs.datanode.RegisterCommandProto\022;\n\010bl"
      "kIdCmd\030\010 \001(\0132).hadoop.hdfs.datanode.Bloc"
      "kIdCommandProto\022W\n\026blkECReconstructionCm"
      "d\030\t \001(\01327.hadoop.hdfs.datanode.BlockECRe"
      "constructionCommandProto\"\371\001\n\004Type\022\034\n\030Bal"
      "ancerBandwidthCommand\020\000\022\020\n\014BlockCommand\020"
      "\001\022\030\n\024BlockRecoveryCommand\020\002\022\023\n\017FinalizeC"
      "ommand\020\003\022\024\n\020KeyUpdateCommand\020\004\022\023\n\017Regist"
      "erCommand\020\005\022\030\n\024UnusedUpgradeCommand\020\006\022\027\n"
      "\023NullDatanodeCommand\020\007\022\022\n\016BlockIdCommand"
      "\020\010\022 \n\034BlockECReconstructionCommand\020\t\"2\n\035"
      "BalancerBandwidthCommandProto\022\021\n\tbandwid"
      "th\030\001 \002(\004\"\361\002\n\021BlockCommandProto\022>\n\006action"
      "\030\001 \002(\0162..hadoop.hdfs.datanode.BlockComma"
      "ndProto.Action\022\023\n\013blockPoolId\030\002 \002(\t\022\'\n\006b"
      "locks\030\003 \003(\0132\027.hadoop.hdfs.BlockProto\0220\n\007"
      "targets\030\004 \003(\0132\037.hadoop.hdfs.DatanodeInfo"
      "sProto\022:\n\022targetStorageUuids\030\005 \003(\0132\036.had"
      "oop.hdfs.StorageUuidsProto\022:\n\022targetStor"
      "ageTypes\030\006 \003(\0132\036.hadoop.hdfs.StorageType"
      "sProto\"4\n\006Action\022\014\n\010TRANSFER\020\001\022\016\n\nINVALI"
      "DATE\020\002\022\014\n\010SHUTDOWN\020\003\"\244\001\n\023BlockIdCommandP"
      "roto\022@\n\006action\030\001 \002(\01620.hadoop.hdfs.datan"
      "ode.BlockIdCommandProto.Action\022\023\n\013blockP"
      "oolId\030\002 \002(\t\022\024\n\010blockIds\030\003 \003(\004B\002\020\001\" \n\006Act"
      "ion\022\t\n\005CACHE\020\001\022\013\n\007UNCACHE\020\002\"N\n\031BlockReco"
      "veryCommandProto\0221\n\006blocks\030\001 \003(\0132!.hadoo"
      "p.hdfs.RecoveringBlockProto\"+\n\024FinalizeC"
      "ommandProto\022\023\n\013blockPoolId\030\001 \002(\t\"J\n\025KeyU"
      "pdateCommandProto\0221\n\004keys\030\001 \002(\0132#.hadoop"
      ".hdfs.ExportedBlockKeysProto\"\026\n\024Register"
      "CommandProto\"s\n!BlockECReconstructionCom"
      "mandProto\022N\n\031blockECReconstructioninfo\030\001"
      " \003(\0132+.hadoop.hdfs.BlockECReconstruction"
      "InfoProto\"e\n\034RegisterDatanodeRequestProt"
      "o\022E\n\014registration\030\001 \002(\0132/.hadoop.hdfs.da"
      "tanode.DatanodeRegistrationProto\"f\n\035Regi"
      "sterDatanodeResponseProto\022E\n\014registratio"
      "n\030\001 \002(\0132/.hadoop.hdfs.datanode.DatanodeR"
      "egistrationProto\"~\n\031VolumeFailureSummary"
      "Proto\022\036\n\026failedStorageLocations\030\001 \003(\t\022\035\n"
      "\025lastVolumeFailureDate\030\002 \002(\004\022\"\n\032estimate"
      "dCapacityLostTotal\030\003 \002(\004\"\206\004\n\025HeartbeatRe"
      "questProto\022E\n\014registration\030\001 \002(\0132/.hadoo"
      "p.hdfs.datanode.DatanodeRegistrationProt"
      "o\0220\n\007reports\030\002 \003(\0132\037.hadoop.hdfs.Storage"
      "ReportProto\022\032\n\017xmitsInProgress\030\003 \001(\r:\0010\022"
      "\027\n\014xceiverCount\030\004 \001(\r:\0010\022\030\n\rfailedVolume"
      "s\030\005 \001(\r:\0010\022\030\n\rcacheCapacity\030\006 \001(\004:\0010\022\024\n\t"
      "cacheUsed\030\007 \001(\004:\0010\022M\n\024volumeFailureSumma"
      "ry\030\010 \001(\0132/.hadoop.hdfs.datanode.VolumeFa"
      "ilureSummaryProto\022*\n\033requestFullBlockRep"
      "ortLease\030\t \001(\010:\005false\022<\n\tslowPeers\030\n \003(\013"
      "2).hadoop.hdfs.datanode.SlowPeerReportPr"
      "oto\022<\n\tslowDisks\030\013 \003(\0132).hadoop.hdfs.dat"
      "anode.SlowDiskReportProto\"\274\002\n\026HeartbeatR"
      "esponseProto\0228\n\004cmds\030\001 \003(\0132*.hadoop.hdfs"
      ".datanode.DatanodeCommandProto\0227\n\010haStat"
      "us\030\002 \002(\0132%.hadoop.hdfs.NNHAStatusHeartbe"
      "atProto\022D\n\024rollingUpgradeStatus\030\003 \001(\0132&."
      "hadoop.hdfs.RollingUpgradeStatusProto\022F\n"
      "\026rollingUpgradeStatusV2\030\004 \001(\0132&.hadoop.h"
      "dfs.RollingUpgradeStatusProto\022!\n\026fullBlo"
      "ckReportLeaseId\030\005 \001(\004:\0010\"\365\001\n\027BlockReport"
      "RequestProto\022E\n\014registration\030\001 \002(\0132/.had"
      "oop.hdfs.datanode.DatanodeRegistrationPr"
      "oto\022\023\n\013blockPoolId\030\002 \002(\t\022>\n\007reports\030\003 \003("
      "\0132-.hadoop.hdfs.datanode.StorageBlockRep"
      "ortProto\022>\n\007context\030\004 \001(\0132-.hadoop.hdfs."
      "datanode.BlockReportContextProto\"\\\n\027Bloc"
      "kReportContextProto\022\021\n\ttotalRpcs\030\001 \002(\005\022\016"
      "\n\006curRpc\030\002 \002(\005\022\n\n\002id\030\003 \002(\003\022\022\n\007leaseId\030\004 "
      "\001(\004:\0010\"\220\001\n\027StorageBlockReportProto\0222\n\007st"
      "orage\030\001 \002(\0132!.hadoop.hdfs.DatanodeStorag"
      "eProto\022\022\n\006blocks\030\002 \003(\004B\002\020\001\022\026\n\016numberOfBl"
      "ocks\030\003 \001(\004\022\025\n\rblocksBuffers\030\004 \003(\014\"S\n\030Blo"
      "ckReportResponseProto\0227\n\003cmd\030\001 \001(\0132*.had"
      "oop.hdfs.datanode.DatanodeCommandProto\"\211"
      "\001\n\027CacheReportRequestProto\022E\n\014registrati"
      "on\030\001 \002(\0132/.hadoop.hdfs.datanode.Datanode"
      "RegistrationProto\022\023\n\013blockPoolId\030\002 \002(\t\022\022"
      "\n\006blocks\030\003 \003(\004B\002\020\001\"S\n\030CacheReportRespons"
      "eProto\0227\n\003cmd\030\001 \001(\0132*.hadoop.hdfs.datano"
      "de.DatanodeCommandProto\"\345\001\n\035ReceivedDele"
      "tedBlockInfoProto\022&\n\005block\030\001 \002(\0132\027.hadoo"
      "p.hdfs.BlockProto\022O\n\006status\030\003 \002(\0162\?.hado"
      "op.hdfs.datanode.ReceivedDeletedBlockInf"
      "oProto.BlockStatus\022\022\n\ndeleteHint\030\002 \001(\t\"7"
      "\n\013BlockStatus\022\r\n\tRECEIVING\020\001\022\014\n\010RECEIVED"
      "\020\002\022\013\n\007DELETED\020\003\"\265\001\n!StorageReceivedDelet"
      "edBlocksProto\022\027\n\013storageUuid\030\001 \002(\tB\002\030\001\022C"
      "\n\006blocks\030\002 \003(\01323.hadoop.hdfs.datanode.Re"
      "ceivedDeletedBlockInfoProto\0222\n\007storage\030\003"
      " \001(\0132!.hadoop.hdfs.DatanodeStorageProto\""
      "\312\001\n#BlockReceivedAndDeletedRequestProto\022"
      "E\n\014registration\030\001 \002(\0132/.hadoop.hdfs.data"
      "node.DatanodeRegistrationProto\022\023\n\013blockP"
      "oolId\030\002 \002(\t\022G\n\006blocks\030\003 \003(\01327.hadoop.hdf"
      "s.datanode.StorageReceivedDeletedBlocksP"
      "roto\"&\n$BlockReceivedAndDeletedResponseP"
      "roto\"\322\001\n\027ErrorReportRequestProto\022E\n\014regi"
      "startion\030\001 \002(\0132/.hadoop.hdfs.datanode.Da"
      "tanodeRegistrationProto\022\021\n\terrorCode\030\002 \002"
      "(\r\022\013\n\003msg\030\003 \002(\t\"P\n\tErrorCode\022\n\n\006NOTIFY\020\000"
      "\022\016\n\nDISK_ERROR\020\001\022\021\n\rINVALID_BLOCK\020\002\022\024\n\020F"
      "ATAL_DISK_ERROR\020\003\"\032\n\030ErrorReportResponse"
      "Proto\"M\n\033ReportBadBlocksRequestProto\022.\n\006"
      "blocks\030\001 \003(\0132\036.hadoop.hdfs.LocatedBlockP"
      "roto\"\036\n\034ReportBadBlocksResponseProto\"\366\001\n"
      "&CommitBlockSynchronizationRequestProto\022"
      ".\n\005block\030\001 \002(\0132\037.hadoop.hdfs.ExtendedBlo"
      "ckProto\022\023\n\013newGenStamp\030\002 \002(\004\022\021\n\tnewLengt"
      "h\030\003 \002(\004\022\021\n\tcloseFile\030\004 \002(\010\022\023\n\013deleteBloc"
      "k\030\005 \002(\010\0221\n\013newTaragets\030\006 \003(\0132\034.hadoop.hd"
      "fs.DatanodeIDProto\022\031\n\021newTargetStorages\030"
      "\007 \003(\t\")\n\'CommitBlockSynchronizationRespo"
      "nseProto\"{\n\023SlowPeerReportProto\022\022\n\ndataN"
      "odeId\030\001 \001(\t\022\030\n\020aggregateLatency\030\002 \001(\001\022\016\n"
      "\006median\030\003 \001(\001\022\013\n\003mad\030\004 \001(\001\022\031\n\021upperLimit"
      "Latency\030\005 \001(\001\"}\n\023SlowDiskReportProto\022\020\n\010"
      "basePath\030\001 \001(\t\022\035\n\025meanMetadataOpLatency\030"
      "\002 \001(\001\022\031\n\021meanReadIoLatency\030\003 \001(\001\022\032\n\022mean"
      "WriteIoLatency\030\004 \001(\0012\314\010\n\027DatanodeProtoco"
      "lService\022{\n\020registerDatanode\0222.hadoop.hd"
      "fs.datanode.RegisterDatanodeRequestProto"
      "\0323.hadoop.hdfs.datanode.RegisterDatanode"
      "ResponseProto\022j\n\rsendHeartbeat\022+.hadoop."
      "hdfs.datanode.HeartbeatRequestProto\032,.ha"
      "doop.hdfs.datanode.HeartbeatResponseProt"
      "o\022l\n\013blockReport\022-.hadoop.hdfs.datanode."
      "BlockReportRequestProto\032..hadoop.hdfs.da"
      "tanode.BlockReportResponseProto\022l\n\013cache"
      "Report\022-.hadoop.hdfs.datanode.CacheRepor"
      "tRequestProto\032..hadoop.hdfs.datanode.Cac"
      "heReportResponseProto\022\220\001\n\027blockReceivedA"
      "ndDeleted\0229.hadoop.hdfs.datanode.BlockRe"
      "ceivedAndDeletedRequestProto\032:.hadoop.hd"
      "fs.datanode.BlockReceivedAndDeletedRespo"
      "nseProto\022l\n\013errorReport\022-.hadoop.hdfs.da"
      "tanode.ErrorReportRequestProto\032..hadoop."
      "hdfs.datanode.ErrorReportResponseProto\022U"
      "\n\016versionRequest\022 .hadoop.hdfs.VersionRe"
      "questProto\032!.hadoop.hdfs.VersionResponse"
      "Proto\022x\n\017reportBadBlocks\0221.hadoop.hdfs.d"
      "atanode.ReportBadBlocksRequestProto\0322.ha"
      "doop.hdfs.datanode.ReportBadBlocksRespon"
      "seProto\022\231\001\n\032commitBlockSynchronization\022<"
      ".hadoop.hdfs.datanode.CommitBlockSynchro"
      "nizationRequestProto\032=.hadoop.hdfs.datan"
      "ode.CommitBlockSynchronizationResponsePr"
      "otoBE\n%org.apache.hadoop.hdfs.protocol.p"
      "rotoB\026DatanodeProtocolProtos\210\001\001\240\001\001"
  };
  ::google::protobuf::DescriptorPool::InternalAddGeneratedFile(
      descriptor, 6834);
  ::google::protobuf::MessageFactory::InternalRegisterGeneratedFile(
    "DatanodeProtocol.proto", &protobuf_RegisterTypes);
  ::protobuf_hdfs_2eproto::AddDescriptors();
  ::protobuf_erasurecoding_2eproto::AddDescriptors();
  ::protobuf_HdfsServer_2eproto::AddDescriptors();
}

void AddDescriptors() {
  static ::google::protobuf::internal::once_flag once;
  ::google::protobuf::internal::call_once(once, AddDescriptorsImpl);
}
// Force AddDescriptors() to be called at dynamic initialization time.
struct StaticDescriptorInitializer {
  StaticDescriptorInitializer() {
    AddDescriptors();
  }
} static_descriptor_initializer;
}  // namespace protobuf_DatanodeProtocol_2eproto
namespace hadoop {
namespace hdfs {
namespace datanode {
const ::google::protobuf::EnumDescriptor* DatanodeCommandProto_Type_descriptor() {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return protobuf_DatanodeProtocol_2eproto::file_level_enum_descriptors[0];
}
bool DatanodeCommandProto_Type_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
    case 4:
    case 5:
    case 6:
    case 7:
    case 8:
    case 9:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const DatanodeCommandProto_Type DatanodeCommandProto::BalancerBandwidthCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::BlockCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::BlockRecoveryCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::FinalizeCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::KeyUpdateCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::RegisterCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::UnusedUpgradeCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::NullDatanodeCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::BlockIdCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::BlockECReconstructionCommand;
const DatanodeCommandProto_Type DatanodeCommandProto::Type_MIN;
const DatanodeCommandProto_Type DatanodeCommandProto::Type_MAX;
const int DatanodeCommandProto::Type_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
const ::google::protobuf::EnumDescriptor* BlockCommandProto_Action_descriptor() {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return protobuf_DatanodeProtocol_2eproto::file_level_enum_descriptors[1];
}
bool BlockCommandProto_Action_IsValid(int value) {
  switch (value) {
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const BlockCommandProto_Action BlockCommandProto::TRANSFER;
const BlockCommandProto_Action BlockCommandProto::INVALIDATE;
const BlockCommandProto_Action BlockCommandProto::SHUTDOWN;
const BlockCommandProto_Action BlockCommandProto::Action_MIN;
const BlockCommandProto_Action BlockCommandProto::Action_MAX;
const int BlockCommandProto::Action_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
const ::google::protobuf::EnumDescriptor* BlockIdCommandProto_Action_descriptor() {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return protobuf_DatanodeProtocol_2eproto::file_level_enum_descriptors[2];
}
bool BlockIdCommandProto_Action_IsValid(int value) {
  switch (value) {
    case 1:
    case 2:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const BlockIdCommandProto_Action BlockIdCommandProto::CACHE;
const BlockIdCommandProto_Action BlockIdCommandProto::UNCACHE;
const BlockIdCommandProto_Action BlockIdCommandProto::Action_MIN;
const BlockIdCommandProto_Action BlockIdCommandProto::Action_MAX;
const int BlockIdCommandProto::Action_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
const ::google::protobuf::EnumDescriptor* ReceivedDeletedBlockInfoProto_BlockStatus_descriptor() {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return protobuf_DatanodeProtocol_2eproto::file_level_enum_descriptors[3];
}
bool ReceivedDeletedBlockInfoProto_BlockStatus_IsValid(int value) {
  switch (value) {
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::RECEIVING;
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::RECEIVED;
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::DELETED;
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::BlockStatus_MIN;
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::BlockStatus_MAX;
const int ReceivedDeletedBlockInfoProto::BlockStatus_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900
const ::google::protobuf::EnumDescriptor* ErrorReportRequestProto_ErrorCode_descriptor() {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return protobuf_DatanodeProtocol_2eproto::file_level_enum_descriptors[4];
}
bool ErrorReportRequestProto_ErrorCode_IsValid(int value) {
  switch (value) {
    case 0:
    case 1:
    case 2:
    case 3:
      return true;
    default:
      return false;
  }
}

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::NOTIFY;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::DISK_ERROR;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::INVALID_BLOCK;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::FATAL_DISK_ERROR;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::ErrorCode_MIN;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto::ErrorCode_MAX;
const int ErrorReportRequestProto::ErrorCode_ARRAYSIZE;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

// ===================================================================

void DatanodeRegistrationProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_._instance.get_mutable()->datanodeid_ = const_cast< ::hadoop::hdfs::DatanodeIDProto*>(
      ::hadoop::hdfs::DatanodeIDProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_._instance.get_mutable()->storageinfo_ = const_cast< ::hadoop::hdfs::StorageInfoProto*>(
      ::hadoop::hdfs::StorageInfoProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_._instance.get_mutable()->keys_ = const_cast< ::hadoop::hdfs::ExportedBlockKeysProto*>(
      ::hadoop::hdfs::ExportedBlockKeysProto::internal_default_instance());
}
void DatanodeRegistrationProto::clear_datanodeid() {
  if (datanodeid_ != NULL) datanodeid_->Clear();
  clear_has_datanodeid();
}
void DatanodeRegistrationProto::clear_storageinfo() {
  if (storageinfo_ != NULL) storageinfo_->Clear();
  clear_has_storageinfo();
}
void DatanodeRegistrationProto::clear_keys() {
  if (keys_ != NULL) keys_->Clear();
  clear_has_keys();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DatanodeRegistrationProto::kDatanodeIDFieldNumber;
const int DatanodeRegistrationProto::kStorageInfoFieldNumber;
const int DatanodeRegistrationProto::kKeysFieldNumber;
const int DatanodeRegistrationProto::kSoftwareVersionFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DatanodeRegistrationProto::DatanodeRegistrationProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.DatanodeRegistrationProto)
}
DatanodeRegistrationProto::DatanodeRegistrationProto(const DatanodeRegistrationProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  softwareversion_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_softwareversion()) {
    softwareversion_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.softwareversion_);
  }
  if (from.has_datanodeid()) {
    datanodeid_ = new ::hadoop::hdfs::DatanodeIDProto(*from.datanodeid_);
  } else {
    datanodeid_ = NULL;
  }
  if (from.has_storageinfo()) {
    storageinfo_ = new ::hadoop::hdfs::StorageInfoProto(*from.storageinfo_);
  } else {
    storageinfo_ = NULL;
  }
  if (from.has_keys()) {
    keys_ = new ::hadoop::hdfs::ExportedBlockKeysProto(*from.keys_);
  } else {
    keys_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.DatanodeRegistrationProto)
}

void DatanodeRegistrationProto::SharedCtor() {
  softwareversion_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&datanodeid_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&keys_) -
      reinterpret_cast<char*>(&datanodeid_)) + sizeof(keys_));
}

DatanodeRegistrationProto::~DatanodeRegistrationProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  SharedDtor();
}

void DatanodeRegistrationProto::SharedDtor() {
  softwareversion_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete datanodeid_;
  if (this != internal_default_instance()) delete storageinfo_;
  if (this != internal_default_instance()) delete keys_;
}

void DatanodeRegistrationProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* DatanodeRegistrationProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const DatanodeRegistrationProto& DatanodeRegistrationProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeRegistrationProto.base);
  return *internal_default_instance();
}


void DatanodeRegistrationProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      softwareversion_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(datanodeid_ != NULL);
      datanodeid_->Clear();
    }
    if (cached_has_bits & 0x00000004u) {
      GOOGLE_DCHECK(storageinfo_ != NULL);
      storageinfo_->Clear();
    }
    if (cached_has_bits & 0x00000008u) {
      GOOGLE_DCHECK(keys_ != NULL);
      keys_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool DatanodeRegistrationProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_datanodeid()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_storageinfo()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_keys()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string softwareVersion = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_softwareversion()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->softwareversion().data(), static_cast<int>(this->softwareversion().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion");
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  return false;
#undef DO_
}

void DatanodeRegistrationProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_datanodeid(), output);
  }

  // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_storageinfo(), output);
  }

  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, this->_internal_keys(), output);
  }

  // required string softwareVersion = 4;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->softwareversion().data(), static_cast<int>(this->softwareversion().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      4, this->softwareversion(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.DatanodeRegistrationProto)
}

::google::protobuf::uint8* DatanodeRegistrationProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_datanodeid(), deterministic, target);
  }

  // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_storageinfo(), deterministic, target);
  }

  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->_internal_keys(), deterministic, target);
  }

  // required string softwareVersion = 4;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->softwareversion().data(), static_cast<int>(this->softwareversion().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        4, this->softwareversion(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  return target;
}

size_t DatanodeRegistrationProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  size_t total_size = 0;

  if (has_softwareversion()) {
    // required string softwareVersion = 4;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->softwareversion());
  }

  if (has_datanodeid()) {
    // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *datanodeid_);
  }

  if (has_storageinfo()) {
    // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *storageinfo_);
  }

  if (has_keys()) {
    // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *keys_);
  }

  return total_size;
}
size_t DatanodeRegistrationProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x0000000f) ^ 0x0000000f) == 0) {  // All required fields are present.
    // required string softwareVersion = 4;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->softwareversion());

    // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *datanodeid_);

    // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *storageinfo_);

    // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *keys_);

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void DatanodeRegistrationProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  GOOGLE_DCHECK_NE(&from, this);
  const DatanodeRegistrationProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const DatanodeRegistrationProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.DatanodeRegistrationProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.DatanodeRegistrationProto)
    MergeFrom(*source);
  }
}

void DatanodeRegistrationProto::MergeFrom(const DatanodeRegistrationProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_softwareversion();
      softwareversion_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.softwareversion_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_datanodeid()->::hadoop::hdfs::DatanodeIDProto::MergeFrom(from.datanodeid());
    }
    if (cached_has_bits & 0x00000004u) {
      mutable_storageinfo()->::hadoop::hdfs::StorageInfoProto::MergeFrom(from.storageinfo());
    }
    if (cached_has_bits & 0x00000008u) {
      mutable_keys()->::hadoop::hdfs::ExportedBlockKeysProto::MergeFrom(from.keys());
    }
  }
}

void DatanodeRegistrationProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void DatanodeRegistrationProto::CopyFrom(const DatanodeRegistrationProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.DatanodeRegistrationProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DatanodeRegistrationProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x0000000f) != 0x0000000f) return false;
  if (has_datanodeid()) {
    if (!this->datanodeid_->IsInitialized()) return false;
  }
  if (has_storageinfo()) {
    if (!this->storageinfo_->IsInitialized()) return false;
  }
  if (has_keys()) {
    if (!this->keys_->IsInitialized()) return false;
  }
  return true;
}

void DatanodeRegistrationProto::Swap(DatanodeRegistrationProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DatanodeRegistrationProto::InternalSwap(DatanodeRegistrationProto* other) {
  using std::swap;
  softwareversion_.Swap(&other->softwareversion_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(datanodeid_, other->datanodeid_);
  swap(storageinfo_, other->storageinfo_);
  swap(keys_, other->keys_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata DatanodeRegistrationProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void DatanodeCommandProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->balancercmd_ = const_cast< ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto*>(
      ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->blkcmd_ = const_cast< ::hadoop::hdfs::datanode::BlockCommandProto*>(
      ::hadoop::hdfs::datanode::BlockCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->recoverycmd_ = const_cast< ::hadoop::hdfs::datanode::BlockRecoveryCommandProto*>(
      ::hadoop::hdfs::datanode::BlockRecoveryCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->finalizecmd_ = const_cast< ::hadoop::hdfs::datanode::FinalizeCommandProto*>(
      ::hadoop::hdfs::datanode::FinalizeCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->keyupdatecmd_ = const_cast< ::hadoop::hdfs::datanode::KeyUpdateCommandProto*>(
      ::hadoop::hdfs::datanode::KeyUpdateCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->registercmd_ = const_cast< ::hadoop::hdfs::datanode::RegisterCommandProto*>(
      ::hadoop::hdfs::datanode::RegisterCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->blkidcmd_ = const_cast< ::hadoop::hdfs::datanode::BlockIdCommandProto*>(
      ::hadoop::hdfs::datanode::BlockIdCommandProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_._instance.get_mutable()->blkecreconstructioncmd_ = const_cast< ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto*>(
      ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int DatanodeCommandProto::kCmdTypeFieldNumber;
const int DatanodeCommandProto::kBalancerCmdFieldNumber;
const int DatanodeCommandProto::kBlkCmdFieldNumber;
const int DatanodeCommandProto::kRecoveryCmdFieldNumber;
const int DatanodeCommandProto::kFinalizeCmdFieldNumber;
const int DatanodeCommandProto::kKeyUpdateCmdFieldNumber;
const int DatanodeCommandProto::kRegisterCmdFieldNumber;
const int DatanodeCommandProto::kBlkIdCmdFieldNumber;
const int DatanodeCommandProto::kBlkECReconstructionCmdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

DatanodeCommandProto::DatanodeCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.DatanodeCommandProto)
}
DatanodeCommandProto::DatanodeCommandProto(const DatanodeCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_balancercmd()) {
    balancercmd_ = new ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto(*from.balancercmd_);
  } else {
    balancercmd_ = NULL;
  }
  if (from.has_blkcmd()) {
    blkcmd_ = new ::hadoop::hdfs::datanode::BlockCommandProto(*from.blkcmd_);
  } else {
    blkcmd_ = NULL;
  }
  if (from.has_recoverycmd()) {
    recoverycmd_ = new ::hadoop::hdfs::datanode::BlockRecoveryCommandProto(*from.recoverycmd_);
  } else {
    recoverycmd_ = NULL;
  }
  if (from.has_finalizecmd()) {
    finalizecmd_ = new ::hadoop::hdfs::datanode::FinalizeCommandProto(*from.finalizecmd_);
  } else {
    finalizecmd_ = NULL;
  }
  if (from.has_keyupdatecmd()) {
    keyupdatecmd_ = new ::hadoop::hdfs::datanode::KeyUpdateCommandProto(*from.keyupdatecmd_);
  } else {
    keyupdatecmd_ = NULL;
  }
  if (from.has_registercmd()) {
    registercmd_ = new ::hadoop::hdfs::datanode::RegisterCommandProto(*from.registercmd_);
  } else {
    registercmd_ = NULL;
  }
  if (from.has_blkidcmd()) {
    blkidcmd_ = new ::hadoop::hdfs::datanode::BlockIdCommandProto(*from.blkidcmd_);
  } else {
    blkidcmd_ = NULL;
  }
  if (from.has_blkecreconstructioncmd()) {
    blkecreconstructioncmd_ = new ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto(*from.blkecreconstructioncmd_);
  } else {
    blkecreconstructioncmd_ = NULL;
  }
  cmdtype_ = from.cmdtype_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.DatanodeCommandProto)
}

void DatanodeCommandProto::SharedCtor() {
  ::memset(&balancercmd_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&cmdtype_) -
      reinterpret_cast<char*>(&balancercmd_)) + sizeof(cmdtype_));
}

DatanodeCommandProto::~DatanodeCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.DatanodeCommandProto)
  SharedDtor();
}

void DatanodeCommandProto::SharedDtor() {
  if (this != internal_default_instance()) delete balancercmd_;
  if (this != internal_default_instance()) delete blkcmd_;
  if (this != internal_default_instance()) delete recoverycmd_;
  if (this != internal_default_instance()) delete finalizecmd_;
  if (this != internal_default_instance()) delete keyupdatecmd_;
  if (this != internal_default_instance()) delete registercmd_;
  if (this != internal_default_instance()) delete blkidcmd_;
  if (this != internal_default_instance()) delete blkecreconstructioncmd_;
}

void DatanodeCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* DatanodeCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const DatanodeCommandProto& DatanodeCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_DatanodeCommandProto.base);
  return *internal_default_instance();
}


void DatanodeCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 255u) {
    if (cached_has_bits & 0x00000001u) {
      GOOGLE_DCHECK(balancercmd_ != NULL);
      balancercmd_->Clear();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(blkcmd_ != NULL);
      blkcmd_->Clear();
    }
    if (cached_has_bits & 0x00000004u) {
      GOOGLE_DCHECK(recoverycmd_ != NULL);
      recoverycmd_->Clear();
    }
    if (cached_has_bits & 0x00000008u) {
      GOOGLE_DCHECK(finalizecmd_ != NULL);
      finalizecmd_->Clear();
    }
    if (cached_has_bits & 0x00000010u) {
      GOOGLE_DCHECK(keyupdatecmd_ != NULL);
      keyupdatecmd_->Clear();
    }
    if (cached_has_bits & 0x00000020u) {
      GOOGLE_DCHECK(registercmd_ != NULL);
      registercmd_->Clear();
    }
    if (cached_has_bits & 0x00000040u) {
      GOOGLE_DCHECK(blkidcmd_ != NULL);
      blkidcmd_->Clear();
    }
    if (cached_has_bits & 0x00000080u) {
      GOOGLE_DCHECK(blkecreconstructioncmd_ != NULL);
      blkecreconstructioncmd_->Clear();
    }
  }
  cmdtype_ = 0;
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool DatanodeCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u /* 8 & 0xFF */)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          if (::hadoop::hdfs::datanode::DatanodeCommandProto_Type_IsValid(value)) {
            set_cmdtype(static_cast< ::hadoop::hdfs::datanode::DatanodeCommandProto_Type >(value));
          } else {
            mutable_unknown_fields()->AddVarint(
                1, static_cast< ::google::protobuf::uint64>(value));
          }
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_balancercmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_blkcmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_recoverycmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u /* 42 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_finalizecmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(50u /* 50 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_keyupdatecmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(58u /* 58 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registercmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
      case 8: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(66u /* 66 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_blkidcmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
      case 9: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(74u /* 74 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_blkecreconstructioncmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.DatanodeCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.DatanodeCommandProto)
  return false;
#undef DO_
}

void DatanodeCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
  if (cached_has_bits & 0x00000100u) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->cmdtype(), output);
  }

  // optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_balancercmd(), output);
  }

  // optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, this->_internal_blkcmd(), output);
  }

  // optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4, this->_internal_recoverycmd(), output);
  }

  // optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      5, this->_internal_finalizecmd(), output);
  }

  // optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
  if (cached_has_bits & 0x00000010u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      6, this->_internal_keyupdatecmd(), output);
  }

  // optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
  if (cached_has_bits & 0x00000020u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      7, this->_internal_registercmd(), output);
  }

  // optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
  if (cached_has_bits & 0x00000040u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      8, this->_internal_blkidcmd(), output);
  }

  // optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
  if (cached_has_bits & 0x00000080u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      9, this->_internal_blkecreconstructioncmd(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.DatanodeCommandProto)
}

::google::protobuf::uint8* DatanodeCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
  if (cached_has_bits & 0x00000100u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteEnumToArray(
      1, this->cmdtype(), target);
  }

  // optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_balancercmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->_internal_blkcmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        4, this->_internal_recoverycmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        5, this->_internal_finalizecmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
  if (cached_has_bits & 0x00000010u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        6, this->_internal_keyupdatecmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
  if (cached_has_bits & 0x00000020u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        7, this->_internal_registercmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
  if (cached_has_bits & 0x00000040u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        8, this->_internal_blkidcmd(), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
  if (cached_has_bits & 0x00000080u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        9, this->_internal_blkecreconstructioncmd(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.DatanodeCommandProto)
  return target;
}

size_t DatanodeCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
  if (has_cmdtype()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->cmdtype());
  }
  if (_has_bits_[0 / 32] & 255u) {
    // optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
    if (has_balancercmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *balancercmd_);
    }

    // optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
    if (has_blkcmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *blkcmd_);
    }

    // optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
    if (has_recoverycmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *recoverycmd_);
    }

    // optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
    if (has_finalizecmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *finalizecmd_);
    }

    // optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
    if (has_keyupdatecmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *keyupdatecmd_);
    }

    // optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
    if (has_registercmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *registercmd_);
    }

    // optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
    if (has_blkidcmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *blkidcmd_);
    }

    // optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
    if (has_blkecreconstructioncmd()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *blkecreconstructioncmd_);
    }

  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void DatanodeCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const DatanodeCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const DatanodeCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.DatanodeCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.DatanodeCommandProto)
    MergeFrom(*source);
  }
}

void DatanodeCommandProto::MergeFrom(const DatanodeCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 255u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_balancercmd()->::hadoop::hdfs::datanode::BalancerBandwidthCommandProto::MergeFrom(from.balancercmd());
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_blkcmd()->::hadoop::hdfs::datanode::BlockCommandProto::MergeFrom(from.blkcmd());
    }
    if (cached_has_bits & 0x00000004u) {
      mutable_recoverycmd()->::hadoop::hdfs::datanode::BlockRecoveryCommandProto::MergeFrom(from.recoverycmd());
    }
    if (cached_has_bits & 0x00000008u) {
      mutable_finalizecmd()->::hadoop::hdfs::datanode::FinalizeCommandProto::MergeFrom(from.finalizecmd());
    }
    if (cached_has_bits & 0x00000010u) {
      mutable_keyupdatecmd()->::hadoop::hdfs::datanode::KeyUpdateCommandProto::MergeFrom(from.keyupdatecmd());
    }
    if (cached_has_bits & 0x00000020u) {
      mutable_registercmd()->::hadoop::hdfs::datanode::RegisterCommandProto::MergeFrom(from.registercmd());
    }
    if (cached_has_bits & 0x00000040u) {
      mutable_blkidcmd()->::hadoop::hdfs::datanode::BlockIdCommandProto::MergeFrom(from.blkidcmd());
    }
    if (cached_has_bits & 0x00000080u) {
      mutable_blkecreconstructioncmd()->::hadoop::hdfs::datanode::BlockECReconstructionCommandProto::MergeFrom(from.blkecreconstructioncmd());
    }
  }
  if (cached_has_bits & 0x00000100u) {
    set_cmdtype(from.cmdtype());
  }
}

void DatanodeCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void DatanodeCommandProto::CopyFrom(const DatanodeCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.DatanodeCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool DatanodeCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000100) != 0x00000100) return false;
  if (has_balancercmd()) {
    if (!this->balancercmd_->IsInitialized()) return false;
  }
  if (has_blkcmd()) {
    if (!this->blkcmd_->IsInitialized()) return false;
  }
  if (has_recoverycmd()) {
    if (!this->recoverycmd_->IsInitialized()) return false;
  }
  if (has_finalizecmd()) {
    if (!this->finalizecmd_->IsInitialized()) return false;
  }
  if (has_keyupdatecmd()) {
    if (!this->keyupdatecmd_->IsInitialized()) return false;
  }
  if (has_blkidcmd()) {
    if (!this->blkidcmd_->IsInitialized()) return false;
  }
  if (has_blkecreconstructioncmd()) {
    if (!this->blkecreconstructioncmd_->IsInitialized()) return false;
  }
  return true;
}

void DatanodeCommandProto::Swap(DatanodeCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void DatanodeCommandProto::InternalSwap(DatanodeCommandProto* other) {
  using std::swap;
  swap(balancercmd_, other->balancercmd_);
  swap(blkcmd_, other->blkcmd_);
  swap(recoverycmd_, other->recoverycmd_);
  swap(finalizecmd_, other->finalizecmd_);
  swap(keyupdatecmd_, other->keyupdatecmd_);
  swap(registercmd_, other->registercmd_);
  swap(blkidcmd_, other->blkidcmd_);
  swap(blkecreconstructioncmd_, other->blkecreconstructioncmd_);
  swap(cmdtype_, other->cmdtype_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata DatanodeCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BalancerBandwidthCommandProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BalancerBandwidthCommandProto::kBandwidthFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BalancerBandwidthCommandProto::BalancerBandwidthCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BalancerBandwidthCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
}
BalancerBandwidthCommandProto::BalancerBandwidthCommandProto(const BalancerBandwidthCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  bandwidth_ = from.bandwidth_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
}

void BalancerBandwidthCommandProto::SharedCtor() {
  bandwidth_ = GOOGLE_ULONGLONG(0);
}

BalancerBandwidthCommandProto::~BalancerBandwidthCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  SharedDtor();
}

void BalancerBandwidthCommandProto::SharedDtor() {
}

void BalancerBandwidthCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BalancerBandwidthCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BalancerBandwidthCommandProto& BalancerBandwidthCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BalancerBandwidthCommandProto.base);
  return *internal_default_instance();
}


void BalancerBandwidthCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  bandwidth_ = GOOGLE_ULONGLONG(0);
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BalancerBandwidthCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required uint64 bandwidth = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u /* 8 & 0xFF */)) {
          set_has_bandwidth();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &bandwidth_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  return false;
#undef DO_
}

void BalancerBandwidthCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required uint64 bandwidth = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(1, this->bandwidth(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
}

::google::protobuf::uint8* BalancerBandwidthCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required uint64 bandwidth = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(1, this->bandwidth(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  return target;
}

size_t BalancerBandwidthCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required uint64 bandwidth = 1;
  if (has_bandwidth()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->bandwidth());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BalancerBandwidthCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BalancerBandwidthCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BalancerBandwidthCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
    MergeFrom(*source);
  }
}

void BalancerBandwidthCommandProto::MergeFrom(const BalancerBandwidthCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_bandwidth()) {
    set_bandwidth(from.bandwidth());
  }
}

void BalancerBandwidthCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BalancerBandwidthCommandProto::CopyFrom(const BalancerBandwidthCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BalancerBandwidthCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  return true;
}

void BalancerBandwidthCommandProto::Swap(BalancerBandwidthCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BalancerBandwidthCommandProto::InternalSwap(BalancerBandwidthCommandProto* other) {
  using std::swap;
  swap(bandwidth_, other->bandwidth_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BalancerBandwidthCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockCommandProto::InitAsDefaultInstance() {
}
void BlockCommandProto::clear_blocks() {
  blocks_.Clear();
}
void BlockCommandProto::clear_targets() {
  targets_.Clear();
}
void BlockCommandProto::clear_targetstorageuuids() {
  targetstorageuuids_.Clear();
}
void BlockCommandProto::clear_targetstoragetypes() {
  targetstoragetypes_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockCommandProto::kActionFieldNumber;
const int BlockCommandProto::kBlockPoolIdFieldNumber;
const int BlockCommandProto::kBlocksFieldNumber;
const int BlockCommandProto::kTargetsFieldNumber;
const int BlockCommandProto::kTargetStorageUuidsFieldNumber;
const int BlockCommandProto::kTargetStorageTypesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockCommandProto::BlockCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockCommandProto)
}
BlockCommandProto::BlockCommandProto(const BlockCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_),
      targets_(from.targets_),
      targetstorageuuids_(from.targetstorageuuids_),
      targetstoragetypes_(from.targetstoragetypes_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  action_ = from.action_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockCommandProto)
}

void BlockCommandProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  action_ = 1;
}

BlockCommandProto::~BlockCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockCommandProto)
  SharedDtor();
}

void BlockCommandProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void BlockCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockCommandProto& BlockCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockCommandProto.base);
  return *internal_default_instance();
}


void BlockCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  targets_.Clear();
  targetstorageuuids_.Clear();
  targetstoragetypes_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      blockpoolid_.ClearNonDefaultToEmptyNoArena();
    }
    action_ = 1;
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u /* 8 & 0xFF */)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          if (::hadoop::hdfs::datanode::BlockCommandProto_Action_IsValid(value)) {
            set_action(static_cast< ::hadoop::hdfs::datanode::BlockCommandProto_Action >(value));
          } else {
            mutable_unknown_fields()->AddVarint(
                1, static_cast< ::google::protobuf::uint64>(value));
          }
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string blockPoolId = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.BlockCommandProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.BlockProto blocks = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blocks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_targets()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(42u /* 42 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_targetstorageuuids()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(50u /* 50 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_targetstoragetypes()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockCommandProto)
  return false;
#undef DO_
}

void BlockCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->action(), output);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockCommandProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->blockpoolid(), output);
  }

  // repeated .hadoop.hdfs.BlockProto blocks = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3,
      this->blocks(static_cast<int>(i)),
      output);
  }

  // repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targets_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4,
      this->targets(static_cast<int>(i)),
      output);
  }

  // repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targetstorageuuids_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      5,
      this->targetstorageuuids(static_cast<int>(i)),
      output);
  }

  // repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targetstoragetypes_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      6,
      this->targetstoragetypes(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockCommandProto)
}

::google::protobuf::uint8* BlockCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteEnumToArray(
      1, this->action(), target);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockCommandProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->blockpoolid(), target);
  }

  // repeated .hadoop.hdfs.BlockProto blocks = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->blocks(static_cast<int>(i)), deterministic, target);
  }

  // repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targets_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        4, this->targets(static_cast<int>(i)), deterministic, target);
  }

  // repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targetstorageuuids_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        5, this->targetstorageuuids(static_cast<int>(i)), deterministic, target);
  }

  // repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->targetstoragetypes_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        6, this->targetstoragetypes(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockCommandProto)
  return target;
}

size_t BlockCommandProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.BlockCommandProto)
  size_t total_size = 0;

  if (has_blockpoolid()) {
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }

  if (has_action()) {
    // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->action());
  }

  return total_size;
}
size_t BlockCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());

    // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->action());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated .hadoop.hdfs.BlockProto blocks = 3;
  {
    unsigned int count = static_cast<unsigned int>(this->blocks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blocks(static_cast<int>(i)));
    }
  }

  // repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
  {
    unsigned int count = static_cast<unsigned int>(this->targets_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->targets(static_cast<int>(i)));
    }
  }

  // repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
  {
    unsigned int count = static_cast<unsigned int>(this->targetstorageuuids_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->targetstorageuuids(static_cast<int>(i)));
    }
  }

  // repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
  {
    unsigned int count = static_cast<unsigned int>(this->targetstoragetypes_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->targetstoragetypes(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockCommandProto)
    MergeFrom(*source);
  }
}

void BlockCommandProto::MergeFrom(const BlockCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
  targets_.MergeFrom(from.targets_);
  targetstorageuuids_.MergeFrom(from.targetstorageuuids_);
  targetstoragetypes_.MergeFrom(from.targetstoragetypes_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_blockpoolid();
      blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
    }
    if (cached_has_bits & 0x00000002u) {
      action_ = from.action_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void BlockCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockCommandProto::CopyFrom(const BlockCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->blocks())) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->targets())) return false;
  return true;
}

void BlockCommandProto::Swap(BlockCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockCommandProto::InternalSwap(BlockCommandProto* other) {
  using std::swap;
  CastToBase(&blocks_)->InternalSwap(CastToBase(&other->blocks_));
  CastToBase(&targets_)->InternalSwap(CastToBase(&other->targets_));
  CastToBase(&targetstorageuuids_)->InternalSwap(CastToBase(&other->targetstorageuuids_));
  CastToBase(&targetstoragetypes_)->InternalSwap(CastToBase(&other->targetstoragetypes_));
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(action_, other->action_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockIdCommandProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockIdCommandProto::kActionFieldNumber;
const int BlockIdCommandProto::kBlockPoolIdFieldNumber;
const int BlockIdCommandProto::kBlockIdsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockIdCommandProto::BlockIdCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockIdCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockIdCommandProto)
}
BlockIdCommandProto::BlockIdCommandProto(const BlockIdCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blockids_(from.blockids_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  action_ = from.action_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockIdCommandProto)
}

void BlockIdCommandProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  action_ = 1;
}

BlockIdCommandProto::~BlockIdCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockIdCommandProto)
  SharedDtor();
}

void BlockIdCommandProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void BlockIdCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockIdCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockIdCommandProto& BlockIdCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockIdCommandProto.base);
  return *internal_default_instance();
}


void BlockIdCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blockids_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      blockpoolid_.ClearNonDefaultToEmptyNoArena();
    }
    action_ = 1;
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockIdCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u /* 8 & 0xFF */)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          if (::hadoop::hdfs::datanode::BlockIdCommandProto_Action_IsValid(value)) {
            set_action(static_cast< ::hadoop::hdfs::datanode::BlockIdCommandProto_Action >(value));
          } else {
            mutable_unknown_fields()->AddVarint(
                1, static_cast< ::google::protobuf::uint64>(value));
          }
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string blockPoolId = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 blockIds = 3 [packed = true];
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_blockids())));
        } else if (
            static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 26u, input, this->mutable_blockids())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockIdCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockIdCommandProto)
  return false;
#undef DO_
}

void BlockIdCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      1, this->action(), output);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->blockpoolid(), output);
  }

  // repeated uint64 blockIds = 3 [packed = true];
  if (this->blockids_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(static_cast< ::google::protobuf::uint32>(
        _blockids_cached_byte_size_));
  }
  for (int i = 0, n = this->blockids_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->blockids(i), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockIdCommandProto)
}

::google::protobuf::uint8* BlockIdCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteEnumToArray(
      1, this->action(), target);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->blockpoolid(), target);
  }

  // repeated uint64 blockIds = 3 [packed = true];
  if (this->blockids_size() > 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteTagToArray(
      3,
      ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED,
      target);
    target = ::google::protobuf::io::CodedOutputStream::WriteVarint32ToArray(
        static_cast< ::google::protobuf::int32>(
            _blockids_cached_byte_size_), target);
    target = ::google::protobuf::internal::WireFormatLite::
      WriteUInt64NoTagToArray(this->blockids_, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockIdCommandProto)
  return target;
}

size_t BlockIdCommandProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  size_t total_size = 0;

  if (has_blockpoolid()) {
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }

  if (has_action()) {
    // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->action());
  }

  return total_size;
}
size_t BlockIdCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());

    // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->action());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated uint64 blockIds = 3 [packed = true];
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->blockids_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
            static_cast< ::google::protobuf::int32>(data_size));
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _blockids_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockIdCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockIdCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockIdCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockIdCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockIdCommandProto)
    MergeFrom(*source);
  }
}

void BlockIdCommandProto::MergeFrom(const BlockIdCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blockids_.MergeFrom(from.blockids_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_blockpoolid();
      blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
    }
    if (cached_has_bits & 0x00000002u) {
      action_ = from.action_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void BlockIdCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockIdCommandProto::CopyFrom(const BlockIdCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockIdCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockIdCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  return true;
}

void BlockIdCommandProto::Swap(BlockIdCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockIdCommandProto::InternalSwap(BlockIdCommandProto* other) {
  using std::swap;
  blockids_.InternalSwap(&other->blockids_);
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(action_, other->action_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockIdCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockRecoveryCommandProto::InitAsDefaultInstance() {
}
void BlockRecoveryCommandProto::clear_blocks() {
  blocks_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockRecoveryCommandProto::kBlocksFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockRecoveryCommandProto::BlockRecoveryCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockRecoveryCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
}
BlockRecoveryCommandProto::BlockRecoveryCommandProto(const BlockRecoveryCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
}

void BlockRecoveryCommandProto::SharedCtor() {
}

BlockRecoveryCommandProto::~BlockRecoveryCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  SharedDtor();
}

void BlockRecoveryCommandProto::SharedDtor() {
}

void BlockRecoveryCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockRecoveryCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockRecoveryCommandProto& BlockRecoveryCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockRecoveryCommandProto.base);
  return *internal_default_instance();
}


void BlockRecoveryCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockRecoveryCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blocks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  return false;
#undef DO_
}

void BlockRecoveryCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->blocks(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
}

::google::protobuf::uint8* BlockRecoveryCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->blocks(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  return target;
}

size_t BlockRecoveryCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->blocks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blocks(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockRecoveryCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockRecoveryCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockRecoveryCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
    MergeFrom(*source);
  }
}

void BlockRecoveryCommandProto::MergeFrom(const BlockRecoveryCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
}

void BlockRecoveryCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockRecoveryCommandProto::CopyFrom(const BlockRecoveryCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockRecoveryCommandProto::IsInitialized() const {
  if (!::google::protobuf::internal::AllAreInitialized(this->blocks())) return false;
  return true;
}

void BlockRecoveryCommandProto::Swap(BlockRecoveryCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockRecoveryCommandProto::InternalSwap(BlockRecoveryCommandProto* other) {
  using std::swap;
  CastToBase(&blocks_)->InternalSwap(CastToBase(&other->blocks_));
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockRecoveryCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void FinalizeCommandProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int FinalizeCommandProto::kBlockPoolIdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

FinalizeCommandProto::FinalizeCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_FinalizeCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.FinalizeCommandProto)
}
FinalizeCommandProto::FinalizeCommandProto(const FinalizeCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.FinalizeCommandProto)
}

void FinalizeCommandProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

FinalizeCommandProto::~FinalizeCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.FinalizeCommandProto)
  SharedDtor();
}

void FinalizeCommandProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void FinalizeCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* FinalizeCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const FinalizeCommandProto& FinalizeCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_FinalizeCommandProto.base);
  return *internal_default_instance();
}


void FinalizeCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    blockpoolid_.ClearNonDefaultToEmptyNoArena();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool FinalizeCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required string blockPoolId = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.FinalizeCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.FinalizeCommandProto)
  return false;
#undef DO_
}

void FinalizeCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required string blockPoolId = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->blockpoolid(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.FinalizeCommandProto)
}

::google::protobuf::uint8* FinalizeCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required string blockPoolId = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        1, this->blockpoolid(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.FinalizeCommandProto)
  return target;
}

size_t FinalizeCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required string blockPoolId = 1;
  if (has_blockpoolid()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void FinalizeCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const FinalizeCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const FinalizeCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.FinalizeCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.FinalizeCommandProto)
    MergeFrom(*source);
  }
}

void FinalizeCommandProto::MergeFrom(const FinalizeCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_blockpoolid()) {
    set_has_blockpoolid();
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
}

void FinalizeCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void FinalizeCommandProto::CopyFrom(const FinalizeCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.FinalizeCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool FinalizeCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  return true;
}

void FinalizeCommandProto::Swap(FinalizeCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void FinalizeCommandProto::InternalSwap(FinalizeCommandProto* other) {
  using std::swap;
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata FinalizeCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void KeyUpdateCommandProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_KeyUpdateCommandProto_default_instance_._instance.get_mutable()->keys_ = const_cast< ::hadoop::hdfs::ExportedBlockKeysProto*>(
      ::hadoop::hdfs::ExportedBlockKeysProto::internal_default_instance());
}
void KeyUpdateCommandProto::clear_keys() {
  if (keys_ != NULL) keys_->Clear();
  clear_has_keys();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int KeyUpdateCommandProto::kKeysFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

KeyUpdateCommandProto::KeyUpdateCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_KeyUpdateCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.KeyUpdateCommandProto)
}
KeyUpdateCommandProto::KeyUpdateCommandProto(const KeyUpdateCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_keys()) {
    keys_ = new ::hadoop::hdfs::ExportedBlockKeysProto(*from.keys_);
  } else {
    keys_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.KeyUpdateCommandProto)
}

void KeyUpdateCommandProto::SharedCtor() {
  keys_ = NULL;
}

KeyUpdateCommandProto::~KeyUpdateCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  SharedDtor();
}

void KeyUpdateCommandProto::SharedDtor() {
  if (this != internal_default_instance()) delete keys_;
}

void KeyUpdateCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* KeyUpdateCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const KeyUpdateCommandProto& KeyUpdateCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_KeyUpdateCommandProto.base);
  return *internal_default_instance();
}


void KeyUpdateCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(keys_ != NULL);
    keys_->Clear();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool KeyUpdateCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_keys()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  return false;
#undef DO_
}

void KeyUpdateCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_keys(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.KeyUpdateCommandProto)
}

::google::protobuf::uint8* KeyUpdateCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_keys(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  return target;
}

size_t KeyUpdateCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
  if (has_keys()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *keys_);
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void KeyUpdateCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const KeyUpdateCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const KeyUpdateCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.KeyUpdateCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.KeyUpdateCommandProto)
    MergeFrom(*source);
  }
}

void KeyUpdateCommandProto::MergeFrom(const KeyUpdateCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_keys()) {
    mutable_keys()->::hadoop::hdfs::ExportedBlockKeysProto::MergeFrom(from.keys());
  }
}

void KeyUpdateCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void KeyUpdateCommandProto::CopyFrom(const KeyUpdateCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.KeyUpdateCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool KeyUpdateCommandProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (has_keys()) {
    if (!this->keys_->IsInitialized()) return false;
  }
  return true;
}

void KeyUpdateCommandProto::Swap(KeyUpdateCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void KeyUpdateCommandProto::InternalSwap(KeyUpdateCommandProto* other) {
  using std::swap;
  swap(keys_, other->keys_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata KeyUpdateCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void RegisterCommandProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegisterCommandProto::RegisterCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_RegisterCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.RegisterCommandProto)
}
RegisterCommandProto::RegisterCommandProto(const RegisterCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.RegisterCommandProto)
}

void RegisterCommandProto::SharedCtor() {
}

RegisterCommandProto::~RegisterCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.RegisterCommandProto)
  SharedDtor();
}

void RegisterCommandProto::SharedDtor() {
}

void RegisterCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* RegisterCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const RegisterCommandProto& RegisterCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_RegisterCommandProto.base);
  return *internal_default_instance();
}


void RegisterCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.RegisterCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool RegisterCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.RegisterCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.RegisterCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.RegisterCommandProto)
  return false;
#undef DO_
}

void RegisterCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.RegisterCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.RegisterCommandProto)
}

::google::protobuf::uint8* RegisterCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.RegisterCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.RegisterCommandProto)
  return target;
}

size_t RegisterCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.RegisterCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void RegisterCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.RegisterCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const RegisterCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const RegisterCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.RegisterCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.RegisterCommandProto)
    MergeFrom(*source);
  }
}

void RegisterCommandProto::MergeFrom(const RegisterCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.RegisterCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void RegisterCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.RegisterCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegisterCommandProto::CopyFrom(const RegisterCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.RegisterCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegisterCommandProto::IsInitialized() const {
  return true;
}

void RegisterCommandProto::Swap(RegisterCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RegisterCommandProto::InternalSwap(RegisterCommandProto* other) {
  using std::swap;
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata RegisterCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockECReconstructionCommandProto::InitAsDefaultInstance() {
}
void BlockECReconstructionCommandProto::clear_blockecreconstructioninfo() {
  blockecreconstructioninfo_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockECReconstructionCommandProto::kBlockECReconstructioninfoFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockECReconstructionCommandProto::BlockECReconstructionCommandProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockECReconstructionCommandProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
}
BlockECReconstructionCommandProto::BlockECReconstructionCommandProto(const BlockECReconstructionCommandProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blockecreconstructioninfo_(from.blockecreconstructioninfo_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
}

void BlockECReconstructionCommandProto::SharedCtor() {
}

BlockECReconstructionCommandProto::~BlockECReconstructionCommandProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  SharedDtor();
}

void BlockECReconstructionCommandProto::SharedDtor() {
}

void BlockECReconstructionCommandProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockECReconstructionCommandProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockECReconstructionCommandProto& BlockECReconstructionCommandProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockECReconstructionCommandProto.base);
  return *internal_default_instance();
}


void BlockECReconstructionCommandProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blockecreconstructioninfo_.Clear();
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockECReconstructionCommandProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blockecreconstructioninfo()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  return false;
#undef DO_
}

void BlockECReconstructionCommandProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blockecreconstructioninfo_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->blockecreconstructioninfo(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
}

::google::protobuf::uint8* BlockECReconstructionCommandProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blockecreconstructioninfo_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->blockecreconstructioninfo(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  return target;
}

size_t BlockECReconstructionCommandProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->blockecreconstructioninfo_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blockecreconstructioninfo(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockECReconstructionCommandProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockECReconstructionCommandProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockECReconstructionCommandProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
    MergeFrom(*source);
  }
}

void BlockECReconstructionCommandProto::MergeFrom(const BlockECReconstructionCommandProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blockecreconstructioninfo_.MergeFrom(from.blockecreconstructioninfo_);
}

void BlockECReconstructionCommandProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockECReconstructionCommandProto::CopyFrom(const BlockECReconstructionCommandProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockECReconstructionCommandProto::IsInitialized() const {
  if (!::google::protobuf::internal::AllAreInitialized(this->blockecreconstructioninfo())) return false;
  return true;
}

void BlockECReconstructionCommandProto::Swap(BlockECReconstructionCommandProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockECReconstructionCommandProto::InternalSwap(BlockECReconstructionCommandProto* other) {
  using std::swap;
  CastToBase(&blockecreconstructioninfo_)->InternalSwap(CastToBase(&other->blockecreconstructioninfo_));
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockECReconstructionCommandProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void RegisterDatanodeRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_RegisterDatanodeRequestProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegisterDatanodeRequestProto::kRegistrationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegisterDatanodeRequestProto::RegisterDatanodeRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_RegisterDatanodeRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
}
RegisterDatanodeRequestProto::RegisterDatanodeRequestProto(const RegisterDatanodeRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
}

void RegisterDatanodeRequestProto::SharedCtor() {
  registration_ = NULL;
}

RegisterDatanodeRequestProto::~RegisterDatanodeRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  SharedDtor();
}

void RegisterDatanodeRequestProto::SharedDtor() {
  if (this != internal_default_instance()) delete registration_;
}

void RegisterDatanodeRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* RegisterDatanodeRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const RegisterDatanodeRequestProto& RegisterDatanodeRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_RegisterDatanodeRequestProto.base);
  return *internal_default_instance();
}


void RegisterDatanodeRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(registration_ != NULL);
    registration_->Clear();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool RegisterDatanodeRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  return false;
#undef DO_
}

void RegisterDatanodeRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
}

::google::protobuf::uint8* RegisterDatanodeRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  return target;
}

size_t RegisterDatanodeRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (has_registration()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void RegisterDatanodeRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const RegisterDatanodeRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const RegisterDatanodeRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
    MergeFrom(*source);
  }
}

void RegisterDatanodeRequestProto::MergeFrom(const RegisterDatanodeRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_registration()) {
    mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
  }
}

void RegisterDatanodeRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegisterDatanodeRequestProto::CopyFrom(const RegisterDatanodeRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegisterDatanodeRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  return true;
}

void RegisterDatanodeRequestProto::Swap(RegisterDatanodeRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RegisterDatanodeRequestProto::InternalSwap(RegisterDatanodeRequestProto* other) {
  using std::swap;
  swap(registration_, other->registration_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata RegisterDatanodeRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void RegisterDatanodeResponseProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_RegisterDatanodeResponseProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int RegisterDatanodeResponseProto::kRegistrationFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

RegisterDatanodeResponseProto::RegisterDatanodeResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_RegisterDatanodeResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
}
RegisterDatanodeResponseProto::RegisterDatanodeResponseProto(const RegisterDatanodeResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
}

void RegisterDatanodeResponseProto::SharedCtor() {
  registration_ = NULL;
}

RegisterDatanodeResponseProto::~RegisterDatanodeResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  SharedDtor();
}

void RegisterDatanodeResponseProto::SharedDtor() {
  if (this != internal_default_instance()) delete registration_;
}

void RegisterDatanodeResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* RegisterDatanodeResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const RegisterDatanodeResponseProto& RegisterDatanodeResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_RegisterDatanodeResponseProto.base);
  return *internal_default_instance();
}


void RegisterDatanodeResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(registration_ != NULL);
    registration_->Clear();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool RegisterDatanodeResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  return false;
#undef DO_
}

void RegisterDatanodeResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
}

::google::protobuf::uint8* RegisterDatanodeResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  return target;
}

size_t RegisterDatanodeResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (has_registration()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void RegisterDatanodeResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const RegisterDatanodeResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const RegisterDatanodeResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
    MergeFrom(*source);
  }
}

void RegisterDatanodeResponseProto::MergeFrom(const RegisterDatanodeResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_registration()) {
    mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
  }
}

void RegisterDatanodeResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void RegisterDatanodeResponseProto::CopyFrom(const RegisterDatanodeResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool RegisterDatanodeResponseProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  return true;
}

void RegisterDatanodeResponseProto::Swap(RegisterDatanodeResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void RegisterDatanodeResponseProto::InternalSwap(RegisterDatanodeResponseProto* other) {
  using std::swap;
  swap(registration_, other->registration_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata RegisterDatanodeResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void VolumeFailureSummaryProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int VolumeFailureSummaryProto::kFailedStorageLocationsFieldNumber;
const int VolumeFailureSummaryProto::kLastVolumeFailureDateFieldNumber;
const int VolumeFailureSummaryProto::kEstimatedCapacityLostTotalFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

VolumeFailureSummaryProto::VolumeFailureSummaryProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_VolumeFailureSummaryProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
}
VolumeFailureSummaryProto::VolumeFailureSummaryProto(const VolumeFailureSummaryProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      failedstoragelocations_(from.failedstoragelocations_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&lastvolumefailuredate_, &from.lastvolumefailuredate_,
    static_cast<size_t>(reinterpret_cast<char*>(&estimatedcapacitylosttotal_) -
    reinterpret_cast<char*>(&lastvolumefailuredate_)) + sizeof(estimatedcapacitylosttotal_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
}

void VolumeFailureSummaryProto::SharedCtor() {
  ::memset(&lastvolumefailuredate_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&estimatedcapacitylosttotal_) -
      reinterpret_cast<char*>(&lastvolumefailuredate_)) + sizeof(estimatedcapacitylosttotal_));
}

VolumeFailureSummaryProto::~VolumeFailureSummaryProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  SharedDtor();
}

void VolumeFailureSummaryProto::SharedDtor() {
}

void VolumeFailureSummaryProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* VolumeFailureSummaryProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const VolumeFailureSummaryProto& VolumeFailureSummaryProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_VolumeFailureSummaryProto.base);
  return *internal_default_instance();
}


void VolumeFailureSummaryProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  failedstoragelocations_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    ::memset(&lastvolumefailuredate_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&estimatedcapacitylosttotal_) -
        reinterpret_cast<char*>(&lastvolumefailuredate_)) + sizeof(estimatedcapacitylosttotal_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool VolumeFailureSummaryProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated string failedStorageLocations = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_failedstoragelocations()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->failedstoragelocations(this->failedstoragelocations_size() - 1).data(),
            static_cast<int>(this->failedstoragelocations(this->failedstoragelocations_size() - 1).length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required uint64 lastVolumeFailureDate = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u /* 16 & 0xFF */)) {
          set_has_lastvolumefailuredate();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &lastvolumefailuredate_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required uint64 estimatedCapacityLostTotal = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_estimatedcapacitylosttotal();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &estimatedcapacitylosttotal_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  return false;
#undef DO_
}

void VolumeFailureSummaryProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated string failedStorageLocations = 1;
  for (int i = 0, n = this->failedstoragelocations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->failedstoragelocations(i).data(), static_cast<int>(this->failedstoragelocations(i).length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      1, this->failedstoragelocations(i), output);
  }

  cached_has_bits = _has_bits_[0];
  // required uint64 lastVolumeFailureDate = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->lastvolumefailuredate(), output);
  }

  // required uint64 estimatedCapacityLostTotal = 3;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->estimatedcapacitylosttotal(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
}

::google::protobuf::uint8* VolumeFailureSummaryProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated string failedStorageLocations = 1;
  for (int i = 0, n = this->failedstoragelocations_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->failedstoragelocations(i).data(), static_cast<int>(this->failedstoragelocations(i).length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations");
    target = ::google::protobuf::internal::WireFormatLite::
      WriteStringToArray(1, this->failedstoragelocations(i), target);
  }

  cached_has_bits = _has_bits_[0];
  // required uint64 lastVolumeFailureDate = 2;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(2, this->lastvolumefailuredate(), target);
  }

  // required uint64 estimatedCapacityLostTotal = 3;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(3, this->estimatedcapacitylosttotal(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  return target;
}

size_t VolumeFailureSummaryProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  size_t total_size = 0;

  if (has_lastvolumefailuredate()) {
    // required uint64 lastVolumeFailureDate = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->lastvolumefailuredate());
  }

  if (has_estimatedcapacitylosttotal()) {
    // required uint64 estimatedCapacityLostTotal = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->estimatedcapacitylosttotal());
  }

  return total_size;
}
size_t VolumeFailureSummaryProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required uint64 lastVolumeFailureDate = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->lastvolumefailuredate());

    // required uint64 estimatedCapacityLostTotal = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->estimatedcapacitylosttotal());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated string failedStorageLocations = 1;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->failedstoragelocations_size());
  for (int i = 0, n = this->failedstoragelocations_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->failedstoragelocations(i));
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void VolumeFailureSummaryProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  GOOGLE_DCHECK_NE(&from, this);
  const VolumeFailureSummaryProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const VolumeFailureSummaryProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
    MergeFrom(*source);
  }
}

void VolumeFailureSummaryProto::MergeFrom(const VolumeFailureSummaryProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  failedstoragelocations_.MergeFrom(from.failedstoragelocations_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      lastvolumefailuredate_ = from.lastvolumefailuredate_;
    }
    if (cached_has_bits & 0x00000002u) {
      estimatedcapacitylosttotal_ = from.estimatedcapacitylosttotal_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void VolumeFailureSummaryProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void VolumeFailureSummaryProto::CopyFrom(const VolumeFailureSummaryProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool VolumeFailureSummaryProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  return true;
}

void VolumeFailureSummaryProto::Swap(VolumeFailureSummaryProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void VolumeFailureSummaryProto::InternalSwap(VolumeFailureSummaryProto* other) {
  using std::swap;
  failedstoragelocations_.InternalSwap(CastToBase(&other->failedstoragelocations_));
  swap(lastvolumefailuredate_, other->lastvolumefailuredate_);
  swap(estimatedcapacitylosttotal_, other->estimatedcapacitylosttotal_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata VolumeFailureSummaryProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void HeartbeatRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_HeartbeatRequestProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_HeartbeatRequestProto_default_instance_._instance.get_mutable()->volumefailuresummary_ = const_cast< ::hadoop::hdfs::datanode::VolumeFailureSummaryProto*>(
      ::hadoop::hdfs::datanode::VolumeFailureSummaryProto::internal_default_instance());
}
void HeartbeatRequestProto::clear_reports() {
  reports_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int HeartbeatRequestProto::kRegistrationFieldNumber;
const int HeartbeatRequestProto::kReportsFieldNumber;
const int HeartbeatRequestProto::kXmitsInProgressFieldNumber;
const int HeartbeatRequestProto::kXceiverCountFieldNumber;
const int HeartbeatRequestProto::kFailedVolumesFieldNumber;
const int HeartbeatRequestProto::kCacheCapacityFieldNumber;
const int HeartbeatRequestProto::kCacheUsedFieldNumber;
const int HeartbeatRequestProto::kVolumeFailureSummaryFieldNumber;
const int HeartbeatRequestProto::kRequestFullBlockReportLeaseFieldNumber;
const int HeartbeatRequestProto::kSlowPeersFieldNumber;
const int HeartbeatRequestProto::kSlowDisksFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

HeartbeatRequestProto::HeartbeatRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_HeartbeatRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.HeartbeatRequestProto)
}
HeartbeatRequestProto::HeartbeatRequestProto(const HeartbeatRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      reports_(from.reports_),
      slowpeers_(from.slowpeers_),
      slowdisks_(from.slowdisks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  if (from.has_volumefailuresummary()) {
    volumefailuresummary_ = new ::hadoop::hdfs::datanode::VolumeFailureSummaryProto(*from.volumefailuresummary_);
  } else {
    volumefailuresummary_ = NULL;
  }
  ::memcpy(&xmitsinprogress_, &from.xmitsinprogress_,
    static_cast<size_t>(reinterpret_cast<char*>(&requestfullblockreportlease_) -
    reinterpret_cast<char*>(&xmitsinprogress_)) + sizeof(requestfullblockreportlease_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.HeartbeatRequestProto)
}

void HeartbeatRequestProto::SharedCtor() {
  ::memset(&registration_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&requestfullblockreportlease_) -
      reinterpret_cast<char*>(&registration_)) + sizeof(requestfullblockreportlease_));
}

HeartbeatRequestProto::~HeartbeatRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.HeartbeatRequestProto)
  SharedDtor();
}

void HeartbeatRequestProto::SharedDtor() {
  if (this != internal_default_instance()) delete registration_;
  if (this != internal_default_instance()) delete volumefailuresummary_;
}

void HeartbeatRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* HeartbeatRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const HeartbeatRequestProto& HeartbeatRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_HeartbeatRequestProto.base);
  return *internal_default_instance();
}


void HeartbeatRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  reports_.Clear();
  slowpeers_.Clear();
  slowdisks_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      GOOGLE_DCHECK(registration_ != NULL);
      registration_->Clear();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(volumefailuresummary_ != NULL);
      volumefailuresummary_->Clear();
    }
  }
  if (cached_has_bits & 252u) {
    ::memset(&xmitsinprogress_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&requestfullblockreportlease_) -
        reinterpret_cast<char*>(&xmitsinprogress_)) + sizeof(requestfullblockreportlease_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool HeartbeatRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.StorageReportProto reports = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_reports()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint32 xmitsInProgress = 3 [default = 0];
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_xmitsinprogress();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &xmitsinprogress_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint32 xceiverCount = 4 [default = 0];
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u /* 32 & 0xFF */)) {
          set_has_xceivercount();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &xceivercount_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint32 failedVolumes = 5 [default = 0];
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u /* 40 & 0xFF */)) {
          set_has_failedvolumes();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &failedvolumes_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint64 cacheCapacity = 6 [default = 0];
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(48u /* 48 & 0xFF */)) {
          set_has_cachecapacity();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &cachecapacity_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint64 cacheUsed = 7 [default = 0];
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(56u /* 56 & 0xFF */)) {
          set_has_cacheused();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &cacheused_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
      case 8: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(66u /* 66 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_volumefailuresummary()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional bool requestFullBlockReportLease = 9 [default = false];
      case 9: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(72u /* 72 & 0xFF */)) {
          set_has_requestfullblockreportlease();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &requestfullblockreportlease_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
      case 10: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(82u /* 82 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_slowpeers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
      case 11: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(90u /* 90 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_slowdisks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.HeartbeatRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.HeartbeatRequestProto)
  return false;
#undef DO_
}

void HeartbeatRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  // repeated .hadoop.hdfs.StorageReportProto reports = 2;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->reports_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2,
      this->reports(static_cast<int>(i)),
      output);
  }

  // optional uint32 xmitsInProgress = 3 [default = 0];
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(3, this->xmitsinprogress(), output);
  }

  // optional uint32 xceiverCount = 4 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(4, this->xceivercount(), output);
  }

  // optional uint32 failedVolumes = 5 [default = 0];
  if (cached_has_bits & 0x00000040u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(5, this->failedvolumes(), output);
  }

  // optional uint64 cacheCapacity = 6 [default = 0];
  if (cached_has_bits & 0x00000010u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(6, this->cachecapacity(), output);
  }

  // optional uint64 cacheUsed = 7 [default = 0];
  if (cached_has_bits & 0x00000020u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(7, this->cacheused(), output);
  }

  // optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      8, this->_internal_volumefailuresummary(), output);
  }

  // optional bool requestFullBlockReportLease = 9 [default = false];
  if (cached_has_bits & 0x00000080u) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(9, this->requestfullblockreportlease(), output);
  }

  // repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->slowpeers_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      10,
      this->slowpeers(static_cast<int>(i)),
      output);
  }

  // repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->slowdisks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      11,
      this->slowdisks(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.HeartbeatRequestProto)
}

::google::protobuf::uint8* HeartbeatRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  // repeated .hadoop.hdfs.StorageReportProto reports = 2;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->reports_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->reports(static_cast<int>(i)), deterministic, target);
  }

  // optional uint32 xmitsInProgress = 3 [default = 0];
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt32ToArray(3, this->xmitsinprogress(), target);
  }

  // optional uint32 xceiverCount = 4 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt32ToArray(4, this->xceivercount(), target);
  }

  // optional uint32 failedVolumes = 5 [default = 0];
  if (cached_has_bits & 0x00000040u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt32ToArray(5, this->failedvolumes(), target);
  }

  // optional uint64 cacheCapacity = 6 [default = 0];
  if (cached_has_bits & 0x00000010u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(6, this->cachecapacity(), target);
  }

  // optional uint64 cacheUsed = 7 [default = 0];
  if (cached_has_bits & 0x00000020u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(7, this->cacheused(), target);
  }

  // optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        8, this->_internal_volumefailuresummary(), deterministic, target);
  }

  // optional bool requestFullBlockReportLease = 9 [default = false];
  if (cached_has_bits & 0x00000080u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteBoolToArray(9, this->requestfullblockreportlease(), target);
  }

  // repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->slowpeers_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        10, this->slowpeers(static_cast<int>(i)), deterministic, target);
  }

  // repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->slowdisks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        11, this->slowdisks(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.HeartbeatRequestProto)
  return target;
}

size_t HeartbeatRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (has_registration()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }
  // repeated .hadoop.hdfs.StorageReportProto reports = 2;
  {
    unsigned int count = static_cast<unsigned int>(this->reports_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->reports(static_cast<int>(i)));
    }
  }

  // repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
  {
    unsigned int count = static_cast<unsigned int>(this->slowpeers_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->slowpeers(static_cast<int>(i)));
    }
  }

  // repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
  {
    unsigned int count = static_cast<unsigned int>(this->slowdisks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->slowdisks(static_cast<int>(i)));
    }
  }

  if (_has_bits_[0 / 32] & 254u) {
    // optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
    if (has_volumefailuresummary()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *volumefailuresummary_);
    }

    // optional uint32 xmitsInProgress = 3 [default = 0];
    if (has_xmitsinprogress()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt32Size(
          this->xmitsinprogress());
    }

    // optional uint32 xceiverCount = 4 [default = 0];
    if (has_xceivercount()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt32Size(
          this->xceivercount());
    }

    // optional uint64 cacheCapacity = 6 [default = 0];
    if (has_cachecapacity()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt64Size(
          this->cachecapacity());
    }

    // optional uint64 cacheUsed = 7 [default = 0];
    if (has_cacheused()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt64Size(
          this->cacheused());
    }

    // optional uint32 failedVolumes = 5 [default = 0];
    if (has_failedvolumes()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt32Size(
          this->failedvolumes());
    }

    // optional bool requestFullBlockReportLease = 9 [default = false];
    if (has_requestfullblockreportlease()) {
      total_size += 1 + 1;
    }

  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void HeartbeatRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const HeartbeatRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const HeartbeatRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.HeartbeatRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.HeartbeatRequestProto)
    MergeFrom(*source);
  }
}

void HeartbeatRequestProto::MergeFrom(const HeartbeatRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  reports_.MergeFrom(from.reports_);
  slowpeers_.MergeFrom(from.slowpeers_);
  slowdisks_.MergeFrom(from.slowdisks_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 255u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_volumefailuresummary()->::hadoop::hdfs::datanode::VolumeFailureSummaryProto::MergeFrom(from.volumefailuresummary());
    }
    if (cached_has_bits & 0x00000004u) {
      xmitsinprogress_ = from.xmitsinprogress_;
    }
    if (cached_has_bits & 0x00000008u) {
      xceivercount_ = from.xceivercount_;
    }
    if (cached_has_bits & 0x00000010u) {
      cachecapacity_ = from.cachecapacity_;
    }
    if (cached_has_bits & 0x00000020u) {
      cacheused_ = from.cacheused_;
    }
    if (cached_has_bits & 0x00000040u) {
      failedvolumes_ = from.failedvolumes_;
    }
    if (cached_has_bits & 0x00000080u) {
      requestfullblockreportlease_ = from.requestfullblockreportlease_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void HeartbeatRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void HeartbeatRequestProto::CopyFrom(const HeartbeatRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.HeartbeatRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool HeartbeatRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->reports())) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  if (has_volumefailuresummary()) {
    if (!this->volumefailuresummary_->IsInitialized()) return false;
  }
  return true;
}

void HeartbeatRequestProto::Swap(HeartbeatRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void HeartbeatRequestProto::InternalSwap(HeartbeatRequestProto* other) {
  using std::swap;
  CastToBase(&reports_)->InternalSwap(CastToBase(&other->reports_));
  CastToBase(&slowpeers_)->InternalSwap(CastToBase(&other->slowpeers_));
  CastToBase(&slowdisks_)->InternalSwap(CastToBase(&other->slowdisks_));
  swap(registration_, other->registration_);
  swap(volumefailuresummary_, other->volumefailuresummary_);
  swap(xmitsinprogress_, other->xmitsinprogress_);
  swap(xceivercount_, other->xceivercount_);
  swap(cachecapacity_, other->cachecapacity_);
  swap(cacheused_, other->cacheused_);
  swap(failedvolumes_, other->failedvolumes_);
  swap(requestfullblockreportlease_, other->requestfullblockreportlease_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata HeartbeatRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void HeartbeatResponseProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_HeartbeatResponseProto_default_instance_._instance.get_mutable()->hastatus_ = const_cast< ::hadoop::hdfs::NNHAStatusHeartbeatProto*>(
      ::hadoop::hdfs::NNHAStatusHeartbeatProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_HeartbeatResponseProto_default_instance_._instance.get_mutable()->rollingupgradestatus_ = const_cast< ::hadoop::hdfs::RollingUpgradeStatusProto*>(
      ::hadoop::hdfs::RollingUpgradeStatusProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_HeartbeatResponseProto_default_instance_._instance.get_mutable()->rollingupgradestatusv2_ = const_cast< ::hadoop::hdfs::RollingUpgradeStatusProto*>(
      ::hadoop::hdfs::RollingUpgradeStatusProto::internal_default_instance());
}
void HeartbeatResponseProto::clear_hastatus() {
  if (hastatus_ != NULL) hastatus_->Clear();
  clear_has_hastatus();
}
void HeartbeatResponseProto::clear_rollingupgradestatus() {
  if (rollingupgradestatus_ != NULL) rollingupgradestatus_->Clear();
  clear_has_rollingupgradestatus();
}
void HeartbeatResponseProto::clear_rollingupgradestatusv2() {
  if (rollingupgradestatusv2_ != NULL) rollingupgradestatusv2_->Clear();
  clear_has_rollingupgradestatusv2();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int HeartbeatResponseProto::kCmdsFieldNumber;
const int HeartbeatResponseProto::kHaStatusFieldNumber;
const int HeartbeatResponseProto::kRollingUpgradeStatusFieldNumber;
const int HeartbeatResponseProto::kRollingUpgradeStatusV2FieldNumber;
const int HeartbeatResponseProto::kFullBlockReportLeaseIdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

HeartbeatResponseProto::HeartbeatResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_HeartbeatResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.HeartbeatResponseProto)
}
HeartbeatResponseProto::HeartbeatResponseProto(const HeartbeatResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      cmds_(from.cmds_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_hastatus()) {
    hastatus_ = new ::hadoop::hdfs::NNHAStatusHeartbeatProto(*from.hastatus_);
  } else {
    hastatus_ = NULL;
  }
  if (from.has_rollingupgradestatus()) {
    rollingupgradestatus_ = new ::hadoop::hdfs::RollingUpgradeStatusProto(*from.rollingupgradestatus_);
  } else {
    rollingupgradestatus_ = NULL;
  }
  if (from.has_rollingupgradestatusv2()) {
    rollingupgradestatusv2_ = new ::hadoop::hdfs::RollingUpgradeStatusProto(*from.rollingupgradestatusv2_);
  } else {
    rollingupgradestatusv2_ = NULL;
  }
  fullblockreportleaseid_ = from.fullblockreportleaseid_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.HeartbeatResponseProto)
}

void HeartbeatResponseProto::SharedCtor() {
  ::memset(&hastatus_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&fullblockreportleaseid_) -
      reinterpret_cast<char*>(&hastatus_)) + sizeof(fullblockreportleaseid_));
}

HeartbeatResponseProto::~HeartbeatResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.HeartbeatResponseProto)
  SharedDtor();
}

void HeartbeatResponseProto::SharedDtor() {
  if (this != internal_default_instance()) delete hastatus_;
  if (this != internal_default_instance()) delete rollingupgradestatus_;
  if (this != internal_default_instance()) delete rollingupgradestatusv2_;
}

void HeartbeatResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* HeartbeatResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const HeartbeatResponseProto& HeartbeatResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_HeartbeatResponseProto.base);
  return *internal_default_instance();
}


void HeartbeatResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cmds_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      GOOGLE_DCHECK(hastatus_ != NULL);
      hastatus_->Clear();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(rollingupgradestatus_ != NULL);
      rollingupgradestatus_->Clear();
    }
    if (cached_has_bits & 0x00000004u) {
      GOOGLE_DCHECK(rollingupgradestatusv2_ != NULL);
      rollingupgradestatusv2_->Clear();
    }
  }
  fullblockreportleaseid_ = GOOGLE_ULONGLONG(0);
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool HeartbeatResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_cmds()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_hastatus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_rollingupgradestatus()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_rollingupgradestatusv2()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint64 fullBlockReportLeaseId = 5 [default = 0];
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u /* 40 & 0xFF */)) {
          set_has_fullblockreportleaseid();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &fullblockreportleaseid_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.HeartbeatResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.HeartbeatResponseProto)
  return false;
#undef DO_
}

void HeartbeatResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->cmds_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->cmds(static_cast<int>(i)),
      output);
  }

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, this->_internal_hastatus(), output);
  }

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, this->_internal_rollingupgradestatus(), output);
  }

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4, this->_internal_rollingupgradestatusv2(), output);
  }

  // optional uint64 fullBlockReportLeaseId = 5 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(5, this->fullblockreportleaseid(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.HeartbeatResponseProto)
}

::google::protobuf::uint8* HeartbeatResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->cmds_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->cmds(static_cast<int>(i)), deterministic, target);
  }

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->_internal_hastatus(), deterministic, target);
  }

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->_internal_rollingupgradestatus(), deterministic, target);
  }

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        4, this->_internal_rollingupgradestatusv2(), deterministic, target);
  }

  // optional uint64 fullBlockReportLeaseId = 5 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(5, this->fullblockreportleaseid(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.HeartbeatResponseProto)
  return target;
}

size_t HeartbeatResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
  if (has_hastatus()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *hastatus_);
  }
  // repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->cmds_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->cmds(static_cast<int>(i)));
    }
  }

  if (_has_bits_[0 / 32] & 14u) {
    // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
    if (has_rollingupgradestatus()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *rollingupgradestatus_);
    }

    // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
    if (has_rollingupgradestatusv2()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *rollingupgradestatusv2_);
    }

    // optional uint64 fullBlockReportLeaseId = 5 [default = 0];
    if (has_fullblockreportleaseid()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::UInt64Size(
          this->fullblockreportleaseid());
    }

  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void HeartbeatResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const HeartbeatResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const HeartbeatResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.HeartbeatResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.HeartbeatResponseProto)
    MergeFrom(*source);
  }
}

void HeartbeatResponseProto::MergeFrom(const HeartbeatResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cmds_.MergeFrom(from.cmds_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_hastatus()->::hadoop::hdfs::NNHAStatusHeartbeatProto::MergeFrom(from.hastatus());
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_rollingupgradestatus()->::hadoop::hdfs::RollingUpgradeStatusProto::MergeFrom(from.rollingupgradestatus());
    }
    if (cached_has_bits & 0x00000004u) {
      mutable_rollingupgradestatusv2()->::hadoop::hdfs::RollingUpgradeStatusProto::MergeFrom(from.rollingupgradestatusv2());
    }
    if (cached_has_bits & 0x00000008u) {
      fullblockreportleaseid_ = from.fullblockreportleaseid_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void HeartbeatResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void HeartbeatResponseProto::CopyFrom(const HeartbeatResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.HeartbeatResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool HeartbeatResponseProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->cmds())) return false;
  if (has_hastatus()) {
    if (!this->hastatus_->IsInitialized()) return false;
  }
  if (has_rollingupgradestatus()) {
    if (!this->rollingupgradestatus_->IsInitialized()) return false;
  }
  if (has_rollingupgradestatusv2()) {
    if (!this->rollingupgradestatusv2_->IsInitialized()) return false;
  }
  return true;
}

void HeartbeatResponseProto::Swap(HeartbeatResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void HeartbeatResponseProto::InternalSwap(HeartbeatResponseProto* other) {
  using std::swap;
  CastToBase(&cmds_)->InternalSwap(CastToBase(&other->cmds_));
  swap(hastatus_, other->hastatus_);
  swap(rollingupgradestatus_, other->rollingupgradestatus_);
  swap(rollingupgradestatusv2_, other->rollingupgradestatusv2_);
  swap(fullblockreportleaseid_, other->fullblockreportleaseid_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata HeartbeatResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockReportRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_BlockReportRequestProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
  ::hadoop::hdfs::datanode::_BlockReportRequestProto_default_instance_._instance.get_mutable()->context_ = const_cast< ::hadoop::hdfs::datanode::BlockReportContextProto*>(
      ::hadoop::hdfs::datanode::BlockReportContextProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockReportRequestProto::kRegistrationFieldNumber;
const int BlockReportRequestProto::kBlockPoolIdFieldNumber;
const int BlockReportRequestProto::kReportsFieldNumber;
const int BlockReportRequestProto::kContextFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockReportRequestProto::BlockReportRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockReportRequestProto)
}
BlockReportRequestProto::BlockReportRequestProto(const BlockReportRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      reports_(from.reports_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  if (from.has_context()) {
    context_ = new ::hadoop::hdfs::datanode::BlockReportContextProto(*from.context_);
  } else {
    context_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockReportRequestProto)
}

void BlockReportRequestProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&registration_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&context_) -
      reinterpret_cast<char*>(&registration_)) + sizeof(context_));
}

BlockReportRequestProto::~BlockReportRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockReportRequestProto)
  SharedDtor();
}

void BlockReportRequestProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete registration_;
  if (this != internal_default_instance()) delete context_;
}

void BlockReportRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockReportRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockReportRequestProto& BlockReportRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportRequestProto.base);
  return *internal_default_instance();
}


void BlockReportRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  reports_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      blockpoolid_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(registration_ != NULL);
      registration_->Clear();
    }
    if (cached_has_bits & 0x00000004u) {
      GOOGLE_DCHECK(context_ != NULL);
      context_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockReportRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string blockPoolId = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_reports()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_context()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockReportRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockReportRequestProto)
  return false;
#undef DO_
}

void BlockReportRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->blockpoolid(), output);
  }

  // repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->reports_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3,
      this->reports(static_cast<int>(i)),
      output);
  }

  // optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      4, this->_internal_context(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockReportRequestProto)
}

::google::protobuf::uint8* BlockReportRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->blockpoolid(), target);
  }

  // repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->reports_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->reports(static_cast<int>(i)), deterministic, target);
  }

  // optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        4, this->_internal_context(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockReportRequestProto)
  return target;
}

size_t BlockReportRequestProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  size_t total_size = 0;

  if (has_blockpoolid()) {
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }

  if (has_registration()) {
    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }

  return total_size;
}
size_t BlockReportRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());

    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
  {
    unsigned int count = static_cast<unsigned int>(this->reports_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->reports(static_cast<int>(i)));
    }
  }

  // optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
  if (has_context()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *context_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockReportRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockReportRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockReportRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockReportRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockReportRequestProto)
    MergeFrom(*source);
  }
}

void BlockReportRequestProto::MergeFrom(const BlockReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  reports_.MergeFrom(from.reports_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_blockpoolid();
      blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
    }
    if (cached_has_bits & 0x00000004u) {
      mutable_context()->::hadoop::hdfs::datanode::BlockReportContextProto::MergeFrom(from.context());
    }
  }
}

void BlockReportRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockReportRequestProto::CopyFrom(const BlockReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockReportRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->reports())) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  if (has_context()) {
    if (!this->context_->IsInitialized()) return false;
  }
  return true;
}

void BlockReportRequestProto::Swap(BlockReportRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockReportRequestProto::InternalSwap(BlockReportRequestProto* other) {
  using std::swap;
  CastToBase(&reports_)->InternalSwap(CastToBase(&other->reports_));
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(registration_, other->registration_);
  swap(context_, other->context_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockReportRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockReportContextProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockReportContextProto::kTotalRpcsFieldNumber;
const int BlockReportContextProto::kCurRpcFieldNumber;
const int BlockReportContextProto::kIdFieldNumber;
const int BlockReportContextProto::kLeaseIdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockReportContextProto::BlockReportContextProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportContextProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockReportContextProto)
}
BlockReportContextProto::BlockReportContextProto(const BlockReportContextProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::memcpy(&totalrpcs_, &from.totalrpcs_,
    static_cast<size_t>(reinterpret_cast<char*>(&leaseid_) -
    reinterpret_cast<char*>(&totalrpcs_)) + sizeof(leaseid_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockReportContextProto)
}

void BlockReportContextProto::SharedCtor() {
  ::memset(&totalrpcs_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&leaseid_) -
      reinterpret_cast<char*>(&totalrpcs_)) + sizeof(leaseid_));
}

BlockReportContextProto::~BlockReportContextProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockReportContextProto)
  SharedDtor();
}

void BlockReportContextProto::SharedDtor() {
}

void BlockReportContextProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockReportContextProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockReportContextProto& BlockReportContextProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportContextProto.base);
  return *internal_default_instance();
}


void BlockReportContextProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockReportContextProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 15u) {
    ::memset(&totalrpcs_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&leaseid_) -
        reinterpret_cast<char*>(&totalrpcs_)) + sizeof(leaseid_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockReportContextProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockReportContextProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required int32 totalRpcs = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(8u /* 8 & 0xFF */)) {
          set_has_totalrpcs();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &totalrpcs_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required int32 curRpc = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u /* 16 & 0xFF */)) {
          set_has_currpc();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int32, ::google::protobuf::internal::WireFormatLite::TYPE_INT32>(
                 input, &currpc_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required int64 id = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_id();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::int64, ::google::protobuf::internal::WireFormatLite::TYPE_INT64>(
                 input, &id_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint64 leaseId = 4 [default = 0];
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u /* 32 & 0xFF */)) {
          set_has_leaseid();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &leaseid_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockReportContextProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockReportContextProto)
  return false;
#undef DO_
}

void BlockReportContextProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockReportContextProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required int32 totalRpcs = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(1, this->totalrpcs(), output);
  }

  // required int32 curRpc = 2;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteInt32(2, this->currpc(), output);
  }

  // required int64 id = 3;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteInt64(3, this->id(), output);
  }

  // optional uint64 leaseId = 4 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(4, this->leaseid(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockReportContextProto)
}

::google::protobuf::uint8* BlockReportContextProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockReportContextProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required int32 totalRpcs = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteInt32ToArray(1, this->totalrpcs(), target);
  }

  // required int32 curRpc = 2;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteInt32ToArray(2, this->currpc(), target);
  }

  // required int64 id = 3;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteInt64ToArray(3, this->id(), target);
  }

  // optional uint64 leaseId = 4 [default = 0];
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(4, this->leaseid(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockReportContextProto)
  return target;
}

size_t BlockReportContextProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.BlockReportContextProto)
  size_t total_size = 0;

  if (has_totalrpcs()) {
    // required int32 totalRpcs = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->totalrpcs());
  }

  if (has_currpc()) {
    // required int32 curRpc = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->currpc());
  }

  if (has_id()) {
    // required int64 id = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->id());
  }

  return total_size;
}
size_t BlockReportContextProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockReportContextProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000007) ^ 0x00000007) == 0) {  // All required fields are present.
    // required int32 totalRpcs = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->totalrpcs());

    // required int32 curRpc = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int32Size(
        this->currpc());

    // required int64 id = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::Int64Size(
        this->id());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // optional uint64 leaseId = 4 [default = 0];
  if (has_leaseid()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->leaseid());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockReportContextProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockReportContextProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockReportContextProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockReportContextProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockReportContextProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockReportContextProto)
    MergeFrom(*source);
  }
}

void BlockReportContextProto::MergeFrom(const BlockReportContextProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockReportContextProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      totalrpcs_ = from.totalrpcs_;
    }
    if (cached_has_bits & 0x00000002u) {
      currpc_ = from.currpc_;
    }
    if (cached_has_bits & 0x00000004u) {
      id_ = from.id_;
    }
    if (cached_has_bits & 0x00000008u) {
      leaseid_ = from.leaseid_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void BlockReportContextProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockReportContextProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockReportContextProto::CopyFrom(const BlockReportContextProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockReportContextProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockReportContextProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000007) != 0x00000007) return false;
  return true;
}

void BlockReportContextProto::Swap(BlockReportContextProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockReportContextProto::InternalSwap(BlockReportContextProto* other) {
  using std::swap;
  swap(totalrpcs_, other->totalrpcs_);
  swap(currpc_, other->currpc_);
  swap(id_, other->id_);
  swap(leaseid_, other->leaseid_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockReportContextProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void StorageBlockReportProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_StorageBlockReportProto_default_instance_._instance.get_mutable()->storage_ = const_cast< ::hadoop::hdfs::DatanodeStorageProto*>(
      ::hadoop::hdfs::DatanodeStorageProto::internal_default_instance());
}
void StorageBlockReportProto::clear_storage() {
  if (storage_ != NULL) storage_->Clear();
  clear_has_storage();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int StorageBlockReportProto::kStorageFieldNumber;
const int StorageBlockReportProto::kBlocksFieldNumber;
const int StorageBlockReportProto::kNumberOfBlocksFieldNumber;
const int StorageBlockReportProto::kBlocksBuffersFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

StorageBlockReportProto::StorageBlockReportProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_StorageBlockReportProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.StorageBlockReportProto)
}
StorageBlockReportProto::StorageBlockReportProto(const StorageBlockReportProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_),
      blocksbuffers_(from.blocksbuffers_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_storage()) {
    storage_ = new ::hadoop::hdfs::DatanodeStorageProto(*from.storage_);
  } else {
    storage_ = NULL;
  }
  numberofblocks_ = from.numberofblocks_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.StorageBlockReportProto)
}

void StorageBlockReportProto::SharedCtor() {
  ::memset(&storage_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&numberofblocks_) -
      reinterpret_cast<char*>(&storage_)) + sizeof(numberofblocks_));
}

StorageBlockReportProto::~StorageBlockReportProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.StorageBlockReportProto)
  SharedDtor();
}

void StorageBlockReportProto::SharedDtor() {
  if (this != internal_default_instance()) delete storage_;
}

void StorageBlockReportProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* StorageBlockReportProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const StorageBlockReportProto& StorageBlockReportProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_StorageBlockReportProto.base);
  return *internal_default_instance();
}


void StorageBlockReportProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  blocksbuffers_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(storage_ != NULL);
    storage_->Clear();
  }
  numberofblocks_ = GOOGLE_ULONGLONG(0);
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool StorageBlockReportProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.DatanodeStorageProto storage = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_storage()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 blocks = 2 [packed = true];
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_blocks())));
        } else if (
            static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u /* 16 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 18u, input, this->mutable_blocks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional uint64 numberOfBlocks = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_numberofblocks();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &numberofblocks_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated bytes blocksBuffers = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(34u /* 34 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadBytes(
                input, this->add_blocksbuffers()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.StorageBlockReportProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.StorageBlockReportProto)
  return false;
#undef DO_
}

void StorageBlockReportProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.DatanodeStorageProto storage = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_storage(), output);
  }

  // repeated uint64 blocks = 2 [packed = true];
  if (this->blocks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(2, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(static_cast< ::google::protobuf::uint32>(
        _blocks_cached_byte_size_));
  }
  for (int i = 0, n = this->blocks_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->blocks(i), output);
  }

  // optional uint64 numberOfBlocks = 3;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->numberofblocks(), output);
  }

  // repeated bytes blocksBuffers = 4;
  for (int i = 0, n = this->blocksbuffers_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteBytes(
      4, this->blocksbuffers(i), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.StorageBlockReportProto)
}

::google::protobuf::uint8* StorageBlockReportProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.DatanodeStorageProto storage = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_storage(), deterministic, target);
  }

  // repeated uint64 blocks = 2 [packed = true];
  if (this->blocks_size() > 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteTagToArray(
      2,
      ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED,
      target);
    target = ::google::protobuf::io::CodedOutputStream::WriteVarint32ToArray(
        static_cast< ::google::protobuf::int32>(
            _blocks_cached_byte_size_), target);
    target = ::google::protobuf::internal::WireFormatLite::
      WriteUInt64NoTagToArray(this->blocks_, target);
  }

  // optional uint64 numberOfBlocks = 3;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(3, this->numberofblocks(), target);
  }

  // repeated bytes blocksBuffers = 4;
  for (int i = 0, n = this->blocksbuffers_size(); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      WriteBytesToArray(4, this->blocksbuffers(i), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.StorageBlockReportProto)
  return target;
}

size_t StorageBlockReportProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required .hadoop.hdfs.DatanodeStorageProto storage = 1;
  if (has_storage()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *storage_);
  }
  // repeated uint64 blocks = 2 [packed = true];
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->blocks_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
            static_cast< ::google::protobuf::int32>(data_size));
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _blocks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  // repeated bytes blocksBuffers = 4;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->blocksbuffers_size());
  for (int i = 0, n = this->blocksbuffers_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::BytesSize(
      this->blocksbuffers(i));
  }

  // optional uint64 numberOfBlocks = 3;
  if (has_numberofblocks()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->numberofblocks());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void StorageBlockReportProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  const StorageBlockReportProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const StorageBlockReportProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.StorageBlockReportProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.StorageBlockReportProto)
    MergeFrom(*source);
  }
}

void StorageBlockReportProto::MergeFrom(const StorageBlockReportProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
  blocksbuffers_.MergeFrom(from.blocksbuffers_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_storage()->::hadoop::hdfs::DatanodeStorageProto::MergeFrom(from.storage());
    }
    if (cached_has_bits & 0x00000002u) {
      numberofblocks_ = from.numberofblocks_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void StorageBlockReportProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void StorageBlockReportProto::CopyFrom(const StorageBlockReportProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.StorageBlockReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StorageBlockReportProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (has_storage()) {
    if (!this->storage_->IsInitialized()) return false;
  }
  return true;
}

void StorageBlockReportProto::Swap(StorageBlockReportProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void StorageBlockReportProto::InternalSwap(StorageBlockReportProto* other) {
  using std::swap;
  blocks_.InternalSwap(&other->blocks_);
  blocksbuffers_.InternalSwap(CastToBase(&other->blocksbuffers_));
  swap(storage_, other->storage_);
  swap(numberofblocks_, other->numberofblocks_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata StorageBlockReportProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockReportResponseProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_BlockReportResponseProto_default_instance_._instance.get_mutable()->cmd_ = const_cast< ::hadoop::hdfs::datanode::DatanodeCommandProto*>(
      ::hadoop::hdfs::datanode::DatanodeCommandProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockReportResponseProto::kCmdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockReportResponseProto::BlockReportResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockReportResponseProto)
}
BlockReportResponseProto::BlockReportResponseProto(const BlockReportResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_cmd()) {
    cmd_ = new ::hadoop::hdfs::datanode::DatanodeCommandProto(*from.cmd_);
  } else {
    cmd_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockReportResponseProto)
}

void BlockReportResponseProto::SharedCtor() {
  cmd_ = NULL;
}

BlockReportResponseProto::~BlockReportResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockReportResponseProto)
  SharedDtor();
}

void BlockReportResponseProto::SharedDtor() {
  if (this != internal_default_instance()) delete cmd_;
}

void BlockReportResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockReportResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockReportResponseProto& BlockReportResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockReportResponseProto.base);
  return *internal_default_instance();
}


void BlockReportResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(cmd_ != NULL);
    cmd_->Clear();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockReportResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_cmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockReportResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockReportResponseProto)
  return false;
#undef DO_
}

void BlockReportResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_cmd(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockReportResponseProto)
}

::google::protobuf::uint8* BlockReportResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_cmd(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockReportResponseProto)
  return target;
}

size_t BlockReportResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (has_cmd()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *cmd_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockReportResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockReportResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockReportResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockReportResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockReportResponseProto)
    MergeFrom(*source);
  }
}

void BlockReportResponseProto::MergeFrom(const BlockReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_cmd()) {
    mutable_cmd()->::hadoop::hdfs::datanode::DatanodeCommandProto::MergeFrom(from.cmd());
  }
}

void BlockReportResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockReportResponseProto::CopyFrom(const BlockReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockReportResponseProto::IsInitialized() const {
  if (has_cmd()) {
    if (!this->cmd_->IsInitialized()) return false;
  }
  return true;
}

void BlockReportResponseProto::Swap(BlockReportResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockReportResponseProto::InternalSwap(BlockReportResponseProto* other) {
  using std::swap;
  swap(cmd_, other->cmd_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockReportResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void CacheReportRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_CacheReportRequestProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CacheReportRequestProto::kRegistrationFieldNumber;
const int CacheReportRequestProto::kBlockPoolIdFieldNumber;
const int CacheReportRequestProto::kBlocksFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CacheReportRequestProto::CacheReportRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_CacheReportRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.CacheReportRequestProto)
}
CacheReportRequestProto::CacheReportRequestProto(const CacheReportRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.CacheReportRequestProto)
}

void CacheReportRequestProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  registration_ = NULL;
}

CacheReportRequestProto::~CacheReportRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.CacheReportRequestProto)
  SharedDtor();
}

void CacheReportRequestProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete registration_;
}

void CacheReportRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* CacheReportRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const CacheReportRequestProto& CacheReportRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_CacheReportRequestProto.base);
  return *internal_default_instance();
}


void CacheReportRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      blockpoolid_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(registration_ != NULL);
      registration_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool CacheReportRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string blockPoolId = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated uint64 blocks = 3 [packed = true];
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadPackedPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, this->mutable_blocks())));
        } else if (
            static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          DO_((::google::protobuf::internal::WireFormatLite::ReadRepeatedPrimitiveNoInline<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 1, 26u, input, this->mutable_blocks())));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.CacheReportRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.CacheReportRequestProto)
  return false;
#undef DO_
}

void CacheReportRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->blockpoolid(), output);
  }

  // repeated uint64 blocks = 3 [packed = true];
  if (this->blocks_size() > 0) {
    ::google::protobuf::internal::WireFormatLite::WriteTag(3, ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED, output);
    output->WriteVarint32(static_cast< ::google::protobuf::uint32>(
        _blocks_cached_byte_size_));
  }
  for (int i = 0, n = this->blocks_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64NoTag(
      this->blocks(i), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.CacheReportRequestProto)
}

::google::protobuf::uint8* CacheReportRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->blockpoolid(), target);
  }

  // repeated uint64 blocks = 3 [packed = true];
  if (this->blocks_size() > 0) {
    target = ::google::protobuf::internal::WireFormatLite::WriteTagToArray(
      3,
      ::google::protobuf::internal::WireFormatLite::WIRETYPE_LENGTH_DELIMITED,
      target);
    target = ::google::protobuf::io::CodedOutputStream::WriteVarint32ToArray(
        static_cast< ::google::protobuf::int32>(
            _blocks_cached_byte_size_), target);
    target = ::google::protobuf::internal::WireFormatLite::
      WriteUInt64NoTagToArray(this->blocks_, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.CacheReportRequestProto)
  return target;
}

size_t CacheReportRequestProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  size_t total_size = 0;

  if (has_blockpoolid()) {
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }

  if (has_registration()) {
    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }

  return total_size;
}
size_t CacheReportRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());

    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated uint64 blocks = 3 [packed = true];
  {
    size_t data_size = ::google::protobuf::internal::WireFormatLite::
      UInt64Size(this->blocks_);
    if (data_size > 0) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::Int32Size(
            static_cast< ::google::protobuf::int32>(data_size));
    }
    int cached_size = ::google::protobuf::internal::ToCachedSize(data_size);
    GOOGLE_SAFE_CONCURRENT_WRITES_BEGIN();
    _blocks_cached_byte_size_ = cached_size;
    GOOGLE_SAFE_CONCURRENT_WRITES_END();
    total_size += data_size;
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void CacheReportRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const CacheReportRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const CacheReportRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.CacheReportRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.CacheReportRequestProto)
    MergeFrom(*source);
  }
}

void CacheReportRequestProto::MergeFrom(const CacheReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_blockpoolid();
      blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
    }
  }
}

void CacheReportRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void CacheReportRequestProto::CopyFrom(const CacheReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.CacheReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CacheReportRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  return true;
}

void CacheReportRequestProto::Swap(CacheReportRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CacheReportRequestProto::InternalSwap(CacheReportRequestProto* other) {
  using std::swap;
  blocks_.InternalSwap(&other->blocks_);
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(registration_, other->registration_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata CacheReportRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void CacheReportResponseProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_CacheReportResponseProto_default_instance_._instance.get_mutable()->cmd_ = const_cast< ::hadoop::hdfs::datanode::DatanodeCommandProto*>(
      ::hadoop::hdfs::datanode::DatanodeCommandProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CacheReportResponseProto::kCmdFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CacheReportResponseProto::CacheReportResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_CacheReportResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.CacheReportResponseProto)
}
CacheReportResponseProto::CacheReportResponseProto(const CacheReportResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_cmd()) {
    cmd_ = new ::hadoop::hdfs::datanode::DatanodeCommandProto(*from.cmd_);
  } else {
    cmd_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.CacheReportResponseProto)
}

void CacheReportResponseProto::SharedCtor() {
  cmd_ = NULL;
}

CacheReportResponseProto::~CacheReportResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.CacheReportResponseProto)
  SharedDtor();
}

void CacheReportResponseProto::SharedDtor() {
  if (this != internal_default_instance()) delete cmd_;
}

void CacheReportResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* CacheReportResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const CacheReportResponseProto& CacheReportResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_CacheReportResponseProto.base);
  return *internal_default_instance();
}


void CacheReportResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(cmd_ != NULL);
    cmd_->Clear();
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool CacheReportResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_cmd()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.CacheReportResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.CacheReportResponseProto)
  return false;
#undef DO_
}

void CacheReportResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_cmd(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.CacheReportResponseProto)
}

::google::protobuf::uint8* CacheReportResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_cmd(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.CacheReportResponseProto)
  return target;
}

size_t CacheReportResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  if (has_cmd()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *cmd_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void CacheReportResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const CacheReportResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const CacheReportResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.CacheReportResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.CacheReportResponseProto)
    MergeFrom(*source);
  }
}

void CacheReportResponseProto::MergeFrom(const CacheReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_cmd()) {
    mutable_cmd()->::hadoop::hdfs::datanode::DatanodeCommandProto::MergeFrom(from.cmd());
  }
}

void CacheReportResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void CacheReportResponseProto::CopyFrom(const CacheReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.CacheReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CacheReportResponseProto::IsInitialized() const {
  if (has_cmd()) {
    if (!this->cmd_->IsInitialized()) return false;
  }
  return true;
}

void CacheReportResponseProto::Swap(CacheReportResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CacheReportResponseProto::InternalSwap(CacheReportResponseProto* other) {
  using std::swap;
  swap(cmd_, other->cmd_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata CacheReportResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void ReceivedDeletedBlockInfoProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_ReceivedDeletedBlockInfoProto_default_instance_._instance.get_mutable()->block_ = const_cast< ::hadoop::hdfs::BlockProto*>(
      ::hadoop::hdfs::BlockProto::internal_default_instance());
}
void ReceivedDeletedBlockInfoProto::clear_block() {
  if (block_ != NULL) block_->Clear();
  clear_has_block();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReceivedDeletedBlockInfoProto::kBlockFieldNumber;
const int ReceivedDeletedBlockInfoProto::kStatusFieldNumber;
const int ReceivedDeletedBlockInfoProto::kDeleteHintFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReceivedDeletedBlockInfoProto::ReceivedDeletedBlockInfoProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_ReceivedDeletedBlockInfoProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
}
ReceivedDeletedBlockInfoProto::ReceivedDeletedBlockInfoProto(const ReceivedDeletedBlockInfoProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  deletehint_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_deletehint()) {
    deletehint_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.deletehint_);
  }
  if (from.has_block()) {
    block_ = new ::hadoop::hdfs::BlockProto(*from.block_);
  } else {
    block_ = NULL;
  }
  status_ = from.status_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
}

void ReceivedDeletedBlockInfoProto::SharedCtor() {
  deletehint_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  block_ = NULL;
  status_ = 1;
}

ReceivedDeletedBlockInfoProto::~ReceivedDeletedBlockInfoProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  SharedDtor();
}

void ReceivedDeletedBlockInfoProto::SharedDtor() {
  deletehint_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete block_;
}

void ReceivedDeletedBlockInfoProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* ReceivedDeletedBlockInfoProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const ReceivedDeletedBlockInfoProto& ReceivedDeletedBlockInfoProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_ReceivedDeletedBlockInfoProto.base);
  return *internal_default_instance();
}


void ReceivedDeletedBlockInfoProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      deletehint_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(block_ != NULL);
      block_->Clear();
    }
    status_ = 1;
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool ReceivedDeletedBlockInfoProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.BlockProto block = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_block()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional string deleteHint = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_deletehint()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->deletehint().data(), static_cast<int>(this->deletehint().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          int value;
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   int, ::google::protobuf::internal::WireFormatLite::TYPE_ENUM>(
                 input, &value)));
          if (::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus_IsValid(value)) {
            set_status(static_cast< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus >(value));
          } else {
            mutable_unknown_fields()->AddVarint(
                3, static_cast< ::google::protobuf::uint64>(value));
          }
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  return false;
#undef DO_
}

void ReceivedDeletedBlockInfoProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.BlockProto block = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_block(), output);
  }

  // optional string deleteHint = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->deletehint().data(), static_cast<int>(this->deletehint().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->deletehint(), output);
  }

  // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteEnum(
      3, this->status(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
}

::google::protobuf::uint8* ReceivedDeletedBlockInfoProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.BlockProto block = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_block(), deterministic, target);
  }

  // optional string deleteHint = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->deletehint().data(), static_cast<int>(this->deletehint().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->deletehint(), target);
  }

  // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteEnumToArray(
      3, this->status(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  return target;
}

size_t ReceivedDeletedBlockInfoProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  size_t total_size = 0;

  if (has_block()) {
    // required .hadoop.hdfs.BlockProto block = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *block_);
  }

  if (has_status()) {
    // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->status());
  }

  return total_size;
}
size_t ReceivedDeletedBlockInfoProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000006) ^ 0x00000006) == 0) {  // All required fields are present.
    // required .hadoop.hdfs.BlockProto block = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *block_);

    // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::EnumSize(this->status());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // optional string deleteHint = 2;
  if (has_deletehint()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->deletehint());
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void ReceivedDeletedBlockInfoProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  GOOGLE_DCHECK_NE(&from, this);
  const ReceivedDeletedBlockInfoProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const ReceivedDeletedBlockInfoProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
    MergeFrom(*source);
  }
}

void ReceivedDeletedBlockInfoProto::MergeFrom(const ReceivedDeletedBlockInfoProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_deletehint();
      deletehint_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.deletehint_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_block()->::hadoop::hdfs::BlockProto::MergeFrom(from.block());
    }
    if (cached_has_bits & 0x00000004u) {
      status_ = from.status_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void ReceivedDeletedBlockInfoProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ReceivedDeletedBlockInfoProto::CopyFrom(const ReceivedDeletedBlockInfoProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReceivedDeletedBlockInfoProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000006) != 0x00000006) return false;
  if (has_block()) {
    if (!this->block_->IsInitialized()) return false;
  }
  return true;
}

void ReceivedDeletedBlockInfoProto::Swap(ReceivedDeletedBlockInfoProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReceivedDeletedBlockInfoProto::InternalSwap(ReceivedDeletedBlockInfoProto* other) {
  using std::swap;
  deletehint_.Swap(&other->deletehint_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(block_, other->block_);
  swap(status_, other->status_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata ReceivedDeletedBlockInfoProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void StorageReceivedDeletedBlocksProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_StorageReceivedDeletedBlocksProto_default_instance_._instance.get_mutable()->storage_ = const_cast< ::hadoop::hdfs::DatanodeStorageProto*>(
      ::hadoop::hdfs::DatanodeStorageProto::internal_default_instance());
}
void StorageReceivedDeletedBlocksProto::clear_storage() {
  if (storage_ != NULL) storage_->Clear();
  clear_has_storage();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int StorageReceivedDeletedBlocksProto::kStorageUuidFieldNumber;
const int StorageReceivedDeletedBlocksProto::kBlocksFieldNumber;
const int StorageReceivedDeletedBlocksProto::kStorageFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

StorageReceivedDeletedBlocksProto::StorageReceivedDeletedBlocksProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_StorageReceivedDeletedBlocksProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
}
StorageReceivedDeletedBlocksProto::StorageReceivedDeletedBlocksProto(const StorageReceivedDeletedBlocksProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  storageuuid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_storageuuid()) {
    storageuuid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.storageuuid_);
  }
  if (from.has_storage()) {
    storage_ = new ::hadoop::hdfs::DatanodeStorageProto(*from.storage_);
  } else {
    storage_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
}

void StorageReceivedDeletedBlocksProto::SharedCtor() {
  storageuuid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  storage_ = NULL;
}

StorageReceivedDeletedBlocksProto::~StorageReceivedDeletedBlocksProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  SharedDtor();
}

void StorageReceivedDeletedBlocksProto::SharedDtor() {
  storageuuid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete storage_;
}

void StorageReceivedDeletedBlocksProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* StorageReceivedDeletedBlocksProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const StorageReceivedDeletedBlocksProto& StorageReceivedDeletedBlocksProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_StorageReceivedDeletedBlocksProto.base);
  return *internal_default_instance();
}


void StorageReceivedDeletedBlocksProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      storageuuid_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(storage_ != NULL);
      storage_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool StorageReceivedDeletedBlocksProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required string storageUuid = 1 [deprecated = true];
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_storageuuid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->storageuuid().data(), static_cast<int>(this->storageuuid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blocks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_storage()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  return false;
#undef DO_
}

void StorageReceivedDeletedBlocksProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required string storageUuid = 1 [deprecated = true];
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->storageuuid().data(), static_cast<int>(this->storageuuid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->storageuuid(), output);
  }

  // repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2,
      this->blocks(static_cast<int>(i)),
      output);
  }

  // optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, this->_internal_storage(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
}

::google::protobuf::uint8* StorageReceivedDeletedBlocksProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required string storageUuid = 1 [deprecated = true];
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->storageuuid().data(), static_cast<int>(this->storageuuid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        1, this->storageuuid(), target);
  }

  // repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, this->blocks(static_cast<int>(i)), deterministic, target);
  }

  // optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->_internal_storage(), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  return target;
}

size_t StorageReceivedDeletedBlocksProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // required string storageUuid = 1 [deprecated = true];
  if (has_storageuuid()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->storageuuid());
  }
  // repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
  {
    unsigned int count = static_cast<unsigned int>(this->blocks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blocks(static_cast<int>(i)));
    }
  }

  // optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
  if (has_storage()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *storage_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void StorageReceivedDeletedBlocksProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  GOOGLE_DCHECK_NE(&from, this);
  const StorageReceivedDeletedBlocksProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const StorageReceivedDeletedBlocksProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
    MergeFrom(*source);
  }
}

void StorageReceivedDeletedBlocksProto::MergeFrom(const StorageReceivedDeletedBlocksProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_storageuuid();
      storageuuid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.storageuuid_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_storage()->::hadoop::hdfs::DatanodeStorageProto::MergeFrom(from.storage());
    }
  }
}

void StorageReceivedDeletedBlocksProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void StorageReceivedDeletedBlocksProto::CopyFrom(const StorageReceivedDeletedBlocksProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool StorageReceivedDeletedBlocksProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000001) != 0x00000001) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->blocks())) return false;
  if (has_storage()) {
    if (!this->storage_->IsInitialized()) return false;
  }
  return true;
}

void StorageReceivedDeletedBlocksProto::Swap(StorageReceivedDeletedBlocksProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void StorageReceivedDeletedBlocksProto::InternalSwap(StorageReceivedDeletedBlocksProto* other) {
  using std::swap;
  CastToBase(&blocks_)->InternalSwap(CastToBase(&other->blocks_));
  storageuuid_.Swap(&other->storageuuid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(storage_, other->storage_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata StorageReceivedDeletedBlocksProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockReceivedAndDeletedRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_BlockReceivedAndDeletedRequestProto_default_instance_._instance.get_mutable()->registration_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int BlockReceivedAndDeletedRequestProto::kRegistrationFieldNumber;
const int BlockReceivedAndDeletedRequestProto::kBlockPoolIdFieldNumber;
const int BlockReceivedAndDeletedRequestProto::kBlocksFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockReceivedAndDeletedRequestProto::BlockReceivedAndDeletedRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReceivedAndDeletedRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
}
BlockReceivedAndDeletedRequestProto::BlockReceivedAndDeletedRequestProto(const BlockReceivedAndDeletedRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_blockpoolid()) {
    blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
  }
  if (from.has_registration()) {
    registration_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registration_);
  } else {
    registration_ = NULL;
  }
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
}

void BlockReceivedAndDeletedRequestProto::SharedCtor() {
  blockpoolid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  registration_ = NULL;
}

BlockReceivedAndDeletedRequestProto::~BlockReceivedAndDeletedRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  SharedDtor();
}

void BlockReceivedAndDeletedRequestProto::SharedDtor() {
  blockpoolid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete registration_;
}

void BlockReceivedAndDeletedRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockReceivedAndDeletedRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockReceivedAndDeletedRequestProto& BlockReceivedAndDeletedRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockReceivedAndDeletedRequestProto.base);
  return *internal_default_instance();
}


void BlockReceivedAndDeletedRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      blockpoolid_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(registration_ != NULL);
      registration_->Clear();
    }
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockReceivedAndDeletedRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registration()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string blockPoolId = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(18u /* 18 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_blockpoolid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blocks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  return false;
#undef DO_
}

void BlockReceivedAndDeletedRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registration(), output);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->blockpoolid(), output);
  }

  // repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3,
      this->blocks(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
}

::google::protobuf::uint8* BlockReceivedAndDeletedRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registration(), deterministic, target);
  }

  // required string blockPoolId = 2;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->blockpoolid().data(), static_cast<int>(this->blockpoolid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->blockpoolid(), target);
  }

  // repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, this->blocks(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  return target;
}

size_t BlockReceivedAndDeletedRequestProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  size_t total_size = 0;

  if (has_blockpoolid()) {
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());
  }

  if (has_registration()) {
    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);
  }

  return total_size;
}
size_t BlockReceivedAndDeletedRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000003) ^ 0x00000003) == 0) {  // All required fields are present.
    // required string blockPoolId = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->blockpoolid());

    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registration_);

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
  {
    unsigned int count = static_cast<unsigned int>(this->blocks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blocks(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockReceivedAndDeletedRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockReceivedAndDeletedRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockReceivedAndDeletedRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
    MergeFrom(*source);
  }
}

void BlockReceivedAndDeletedRequestProto::MergeFrom(const BlockReceivedAndDeletedRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_blockpoolid();
      blockpoolid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.blockpoolid_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_registration()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registration());
    }
  }
}

void BlockReceivedAndDeletedRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockReceivedAndDeletedRequestProto::CopyFrom(const BlockReceivedAndDeletedRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockReceivedAndDeletedRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000003) != 0x00000003) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->blocks())) return false;
  if (has_registration()) {
    if (!this->registration_->IsInitialized()) return false;
  }
  return true;
}

void BlockReceivedAndDeletedRequestProto::Swap(BlockReceivedAndDeletedRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockReceivedAndDeletedRequestProto::InternalSwap(BlockReceivedAndDeletedRequestProto* other) {
  using std::swap;
  CastToBase(&blocks_)->InternalSwap(CastToBase(&other->blocks_));
  blockpoolid_.Swap(&other->blockpoolid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(registration_, other->registration_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockReceivedAndDeletedRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void BlockReceivedAndDeletedResponseProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

BlockReceivedAndDeletedResponseProto::BlockReceivedAndDeletedResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_BlockReceivedAndDeletedResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
}
BlockReceivedAndDeletedResponseProto::BlockReceivedAndDeletedResponseProto(const BlockReceivedAndDeletedResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
}

void BlockReceivedAndDeletedResponseProto::SharedCtor() {
}

BlockReceivedAndDeletedResponseProto::~BlockReceivedAndDeletedResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  SharedDtor();
}

void BlockReceivedAndDeletedResponseProto::SharedDtor() {
}

void BlockReceivedAndDeletedResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* BlockReceivedAndDeletedResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const BlockReceivedAndDeletedResponseProto& BlockReceivedAndDeletedResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_BlockReceivedAndDeletedResponseProto.base);
  return *internal_default_instance();
}


void BlockReceivedAndDeletedResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool BlockReceivedAndDeletedResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  return false;
#undef DO_
}

void BlockReceivedAndDeletedResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
}

::google::protobuf::uint8* BlockReceivedAndDeletedResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  return target;
}

size_t BlockReceivedAndDeletedResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void BlockReceivedAndDeletedResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const BlockReceivedAndDeletedResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const BlockReceivedAndDeletedResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
    MergeFrom(*source);
  }
}

void BlockReceivedAndDeletedResponseProto::MergeFrom(const BlockReceivedAndDeletedResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void BlockReceivedAndDeletedResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void BlockReceivedAndDeletedResponseProto::CopyFrom(const BlockReceivedAndDeletedResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool BlockReceivedAndDeletedResponseProto::IsInitialized() const {
  return true;
}

void BlockReceivedAndDeletedResponseProto::Swap(BlockReceivedAndDeletedResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void BlockReceivedAndDeletedResponseProto::InternalSwap(BlockReceivedAndDeletedResponseProto* other) {
  using std::swap;
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata BlockReceivedAndDeletedResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void ErrorReportRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_ErrorReportRequestProto_default_instance_._instance.get_mutable()->registartion_ = const_cast< ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      ::hadoop::hdfs::datanode::DatanodeRegistrationProto::internal_default_instance());
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ErrorReportRequestProto::kRegistartionFieldNumber;
const int ErrorReportRequestProto::kErrorCodeFieldNumber;
const int ErrorReportRequestProto::kMsgFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ErrorReportRequestProto::ErrorReportRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_ErrorReportRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.ErrorReportRequestProto)
}
ErrorReportRequestProto::ErrorReportRequestProto(const ErrorReportRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  msg_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_msg()) {
    msg_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.msg_);
  }
  if (from.has_registartion()) {
    registartion_ = new ::hadoop::hdfs::datanode::DatanodeRegistrationProto(*from.registartion_);
  } else {
    registartion_ = NULL;
  }
  errorcode_ = from.errorcode_;
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.ErrorReportRequestProto)
}

void ErrorReportRequestProto::SharedCtor() {
  msg_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&registartion_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&errorcode_) -
      reinterpret_cast<char*>(&registartion_)) + sizeof(errorcode_));
}

ErrorReportRequestProto::~ErrorReportRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.ErrorReportRequestProto)
  SharedDtor();
}

void ErrorReportRequestProto::SharedDtor() {
  msg_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete registartion_;
}

void ErrorReportRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* ErrorReportRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const ErrorReportRequestProto& ErrorReportRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_ErrorReportRequestProto.base);
  return *internal_default_instance();
}


void ErrorReportRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 3u) {
    if (cached_has_bits & 0x00000001u) {
      msg_.ClearNonDefaultToEmptyNoArena();
    }
    if (cached_has_bits & 0x00000002u) {
      GOOGLE_DCHECK(registartion_ != NULL);
      registartion_->Clear();
    }
  }
  errorcode_ = 0u;
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool ErrorReportRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_registartion()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required uint32 errorCode = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u /* 16 & 0xFF */)) {
          set_has_errorcode();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint32, ::google::protobuf::internal::WireFormatLite::TYPE_UINT32>(
                 input, &errorcode_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required string msg = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(26u /* 26 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_msg()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->msg().data(), static_cast<int>(this->msg().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.ErrorReportRequestProto.msg");
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.ErrorReportRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.ErrorReportRequestProto)
  return false;
#undef DO_
}

void ErrorReportRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_registartion(), output);
  }

  // required uint32 errorCode = 2;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt32(2, this->errorcode(), output);
  }

  // required string msg = 3;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->msg().data(), static_cast<int>(this->msg().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.ErrorReportRequestProto.msg");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      3, this->msg(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.ErrorReportRequestProto)
}

::google::protobuf::uint8* ErrorReportRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_registartion(), deterministic, target);
  }

  // required uint32 errorCode = 2;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt32ToArray(2, this->errorcode(), target);
  }

  // required string msg = 3;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->msg().data(), static_cast<int>(this->msg().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.ErrorReportRequestProto.msg");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        3, this->msg(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.ErrorReportRequestProto)
  return target;
}

size_t ErrorReportRequestProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  size_t total_size = 0;

  if (has_msg()) {
    // required string msg = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->msg());
  }

  if (has_registartion()) {
    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registartion_);
  }

  if (has_errorcode()) {
    // required uint32 errorCode = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt32Size(
        this->errorcode());
  }

  return total_size;
}
size_t ErrorReportRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x00000007) ^ 0x00000007) == 0) {  // All required fields are present.
    // required string msg = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->msg());

    // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *registartion_);

    // required uint32 errorCode = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt32Size(
        this->errorcode());

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void ErrorReportRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const ErrorReportRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const ErrorReportRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.ErrorReportRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.ErrorReportRequestProto)
    MergeFrom(*source);
  }
}

void ErrorReportRequestProto::MergeFrom(const ErrorReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 7u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_msg();
      msg_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.msg_);
    }
    if (cached_has_bits & 0x00000002u) {
      mutable_registartion()->::hadoop::hdfs::datanode::DatanodeRegistrationProto::MergeFrom(from.registartion());
    }
    if (cached_has_bits & 0x00000004u) {
      errorcode_ = from.errorcode_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void ErrorReportRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ErrorReportRequestProto::CopyFrom(const ErrorReportRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.ErrorReportRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ErrorReportRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x00000007) != 0x00000007) return false;
  if (has_registartion()) {
    if (!this->registartion_->IsInitialized()) return false;
  }
  return true;
}

void ErrorReportRequestProto::Swap(ErrorReportRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ErrorReportRequestProto::InternalSwap(ErrorReportRequestProto* other) {
  using std::swap;
  msg_.Swap(&other->msg_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(registartion_, other->registartion_);
  swap(errorcode_, other->errorcode_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata ErrorReportRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void ErrorReportResponseProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ErrorReportResponseProto::ErrorReportResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_ErrorReportResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.ErrorReportResponseProto)
}
ErrorReportResponseProto::ErrorReportResponseProto(const ErrorReportResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.ErrorReportResponseProto)
}

void ErrorReportResponseProto::SharedCtor() {
}

ErrorReportResponseProto::~ErrorReportResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.ErrorReportResponseProto)
  SharedDtor();
}

void ErrorReportResponseProto::SharedDtor() {
}

void ErrorReportResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* ErrorReportResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const ErrorReportResponseProto& ErrorReportResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_ErrorReportResponseProto.base);
  return *internal_default_instance();
}


void ErrorReportResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool ErrorReportResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.ErrorReportResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.ErrorReportResponseProto)
  return false;
#undef DO_
}

void ErrorReportResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.ErrorReportResponseProto)
}

::google::protobuf::uint8* ErrorReportResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.ErrorReportResponseProto)
  return target;
}

size_t ErrorReportResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void ErrorReportResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const ErrorReportResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const ErrorReportResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.ErrorReportResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.ErrorReportResponseProto)
    MergeFrom(*source);
  }
}

void ErrorReportResponseProto::MergeFrom(const ErrorReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ErrorReportResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ErrorReportResponseProto::CopyFrom(const ErrorReportResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.ErrorReportResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ErrorReportResponseProto::IsInitialized() const {
  return true;
}

void ErrorReportResponseProto::Swap(ErrorReportResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ErrorReportResponseProto::InternalSwap(ErrorReportResponseProto* other) {
  using std::swap;
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata ErrorReportResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void ReportBadBlocksRequestProto::InitAsDefaultInstance() {
}
void ReportBadBlocksRequestProto::clear_blocks() {
  blocks_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int ReportBadBlocksRequestProto::kBlocksFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReportBadBlocksRequestProto::ReportBadBlocksRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_ReportBadBlocksRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
}
ReportBadBlocksRequestProto::ReportBadBlocksRequestProto(const ReportBadBlocksRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      blocks_(from.blocks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
}

void ReportBadBlocksRequestProto::SharedCtor() {
}

ReportBadBlocksRequestProto::~ReportBadBlocksRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  SharedDtor();
}

void ReportBadBlocksRequestProto::SharedDtor() {
}

void ReportBadBlocksRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* ReportBadBlocksRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const ReportBadBlocksRequestProto& ReportBadBlocksRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_ReportBadBlocksRequestProto.base);
  return *internal_default_instance();
}


void ReportBadBlocksRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  blocks_.Clear();
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool ReportBadBlocksRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_blocks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  return false;
#undef DO_
}

void ReportBadBlocksRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->blocks(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
}

::google::protobuf::uint8* ReportBadBlocksRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->blocks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->blocks(static_cast<int>(i)), deterministic, target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  return target;
}

size_t ReportBadBlocksRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  // repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->blocks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->blocks(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void ReportBadBlocksRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const ReportBadBlocksRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const ReportBadBlocksRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
    MergeFrom(*source);
  }
}

void ReportBadBlocksRequestProto::MergeFrom(const ReportBadBlocksRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  blocks_.MergeFrom(from.blocks_);
}

void ReportBadBlocksRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ReportBadBlocksRequestProto::CopyFrom(const ReportBadBlocksRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReportBadBlocksRequestProto::IsInitialized() const {
  if (!::google::protobuf::internal::AllAreInitialized(this->blocks())) return false;
  return true;
}

void ReportBadBlocksRequestProto::Swap(ReportBadBlocksRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReportBadBlocksRequestProto::InternalSwap(ReportBadBlocksRequestProto* other) {
  using std::swap;
  CastToBase(&blocks_)->InternalSwap(CastToBase(&other->blocks_));
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata ReportBadBlocksRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void ReportBadBlocksResponseProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

ReportBadBlocksResponseProto::ReportBadBlocksResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_ReportBadBlocksResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
}
ReportBadBlocksResponseProto::ReportBadBlocksResponseProto(const ReportBadBlocksResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
}

void ReportBadBlocksResponseProto::SharedCtor() {
}

ReportBadBlocksResponseProto::~ReportBadBlocksResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  SharedDtor();
}

void ReportBadBlocksResponseProto::SharedDtor() {
}

void ReportBadBlocksResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* ReportBadBlocksResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const ReportBadBlocksResponseProto& ReportBadBlocksResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_ReportBadBlocksResponseProto.base);
  return *internal_default_instance();
}


void ReportBadBlocksResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool ReportBadBlocksResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  return false;
#undef DO_
}

void ReportBadBlocksResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
}

::google::protobuf::uint8* ReportBadBlocksResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  return target;
}

size_t ReportBadBlocksResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void ReportBadBlocksResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const ReportBadBlocksResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const ReportBadBlocksResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
    MergeFrom(*source);
  }
}

void ReportBadBlocksResponseProto::MergeFrom(const ReportBadBlocksResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void ReportBadBlocksResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void ReportBadBlocksResponseProto::CopyFrom(const ReportBadBlocksResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool ReportBadBlocksResponseProto::IsInitialized() const {
  return true;
}

void ReportBadBlocksResponseProto::Swap(ReportBadBlocksResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void ReportBadBlocksResponseProto::InternalSwap(ReportBadBlocksResponseProto* other) {
  using std::swap;
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata ReportBadBlocksResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void CommitBlockSynchronizationRequestProto::InitAsDefaultInstance() {
  ::hadoop::hdfs::datanode::_CommitBlockSynchronizationRequestProto_default_instance_._instance.get_mutable()->block_ = const_cast< ::hadoop::hdfs::ExtendedBlockProto*>(
      ::hadoop::hdfs::ExtendedBlockProto::internal_default_instance());
}
void CommitBlockSynchronizationRequestProto::clear_block() {
  if (block_ != NULL) block_->Clear();
  clear_has_block();
}
void CommitBlockSynchronizationRequestProto::clear_newtaragets() {
  newtaragets_.Clear();
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int CommitBlockSynchronizationRequestProto::kBlockFieldNumber;
const int CommitBlockSynchronizationRequestProto::kNewGenStampFieldNumber;
const int CommitBlockSynchronizationRequestProto::kNewLengthFieldNumber;
const int CommitBlockSynchronizationRequestProto::kCloseFileFieldNumber;
const int CommitBlockSynchronizationRequestProto::kDeleteBlockFieldNumber;
const int CommitBlockSynchronizationRequestProto::kNewTaragetsFieldNumber;
const int CommitBlockSynchronizationRequestProto::kNewTargetStoragesFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CommitBlockSynchronizationRequestProto::CommitBlockSynchronizationRequestProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_CommitBlockSynchronizationRequestProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
}
CommitBlockSynchronizationRequestProto::CommitBlockSynchronizationRequestProto(const CommitBlockSynchronizationRequestProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_),
      newtaragets_(from.newtaragets_),
      newtargetstorages_(from.newtargetstorages_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_block()) {
    block_ = new ::hadoop::hdfs::ExtendedBlockProto(*from.block_);
  } else {
    block_ = NULL;
  }
  ::memcpy(&newgenstamp_, &from.newgenstamp_,
    static_cast<size_t>(reinterpret_cast<char*>(&deleteblock_) -
    reinterpret_cast<char*>(&newgenstamp_)) + sizeof(deleteblock_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
}

void CommitBlockSynchronizationRequestProto::SharedCtor() {
  ::memset(&block_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&deleteblock_) -
      reinterpret_cast<char*>(&block_)) + sizeof(deleteblock_));
}

CommitBlockSynchronizationRequestProto::~CommitBlockSynchronizationRequestProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  SharedDtor();
}

void CommitBlockSynchronizationRequestProto::SharedDtor() {
  if (this != internal_default_instance()) delete block_;
}

void CommitBlockSynchronizationRequestProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* CommitBlockSynchronizationRequestProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const CommitBlockSynchronizationRequestProto& CommitBlockSynchronizationRequestProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_CommitBlockSynchronizationRequestProto.base);
  return *internal_default_instance();
}


void CommitBlockSynchronizationRequestProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  newtaragets_.Clear();
  newtargetstorages_.Clear();
  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    GOOGLE_DCHECK(block_ != NULL);
    block_->Clear();
  }
  if (cached_has_bits & 30u) {
    ::memset(&newgenstamp_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&deleteblock_) -
        reinterpret_cast<char*>(&newgenstamp_)) + sizeof(deleteblock_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool CommitBlockSynchronizationRequestProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // required .hadoop.hdfs.ExtendedBlockProto block = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_block()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required uint64 newGenStamp = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(16u /* 16 & 0xFF */)) {
          set_has_newgenstamp();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &newgenstamp_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required uint64 newLength = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(24u /* 24 & 0xFF */)) {
          set_has_newlength();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   ::google::protobuf::uint64, ::google::protobuf::internal::WireFormatLite::TYPE_UINT64>(
                 input, &newlength_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required bool closeFile = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(32u /* 32 & 0xFF */)) {
          set_has_closefile();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &closefile_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // required bool deleteBlock = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(40u /* 40 & 0xFF */)) {
          set_has_deleteblock();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   bool, ::google::protobuf::internal::WireFormatLite::TYPE_BOOL>(
                 input, &deleteblock_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
      case 6: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(50u /* 50 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_newtaragets()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // repeated string newTargetStorages = 7;
      case 7: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(58u /* 58 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->add_newtargetstorages()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->newtargetstorages(this->newtargetstorages_size() - 1).data(),
            static_cast<int>(this->newtargetstorages(this->newtargetstorages_size() - 1).length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages");
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  return false;
#undef DO_
}

void CommitBlockSynchronizationRequestProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.ExtendedBlockProto block = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, this->_internal_block(), output);
  }

  // required uint64 newGenStamp = 2;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(2, this->newgenstamp(), output);
  }

  // required uint64 newLength = 3;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteUInt64(3, this->newlength(), output);
  }

  // required bool closeFile = 4;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(4, this->closefile(), output);
  }

  // required bool deleteBlock = 5;
  if (cached_has_bits & 0x00000010u) {
    ::google::protobuf::internal::WireFormatLite::WriteBool(5, this->deleteblock(), output);
  }

  // repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->newtaragets_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      6,
      this->newtaragets(static_cast<int>(i)),
      output);
  }

  // repeated string newTargetStorages = 7;
  for (int i = 0, n = this->newtargetstorages_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->newtargetstorages(i).data(), static_cast<int>(this->newtargetstorages(i).length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages");
    ::google::protobuf::internal::WireFormatLite::WriteString(
      7, this->newtargetstorages(i), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
}

::google::protobuf::uint8* CommitBlockSynchronizationRequestProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // required .hadoop.hdfs.ExtendedBlockProto block = 1;
  if (cached_has_bits & 0x00000001u) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->_internal_block(), deterministic, target);
  }

  // required uint64 newGenStamp = 2;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(2, this->newgenstamp(), target);
  }

  // required uint64 newLength = 3;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteUInt64ToArray(3, this->newlength(), target);
  }

  // required bool closeFile = 4;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteBoolToArray(4, this->closefile(), target);
  }

  // required bool deleteBlock = 5;
  if (cached_has_bits & 0x00000010u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteBoolToArray(5, this->deleteblock(), target);
  }

  // repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->newtaragets_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        6, this->newtaragets(static_cast<int>(i)), deterministic, target);
  }

  // repeated string newTargetStorages = 7;
  for (int i = 0, n = this->newtargetstorages_size(); i < n; i++) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->newtargetstorages(i).data(), static_cast<int>(this->newtargetstorages(i).length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages");
    target = ::google::protobuf::internal::WireFormatLite::
      WriteStringToArray(7, this->newtargetstorages(i), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  return target;
}

size_t CommitBlockSynchronizationRequestProto::RequiredFieldsByteSizeFallback() const {
// @@protoc_insertion_point(required_fields_byte_size_fallback_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  size_t total_size = 0;

  if (has_block()) {
    // required .hadoop.hdfs.ExtendedBlockProto block = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *block_);
  }

  if (has_newgenstamp()) {
    // required uint64 newGenStamp = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->newgenstamp());
  }

  if (has_newlength()) {
    // required uint64 newLength = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->newlength());
  }

  if (has_closefile()) {
    // required bool closeFile = 4;
    total_size += 1 + 1;
  }

  if (has_deleteblock()) {
    // required bool deleteBlock = 5;
    total_size += 1 + 1;
  }

  return total_size;
}
size_t CommitBlockSynchronizationRequestProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (((_has_bits_[0] & 0x0000001f) ^ 0x0000001f) == 0) {  // All required fields are present.
    // required .hadoop.hdfs.ExtendedBlockProto block = 1;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *block_);

    // required uint64 newGenStamp = 2;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->newgenstamp());

    // required uint64 newLength = 3;
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::UInt64Size(
        this->newlength());

    // required bool closeFile = 4;
    total_size += 1 + 1;

    // required bool deleteBlock = 5;
    total_size += 1 + 1;

  } else {
    total_size += RequiredFieldsByteSizeFallback();
  }
  // repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
  {
    unsigned int count = static_cast<unsigned int>(this->newtaragets_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->newtaragets(static_cast<int>(i)));
    }
  }

  // repeated string newTargetStorages = 7;
  total_size += 1 *
      ::google::protobuf::internal::FromIntSize(this->newtargetstorages_size());
  for (int i = 0, n = this->newtargetstorages_size(); i < n; i++) {
    total_size += ::google::protobuf::internal::WireFormatLite::StringSize(
      this->newtargetstorages(i));
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void CommitBlockSynchronizationRequestProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  const CommitBlockSynchronizationRequestProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const CommitBlockSynchronizationRequestProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
    MergeFrom(*source);
  }
}

void CommitBlockSynchronizationRequestProto::MergeFrom(const CommitBlockSynchronizationRequestProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  newtaragets_.MergeFrom(from.newtaragets_);
  newtargetstorages_.MergeFrom(from.newtargetstorages_);
  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 31u) {
    if (cached_has_bits & 0x00000001u) {
      mutable_block()->::hadoop::hdfs::ExtendedBlockProto::MergeFrom(from.block());
    }
    if (cached_has_bits & 0x00000002u) {
      newgenstamp_ = from.newgenstamp_;
    }
    if (cached_has_bits & 0x00000004u) {
      newlength_ = from.newlength_;
    }
    if (cached_has_bits & 0x00000008u) {
      closefile_ = from.closefile_;
    }
    if (cached_has_bits & 0x00000010u) {
      deleteblock_ = from.deleteblock_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void CommitBlockSynchronizationRequestProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void CommitBlockSynchronizationRequestProto::CopyFrom(const CommitBlockSynchronizationRequestProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CommitBlockSynchronizationRequestProto::IsInitialized() const {
  if ((_has_bits_[0] & 0x0000001f) != 0x0000001f) return false;
  if (!::google::protobuf::internal::AllAreInitialized(this->newtaragets())) return false;
  if (has_block()) {
    if (!this->block_->IsInitialized()) return false;
  }
  return true;
}

void CommitBlockSynchronizationRequestProto::Swap(CommitBlockSynchronizationRequestProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CommitBlockSynchronizationRequestProto::InternalSwap(CommitBlockSynchronizationRequestProto* other) {
  using std::swap;
  CastToBase(&newtaragets_)->InternalSwap(CastToBase(&other->newtaragets_));
  newtargetstorages_.InternalSwap(CastToBase(&other->newtargetstorages_));
  swap(block_, other->block_);
  swap(newgenstamp_, other->newgenstamp_);
  swap(newlength_, other->newlength_);
  swap(closefile_, other->closefile_);
  swap(deleteblock_, other->deleteblock_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata CommitBlockSynchronizationRequestProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void CommitBlockSynchronizationResponseProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

CommitBlockSynchronizationResponseProto::CommitBlockSynchronizationResponseProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_CommitBlockSynchronizationResponseProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
}
CommitBlockSynchronizationResponseProto::CommitBlockSynchronizationResponseProto(const CommitBlockSynchronizationResponseProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
}

void CommitBlockSynchronizationResponseProto::SharedCtor() {
}

CommitBlockSynchronizationResponseProto::~CommitBlockSynchronizationResponseProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  SharedDtor();
}

void CommitBlockSynchronizationResponseProto::SharedDtor() {
}

void CommitBlockSynchronizationResponseProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* CommitBlockSynchronizationResponseProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const CommitBlockSynchronizationResponseProto& CommitBlockSynchronizationResponseProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_CommitBlockSynchronizationResponseProto.base);
  return *internal_default_instance();
}


void CommitBlockSynchronizationResponseProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool CommitBlockSynchronizationResponseProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
  handle_unusual:
    if (tag == 0) {
      goto success;
    }
    DO_(::google::protobuf::internal::WireFormat::SkipField(
          input, tag, _internal_metadata_.mutable_unknown_fields()));
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  return false;
#undef DO_
}

void CommitBlockSynchronizationResponseProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
}

::google::protobuf::uint8* CommitBlockSynchronizationResponseProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  return target;
}

size_t CommitBlockSynchronizationResponseProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void CommitBlockSynchronizationResponseProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  const CommitBlockSynchronizationResponseProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const CommitBlockSynchronizationResponseProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
    MergeFrom(*source);
  }
}

void CommitBlockSynchronizationResponseProto::MergeFrom(const CommitBlockSynchronizationResponseProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

}

void CommitBlockSynchronizationResponseProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void CommitBlockSynchronizationResponseProto::CopyFrom(const CommitBlockSynchronizationResponseProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool CommitBlockSynchronizationResponseProto::IsInitialized() const {
  return true;
}

void CommitBlockSynchronizationResponseProto::Swap(CommitBlockSynchronizationResponseProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void CommitBlockSynchronizationResponseProto::InternalSwap(CommitBlockSynchronizationResponseProto* other) {
  using std::swap;
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata CommitBlockSynchronizationResponseProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void SlowPeerReportProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SlowPeerReportProto::kDataNodeIdFieldNumber;
const int SlowPeerReportProto::kAggregateLatencyFieldNumber;
const int SlowPeerReportProto::kMedianFieldNumber;
const int SlowPeerReportProto::kMadFieldNumber;
const int SlowPeerReportProto::kUpperLimitLatencyFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SlowPeerReportProto::SlowPeerReportProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_SlowPeerReportProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.SlowPeerReportProto)
}
SlowPeerReportProto::SlowPeerReportProto(const SlowPeerReportProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  datanodeid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_datanodeid()) {
    datanodeid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.datanodeid_);
  }
  ::memcpy(&aggregatelatency_, &from.aggregatelatency_,
    static_cast<size_t>(reinterpret_cast<char*>(&upperlimitlatency_) -
    reinterpret_cast<char*>(&aggregatelatency_)) + sizeof(upperlimitlatency_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.SlowPeerReportProto)
}

void SlowPeerReportProto::SharedCtor() {
  datanodeid_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&aggregatelatency_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&upperlimitlatency_) -
      reinterpret_cast<char*>(&aggregatelatency_)) + sizeof(upperlimitlatency_));
}

SlowPeerReportProto::~SlowPeerReportProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.SlowPeerReportProto)
  SharedDtor();
}

void SlowPeerReportProto::SharedDtor() {
  datanodeid_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void SlowPeerReportProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* SlowPeerReportProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const SlowPeerReportProto& SlowPeerReportProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_SlowPeerReportProto.base);
  return *internal_default_instance();
}


void SlowPeerReportProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    datanodeid_.ClearNonDefaultToEmptyNoArena();
  }
  if (cached_has_bits & 30u) {
    ::memset(&aggregatelatency_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&upperlimitlatency_) -
        reinterpret_cast<char*>(&aggregatelatency_)) + sizeof(upperlimitlatency_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool SlowPeerReportProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional string dataNodeId = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_datanodeid()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->datanodeid().data(), static_cast<int>(this->datanodeid().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double aggregateLatency = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(17u /* 17 & 0xFF */)) {
          set_has_aggregatelatency();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &aggregatelatency_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double median = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(25u /* 25 & 0xFF */)) {
          set_has_median();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &median_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double mad = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(33u /* 33 & 0xFF */)) {
          set_has_mad();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &mad_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double upperLimitLatency = 5;
      case 5: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(41u /* 41 & 0xFF */)) {
          set_has_upperlimitlatency();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &upperlimitlatency_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.SlowPeerReportProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.SlowPeerReportProto)
  return false;
#undef DO_
}

void SlowPeerReportProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional string dataNodeId = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->datanodeid().data(), static_cast<int>(this->datanodeid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->datanodeid(), output);
  }

  // optional double aggregateLatency = 2;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(2, this->aggregatelatency(), output);
  }

  // optional double median = 3;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(3, this->median(), output);
  }

  // optional double mad = 4;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(4, this->mad(), output);
  }

  // optional double upperLimitLatency = 5;
  if (cached_has_bits & 0x00000010u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(5, this->upperlimitlatency(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.SlowPeerReportProto)
}

::google::protobuf::uint8* SlowPeerReportProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional string dataNodeId = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->datanodeid().data(), static_cast<int>(this->datanodeid().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        1, this->datanodeid(), target);
  }

  // optional double aggregateLatency = 2;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(2, this->aggregatelatency(), target);
  }

  // optional double median = 3;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(3, this->median(), target);
  }

  // optional double mad = 4;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(4, this->mad(), target);
  }

  // optional double upperLimitLatency = 5;
  if (cached_has_bits & 0x00000010u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(5, this->upperlimitlatency(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.SlowPeerReportProto)
  return target;
}

size_t SlowPeerReportProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (_has_bits_[0 / 32] & 31u) {
    // optional string dataNodeId = 1;
    if (has_datanodeid()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::StringSize(
          this->datanodeid());
    }

    // optional double aggregateLatency = 2;
    if (has_aggregatelatency()) {
      total_size += 1 + 8;
    }

    // optional double median = 3;
    if (has_median()) {
      total_size += 1 + 8;
    }

    // optional double mad = 4;
    if (has_mad()) {
      total_size += 1 + 8;
    }

    // optional double upperLimitLatency = 5;
    if (has_upperlimitlatency()) {
      total_size += 1 + 8;
    }

  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void SlowPeerReportProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  const SlowPeerReportProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const SlowPeerReportProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.SlowPeerReportProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.SlowPeerReportProto)
    MergeFrom(*source);
  }
}

void SlowPeerReportProto::MergeFrom(const SlowPeerReportProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 31u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_datanodeid();
      datanodeid_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.datanodeid_);
    }
    if (cached_has_bits & 0x00000002u) {
      aggregatelatency_ = from.aggregatelatency_;
    }
    if (cached_has_bits & 0x00000004u) {
      median_ = from.median_;
    }
    if (cached_has_bits & 0x00000008u) {
      mad_ = from.mad_;
    }
    if (cached_has_bits & 0x00000010u) {
      upperlimitlatency_ = from.upperlimitlatency_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void SlowPeerReportProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void SlowPeerReportProto::CopyFrom(const SlowPeerReportProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.SlowPeerReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SlowPeerReportProto::IsInitialized() const {
  return true;
}

void SlowPeerReportProto::Swap(SlowPeerReportProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SlowPeerReportProto::InternalSwap(SlowPeerReportProto* other) {
  using std::swap;
  datanodeid_.Swap(&other->datanodeid_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(aggregatelatency_, other->aggregatelatency_);
  swap(median_, other->median_);
  swap(mad_, other->mad_);
  swap(upperlimitlatency_, other->upperlimitlatency_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata SlowPeerReportProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// ===================================================================

void SlowDiskReportProto::InitAsDefaultInstance() {
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int SlowDiskReportProto::kBasePathFieldNumber;
const int SlowDiskReportProto::kMeanMetadataOpLatencyFieldNumber;
const int SlowDiskReportProto::kMeanReadIoLatencyFieldNumber;
const int SlowDiskReportProto::kMeanWriteIoLatencyFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

SlowDiskReportProto::SlowDiskReportProto()
  : ::google::protobuf::Message(), _internal_metadata_(NULL) {
  ::google::protobuf::internal::InitSCC(
      &protobuf_DatanodeProtocol_2eproto::scc_info_SlowDiskReportProto.base);
  SharedCtor();
  // @@protoc_insertion_point(constructor:hadoop.hdfs.datanode.SlowDiskReportProto)
}
SlowDiskReportProto::SlowDiskReportProto(const SlowDiskReportProto& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(NULL),
      _has_bits_(from._has_bits_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  basepath_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.has_basepath()) {
    basepath_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.basepath_);
  }
  ::memcpy(&meanmetadataoplatency_, &from.meanmetadataoplatency_,
    static_cast<size_t>(reinterpret_cast<char*>(&meanwriteiolatency_) -
    reinterpret_cast<char*>(&meanmetadataoplatency_)) + sizeof(meanwriteiolatency_));
  // @@protoc_insertion_point(copy_constructor:hadoop.hdfs.datanode.SlowDiskReportProto)
}

void SlowDiskReportProto::SharedCtor() {
  basepath_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  ::memset(&meanmetadataoplatency_, 0, static_cast<size_t>(
      reinterpret_cast<char*>(&meanwriteiolatency_) -
      reinterpret_cast<char*>(&meanmetadataoplatency_)) + sizeof(meanwriteiolatency_));
}

SlowDiskReportProto::~SlowDiskReportProto() {
  // @@protoc_insertion_point(destructor:hadoop.hdfs.datanode.SlowDiskReportProto)
  SharedDtor();
}

void SlowDiskReportProto::SharedDtor() {
  basepath_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}

void SlowDiskReportProto::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const ::google::protobuf::Descriptor* SlowDiskReportProto::descriptor() {
  ::protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages].descriptor;
}

const SlowDiskReportProto& SlowDiskReportProto::default_instance() {
  ::google::protobuf::internal::InitSCC(&protobuf_DatanodeProtocol_2eproto::scc_info_SlowDiskReportProto.base);
  return *internal_default_instance();
}


void SlowDiskReportProto::Clear() {
// @@protoc_insertion_point(message_clear_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  if (cached_has_bits & 0x00000001u) {
    basepath_.ClearNonDefaultToEmptyNoArena();
  }
  if (cached_has_bits & 14u) {
    ::memset(&meanmetadataoplatency_, 0, static_cast<size_t>(
        reinterpret_cast<char*>(&meanwriteiolatency_) -
        reinterpret_cast<char*>(&meanmetadataoplatency_)) + sizeof(meanwriteiolatency_));
  }
  _has_bits_.Clear();
  _internal_metadata_.Clear();
}

bool SlowDiskReportProto::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!GOOGLE_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // optional string basePath = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(10u /* 10 & 0xFF */)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_basepath()));
          ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
            this->basepath().data(), static_cast<int>(this->basepath().length()),
            ::google::protobuf::internal::WireFormat::PARSE,
            "hadoop.hdfs.datanode.SlowDiskReportProto.basePath");
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double meanMetadataOpLatency = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(17u /* 17 & 0xFF */)) {
          set_has_meanmetadataoplatency();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &meanmetadataoplatency_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double meanReadIoLatency = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(25u /* 25 & 0xFF */)) {
          set_has_meanreadiolatency();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &meanreadiolatency_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // optional double meanWriteIoLatency = 4;
      case 4: {
        if (static_cast< ::google::protobuf::uint8>(tag) ==
            static_cast< ::google::protobuf::uint8>(33u /* 33 & 0xFF */)) {
          set_has_meanwriteiolatency();
          DO_((::google::protobuf::internal::WireFormatLite::ReadPrimitive<
                   double, ::google::protobuf::internal::WireFormatLite::TYPE_DOUBLE>(
                 input, &meanwriteiolatency_)));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:hadoop.hdfs.datanode.SlowDiskReportProto)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:hadoop.hdfs.datanode.SlowDiskReportProto)
  return false;
#undef DO_
}

void SlowDiskReportProto::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional string basePath = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->basepath().data(), static_cast<int>(this->basepath().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.SlowDiskReportProto.basePath");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      1, this->basepath(), output);
  }

  // optional double meanMetadataOpLatency = 2;
  if (cached_has_bits & 0x00000002u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(2, this->meanmetadataoplatency(), output);
  }

  // optional double meanReadIoLatency = 3;
  if (cached_has_bits & 0x00000004u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(3, this->meanreadiolatency(), output);
  }

  // optional double meanWriteIoLatency = 4;
  if (cached_has_bits & 0x00000008u) {
    ::google::protobuf::internal::WireFormatLite::WriteDouble(4, this->meanwriteiolatency(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:hadoop.hdfs.datanode.SlowDiskReportProto)
}

::google::protobuf::uint8* SlowDiskReportProto::InternalSerializeWithCachedSizesToArray(
    bool deterministic, ::google::protobuf::uint8* target) const {
  (void)deterministic; // Unused
  // @@protoc_insertion_point(serialize_to_array_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = _has_bits_[0];
  // optional string basePath = 1;
  if (cached_has_bits & 0x00000001u) {
    ::google::protobuf::internal::WireFormat::VerifyUTF8StringNamedField(
      this->basepath().data(), static_cast<int>(this->basepath().length()),
      ::google::protobuf::internal::WireFormat::SERIALIZE,
      "hadoop.hdfs.datanode.SlowDiskReportProto.basePath");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        1, this->basepath(), target);
  }

  // optional double meanMetadataOpLatency = 2;
  if (cached_has_bits & 0x00000002u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(2, this->meanmetadataoplatency(), target);
  }

  // optional double meanReadIoLatency = 3;
  if (cached_has_bits & 0x00000004u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(3, this->meanreadiolatency(), target);
  }

  // optional double meanWriteIoLatency = 4;
  if (cached_has_bits & 0x00000008u) {
    target = ::google::protobuf::internal::WireFormatLite::WriteDoubleToArray(4, this->meanwriteiolatency(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:hadoop.hdfs.datanode.SlowDiskReportProto)
  return target;
}

size_t SlowDiskReportProto::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  if (_has_bits_[0 / 32] & 15u) {
    // optional string basePath = 1;
    if (has_basepath()) {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::StringSize(
          this->basepath());
    }

    // optional double meanMetadataOpLatency = 2;
    if (has_meanmetadataoplatency()) {
      total_size += 1 + 8;
    }

    // optional double meanReadIoLatency = 3;
    if (has_meanreadiolatency()) {
      total_size += 1 + 8;
    }

    // optional double meanWriteIoLatency = 4;
    if (has_meanwriteiolatency()) {
      total_size += 1 + 8;
    }

  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void SlowDiskReportProto::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  const SlowDiskReportProto* source =
      ::google::protobuf::internal::DynamicCastToGenerated<const SlowDiskReportProto>(
          &from);
  if (source == NULL) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:hadoop.hdfs.datanode.SlowDiskReportProto)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:hadoop.hdfs.datanode.SlowDiskReportProto)
    MergeFrom(*source);
  }
}

void SlowDiskReportProto::MergeFrom(const SlowDiskReportProto& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  cached_has_bits = from._has_bits_[0];
  if (cached_has_bits & 15u) {
    if (cached_has_bits & 0x00000001u) {
      set_has_basepath();
      basepath_.AssignWithDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.basepath_);
    }
    if (cached_has_bits & 0x00000002u) {
      meanmetadataoplatency_ = from.meanmetadataoplatency_;
    }
    if (cached_has_bits & 0x00000004u) {
      meanreadiolatency_ = from.meanreadiolatency_;
    }
    if (cached_has_bits & 0x00000008u) {
      meanwriteiolatency_ = from.meanwriteiolatency_;
    }
    _has_bits_[0] |= cached_has_bits;
  }
}

void SlowDiskReportProto::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void SlowDiskReportProto::CopyFrom(const SlowDiskReportProto& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:hadoop.hdfs.datanode.SlowDiskReportProto)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool SlowDiskReportProto::IsInitialized() const {
  return true;
}

void SlowDiskReportProto::Swap(SlowDiskReportProto* other) {
  if (other == this) return;
  InternalSwap(other);
}
void SlowDiskReportProto::InternalSwap(SlowDiskReportProto* other) {
  using std::swap;
  basepath_.Swap(&other->basepath_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(meanmetadataoplatency_, other->meanmetadataoplatency_);
  swap(meanreadiolatency_, other->meanreadiolatency_);
  swap(meanwriteiolatency_, other->meanwriteiolatency_);
  swap(_has_bits_[0], other->_has_bits_[0]);
  _internal_metadata_.Swap(&other->_internal_metadata_);
}

::google::protobuf::Metadata SlowDiskReportProto::GetMetadata() const {
  protobuf_DatanodeProtocol_2eproto::protobuf_AssignDescriptorsOnce();
  return ::protobuf_DatanodeProtocol_2eproto::file_level_metadata[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace datanode
}  // namespace hdfs
}  // namespace hadoop
namespace google {
namespace protobuf {
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::DatanodeRegistrationProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::DatanodeRegistrationProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::DatanodeRegistrationProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::DatanodeCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::DatanodeCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::DatanodeCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockIdCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockIdCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockIdCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockRecoveryCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockRecoveryCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::FinalizeCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::FinalizeCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::FinalizeCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::KeyUpdateCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::KeyUpdateCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::KeyUpdateCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::RegisterCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::RegisterCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::RegisterCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::VolumeFailureSummaryProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::VolumeFailureSummaryProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::HeartbeatRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::HeartbeatRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::HeartbeatRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::HeartbeatResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::HeartbeatResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::HeartbeatResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockReportRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockReportRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockReportRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockReportContextProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockReportContextProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockReportContextProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::StorageBlockReportProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::StorageBlockReportProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::StorageBlockReportProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockReportResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockReportResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockReportResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::CacheReportRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::CacheReportRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::CacheReportRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::CacheReportResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::CacheReportResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::CacheReportResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::ErrorReportRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::ErrorReportRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::ErrorReportRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::ErrorReportResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::ErrorReportResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::ErrorReportResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::SlowPeerReportProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::SlowPeerReportProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::SlowPeerReportProto >(arena);
}
template<> GOOGLE_PROTOBUF_ATTRIBUTE_NOINLINE ::hadoop::hdfs::datanode::SlowDiskReportProto* Arena::CreateMaybeMessage< ::hadoop::hdfs::datanode::SlowDiskReportProto >(Arena* arena) {
  return Arena::CreateInternal< ::hadoop::hdfs::datanode::SlowDiskReportProto >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
