// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: DatanodeProtocol.proto

#ifndef PROTOBUF_INCLUDED_DatanodeProtocol_2eproto
#define PROTOBUF_INCLUDED_DatanodeProtocol_2eproto

#include <string>

#include <google/protobuf/stubs/common.h>

#if GOOGLE_PROTOBUF_VERSION < 3006001
#error This file was generated by a newer version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please update
#error your headers.
#endif
#if 3006001 < GOOGLE_PROTOBUF_MIN_PROTOC_VERSION
#error This file was generated by an older version of protoc which is
#error incompatible with your Protocol Buffer headers.  Please
#error regenerate this file with a newer version of protoc.
#endif

#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/arena.h>
#include <google/protobuf/arenastring.h>
#include <google/protobuf/generated_message_table_driven.h>
#include <google/protobuf/generated_message_util.h>
#include <google/protobuf/inlined_string_field.h>
#include <google/protobuf/metadata.h>
#include <google/protobuf/message.h>
#include <google/protobuf/repeated_field.h>  // IWYU pragma: export
#include <google/protobuf/extension_set.h>  // IWYU pragma: export
#include <google/protobuf/generated_enum_reflection.h>
#include <google/protobuf/unknown_field_set.h>
#include "hdfs.pb.h"
#include "erasurecoding.pb.h"
#include "HdfsServer.pb.h"
// @@protoc_insertion_point(includes)
#define PROTOBUF_INTERNAL_EXPORT_protobuf_DatanodeProtocol_2eproto 

namespace protobuf_DatanodeProtocol_2eproto {
// Internal implementation detail -- do not use these members.
struct TableStruct {
  static const ::google::protobuf::internal::ParseTableField entries[];
  static const ::google::protobuf::internal::AuxillaryParseTableField aux[];
  static const ::google::protobuf::internal::ParseTable schema[33];
  static const ::google::protobuf::internal::FieldMetadata field_metadata[];
  static const ::google::protobuf::internal::SerializationTable serialization_table[];
  static const ::google::protobuf::uint32 offsets[];
};
void AddDescriptors();
}  // namespace protobuf_DatanodeProtocol_2eproto
namespace hadoop {
namespace hdfs {
namespace datanode {
class BalancerBandwidthCommandProto;
class BalancerBandwidthCommandProtoDefaultTypeInternal;
extern BalancerBandwidthCommandProtoDefaultTypeInternal _BalancerBandwidthCommandProto_default_instance_;
class BlockCommandProto;
class BlockCommandProtoDefaultTypeInternal;
extern BlockCommandProtoDefaultTypeInternal _BlockCommandProto_default_instance_;
class BlockECReconstructionCommandProto;
class BlockECReconstructionCommandProtoDefaultTypeInternal;
extern BlockECReconstructionCommandProtoDefaultTypeInternal _BlockECReconstructionCommandProto_default_instance_;
class BlockIdCommandProto;
class BlockIdCommandProtoDefaultTypeInternal;
extern BlockIdCommandProtoDefaultTypeInternal _BlockIdCommandProto_default_instance_;
class BlockReceivedAndDeletedRequestProto;
class BlockReceivedAndDeletedRequestProtoDefaultTypeInternal;
extern BlockReceivedAndDeletedRequestProtoDefaultTypeInternal _BlockReceivedAndDeletedRequestProto_default_instance_;
class BlockReceivedAndDeletedResponseProto;
class BlockReceivedAndDeletedResponseProtoDefaultTypeInternal;
extern BlockReceivedAndDeletedResponseProtoDefaultTypeInternal _BlockReceivedAndDeletedResponseProto_default_instance_;
class BlockRecoveryCommandProto;
class BlockRecoveryCommandProtoDefaultTypeInternal;
extern BlockRecoveryCommandProtoDefaultTypeInternal _BlockRecoveryCommandProto_default_instance_;
class BlockReportContextProto;
class BlockReportContextProtoDefaultTypeInternal;
extern BlockReportContextProtoDefaultTypeInternal _BlockReportContextProto_default_instance_;
class BlockReportRequestProto;
class BlockReportRequestProtoDefaultTypeInternal;
extern BlockReportRequestProtoDefaultTypeInternal _BlockReportRequestProto_default_instance_;
class BlockReportResponseProto;
class BlockReportResponseProtoDefaultTypeInternal;
extern BlockReportResponseProtoDefaultTypeInternal _BlockReportResponseProto_default_instance_;
class CacheReportRequestProto;
class CacheReportRequestProtoDefaultTypeInternal;
extern CacheReportRequestProtoDefaultTypeInternal _CacheReportRequestProto_default_instance_;
class CacheReportResponseProto;
class CacheReportResponseProtoDefaultTypeInternal;
extern CacheReportResponseProtoDefaultTypeInternal _CacheReportResponseProto_default_instance_;
class CommitBlockSynchronizationRequestProto;
class CommitBlockSynchronizationRequestProtoDefaultTypeInternal;
extern CommitBlockSynchronizationRequestProtoDefaultTypeInternal _CommitBlockSynchronizationRequestProto_default_instance_;
class CommitBlockSynchronizationResponseProto;
class CommitBlockSynchronizationResponseProtoDefaultTypeInternal;
extern CommitBlockSynchronizationResponseProtoDefaultTypeInternal _CommitBlockSynchronizationResponseProto_default_instance_;
class DatanodeCommandProto;
class DatanodeCommandProtoDefaultTypeInternal;
extern DatanodeCommandProtoDefaultTypeInternal _DatanodeCommandProto_default_instance_;
class DatanodeRegistrationProto;
class DatanodeRegistrationProtoDefaultTypeInternal;
extern DatanodeRegistrationProtoDefaultTypeInternal _DatanodeRegistrationProto_default_instance_;
class ErrorReportRequestProto;
class ErrorReportRequestProtoDefaultTypeInternal;
extern ErrorReportRequestProtoDefaultTypeInternal _ErrorReportRequestProto_default_instance_;
class ErrorReportResponseProto;
class ErrorReportResponseProtoDefaultTypeInternal;
extern ErrorReportResponseProtoDefaultTypeInternal _ErrorReportResponseProto_default_instance_;
class FinalizeCommandProto;
class FinalizeCommandProtoDefaultTypeInternal;
extern FinalizeCommandProtoDefaultTypeInternal _FinalizeCommandProto_default_instance_;
class HeartbeatRequestProto;
class HeartbeatRequestProtoDefaultTypeInternal;
extern HeartbeatRequestProtoDefaultTypeInternal _HeartbeatRequestProto_default_instance_;
class HeartbeatResponseProto;
class HeartbeatResponseProtoDefaultTypeInternal;
extern HeartbeatResponseProtoDefaultTypeInternal _HeartbeatResponseProto_default_instance_;
class KeyUpdateCommandProto;
class KeyUpdateCommandProtoDefaultTypeInternal;
extern KeyUpdateCommandProtoDefaultTypeInternal _KeyUpdateCommandProto_default_instance_;
class ReceivedDeletedBlockInfoProto;
class ReceivedDeletedBlockInfoProtoDefaultTypeInternal;
extern ReceivedDeletedBlockInfoProtoDefaultTypeInternal _ReceivedDeletedBlockInfoProto_default_instance_;
class RegisterCommandProto;
class RegisterCommandProtoDefaultTypeInternal;
extern RegisterCommandProtoDefaultTypeInternal _RegisterCommandProto_default_instance_;
class RegisterDatanodeRequestProto;
class RegisterDatanodeRequestProtoDefaultTypeInternal;
extern RegisterDatanodeRequestProtoDefaultTypeInternal _RegisterDatanodeRequestProto_default_instance_;
class RegisterDatanodeResponseProto;
class RegisterDatanodeResponseProtoDefaultTypeInternal;
extern RegisterDatanodeResponseProtoDefaultTypeInternal _RegisterDatanodeResponseProto_default_instance_;
class ReportBadBlocksRequestProto;
class ReportBadBlocksRequestProtoDefaultTypeInternal;
extern ReportBadBlocksRequestProtoDefaultTypeInternal _ReportBadBlocksRequestProto_default_instance_;
class ReportBadBlocksResponseProto;
class ReportBadBlocksResponseProtoDefaultTypeInternal;
extern ReportBadBlocksResponseProtoDefaultTypeInternal _ReportBadBlocksResponseProto_default_instance_;
class SlowDiskReportProto;
class SlowDiskReportProtoDefaultTypeInternal;
extern SlowDiskReportProtoDefaultTypeInternal _SlowDiskReportProto_default_instance_;
class SlowPeerReportProto;
class SlowPeerReportProtoDefaultTypeInternal;
extern SlowPeerReportProtoDefaultTypeInternal _SlowPeerReportProto_default_instance_;
class StorageBlockReportProto;
class StorageBlockReportProtoDefaultTypeInternal;
extern StorageBlockReportProtoDefaultTypeInternal _StorageBlockReportProto_default_instance_;
class StorageReceivedDeletedBlocksProto;
class StorageReceivedDeletedBlocksProtoDefaultTypeInternal;
extern StorageReceivedDeletedBlocksProtoDefaultTypeInternal _StorageReceivedDeletedBlocksProto_default_instance_;
class VolumeFailureSummaryProto;
class VolumeFailureSummaryProtoDefaultTypeInternal;
extern VolumeFailureSummaryProtoDefaultTypeInternal _VolumeFailureSummaryProto_default_instance_;
}  // namespace datanode
}  // namespace hdfs
}  // namespace hadoop
namespace google {
namespace protobuf {
template<> ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BalancerBandwidthCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockECReconstructionCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockIdCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockIdCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReceivedAndDeletedRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReceivedAndDeletedResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockRecoveryCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockReportContextProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReportContextProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockReportRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReportRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::BlockReportResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReportResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::CacheReportRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::CacheReportRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::CacheReportResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::CacheReportResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::CommitBlockSynchronizationRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::CommitBlockSynchronizationResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::DatanodeCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::DatanodeRegistrationProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(Arena*);
template<> ::hadoop::hdfs::datanode::ErrorReportRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::ErrorReportRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::ErrorReportResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::ErrorReportResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::FinalizeCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::FinalizeCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::HeartbeatRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::HeartbeatRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::HeartbeatResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::HeartbeatResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::KeyUpdateCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::KeyUpdateCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto>(Arena*);
template<> ::hadoop::hdfs::datanode::RegisterCommandProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::RegisterCommandProto>(Arena*);
template<> ::hadoop::hdfs::datanode::RegisterDatanodeRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::RegisterDatanodeRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::RegisterDatanodeResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::RegisterDatanodeResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::ReportBadBlocksRequestProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::ReportBadBlocksRequestProto>(Arena*);
template<> ::hadoop::hdfs::datanode::ReportBadBlocksResponseProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::ReportBadBlocksResponseProto>(Arena*);
template<> ::hadoop::hdfs::datanode::SlowDiskReportProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::SlowDiskReportProto>(Arena*);
template<> ::hadoop::hdfs::datanode::SlowPeerReportProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::SlowPeerReportProto>(Arena*);
template<> ::hadoop::hdfs::datanode::StorageBlockReportProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::StorageBlockReportProto>(Arena*);
template<> ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto>(Arena*);
template<> ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* Arena::CreateMaybeMessage<::hadoop::hdfs::datanode::VolumeFailureSummaryProto>(Arena*);
}  // namespace protobuf
}  // namespace google
namespace hadoop {
namespace hdfs {
namespace datanode {

enum DatanodeCommandProto_Type {
  DatanodeCommandProto_Type_BalancerBandwidthCommand = 0,
  DatanodeCommandProto_Type_BlockCommand = 1,
  DatanodeCommandProto_Type_BlockRecoveryCommand = 2,
  DatanodeCommandProto_Type_FinalizeCommand = 3,
  DatanodeCommandProto_Type_KeyUpdateCommand = 4,
  DatanodeCommandProto_Type_RegisterCommand = 5,
  DatanodeCommandProto_Type_UnusedUpgradeCommand = 6,
  DatanodeCommandProto_Type_NullDatanodeCommand = 7,
  DatanodeCommandProto_Type_BlockIdCommand = 8,
  DatanodeCommandProto_Type_BlockECReconstructionCommand = 9
};
bool DatanodeCommandProto_Type_IsValid(int value);
const DatanodeCommandProto_Type DatanodeCommandProto_Type_Type_MIN = DatanodeCommandProto_Type_BalancerBandwidthCommand;
const DatanodeCommandProto_Type DatanodeCommandProto_Type_Type_MAX = DatanodeCommandProto_Type_BlockECReconstructionCommand;
const int DatanodeCommandProto_Type_Type_ARRAYSIZE = DatanodeCommandProto_Type_Type_MAX + 1;

const ::google::protobuf::EnumDescriptor* DatanodeCommandProto_Type_descriptor();
inline const ::std::string& DatanodeCommandProto_Type_Name(DatanodeCommandProto_Type value) {
  return ::google::protobuf::internal::NameOfEnum(
    DatanodeCommandProto_Type_descriptor(), value);
}
inline bool DatanodeCommandProto_Type_Parse(
    const ::std::string& name, DatanodeCommandProto_Type* value) {
  return ::google::protobuf::internal::ParseNamedEnum<DatanodeCommandProto_Type>(
    DatanodeCommandProto_Type_descriptor(), name, value);
}
enum BlockCommandProto_Action {
  BlockCommandProto_Action_TRANSFER = 1,
  BlockCommandProto_Action_INVALIDATE = 2,
  BlockCommandProto_Action_SHUTDOWN = 3
};
bool BlockCommandProto_Action_IsValid(int value);
const BlockCommandProto_Action BlockCommandProto_Action_Action_MIN = BlockCommandProto_Action_TRANSFER;
const BlockCommandProto_Action BlockCommandProto_Action_Action_MAX = BlockCommandProto_Action_SHUTDOWN;
const int BlockCommandProto_Action_Action_ARRAYSIZE = BlockCommandProto_Action_Action_MAX + 1;

const ::google::protobuf::EnumDescriptor* BlockCommandProto_Action_descriptor();
inline const ::std::string& BlockCommandProto_Action_Name(BlockCommandProto_Action value) {
  return ::google::protobuf::internal::NameOfEnum(
    BlockCommandProto_Action_descriptor(), value);
}
inline bool BlockCommandProto_Action_Parse(
    const ::std::string& name, BlockCommandProto_Action* value) {
  return ::google::protobuf::internal::ParseNamedEnum<BlockCommandProto_Action>(
    BlockCommandProto_Action_descriptor(), name, value);
}
enum BlockIdCommandProto_Action {
  BlockIdCommandProto_Action_CACHE = 1,
  BlockIdCommandProto_Action_UNCACHE = 2
};
bool BlockIdCommandProto_Action_IsValid(int value);
const BlockIdCommandProto_Action BlockIdCommandProto_Action_Action_MIN = BlockIdCommandProto_Action_CACHE;
const BlockIdCommandProto_Action BlockIdCommandProto_Action_Action_MAX = BlockIdCommandProto_Action_UNCACHE;
const int BlockIdCommandProto_Action_Action_ARRAYSIZE = BlockIdCommandProto_Action_Action_MAX + 1;

const ::google::protobuf::EnumDescriptor* BlockIdCommandProto_Action_descriptor();
inline const ::std::string& BlockIdCommandProto_Action_Name(BlockIdCommandProto_Action value) {
  return ::google::protobuf::internal::NameOfEnum(
    BlockIdCommandProto_Action_descriptor(), value);
}
inline bool BlockIdCommandProto_Action_Parse(
    const ::std::string& name, BlockIdCommandProto_Action* value) {
  return ::google::protobuf::internal::ParseNamedEnum<BlockIdCommandProto_Action>(
    BlockIdCommandProto_Action_descriptor(), name, value);
}
enum ReceivedDeletedBlockInfoProto_BlockStatus {
  ReceivedDeletedBlockInfoProto_BlockStatus_RECEIVING = 1,
  ReceivedDeletedBlockInfoProto_BlockStatus_RECEIVED = 2,
  ReceivedDeletedBlockInfoProto_BlockStatus_DELETED = 3
};
bool ReceivedDeletedBlockInfoProto_BlockStatus_IsValid(int value);
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_MIN = ReceivedDeletedBlockInfoProto_BlockStatus_RECEIVING;
const ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_MAX = ReceivedDeletedBlockInfoProto_BlockStatus_DELETED;
const int ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_ARRAYSIZE = ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_MAX + 1;

const ::google::protobuf::EnumDescriptor* ReceivedDeletedBlockInfoProto_BlockStatus_descriptor();
inline const ::std::string& ReceivedDeletedBlockInfoProto_BlockStatus_Name(ReceivedDeletedBlockInfoProto_BlockStatus value) {
  return ::google::protobuf::internal::NameOfEnum(
    ReceivedDeletedBlockInfoProto_BlockStatus_descriptor(), value);
}
inline bool ReceivedDeletedBlockInfoProto_BlockStatus_Parse(
    const ::std::string& name, ReceivedDeletedBlockInfoProto_BlockStatus* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ReceivedDeletedBlockInfoProto_BlockStatus>(
    ReceivedDeletedBlockInfoProto_BlockStatus_descriptor(), name, value);
}
enum ErrorReportRequestProto_ErrorCode {
  ErrorReportRequestProto_ErrorCode_NOTIFY = 0,
  ErrorReportRequestProto_ErrorCode_DISK_ERROR = 1,
  ErrorReportRequestProto_ErrorCode_INVALID_BLOCK = 2,
  ErrorReportRequestProto_ErrorCode_FATAL_DISK_ERROR = 3
};
bool ErrorReportRequestProto_ErrorCode_IsValid(int value);
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto_ErrorCode_ErrorCode_MIN = ErrorReportRequestProto_ErrorCode_NOTIFY;
const ErrorReportRequestProto_ErrorCode ErrorReportRequestProto_ErrorCode_ErrorCode_MAX = ErrorReportRequestProto_ErrorCode_FATAL_DISK_ERROR;
const int ErrorReportRequestProto_ErrorCode_ErrorCode_ARRAYSIZE = ErrorReportRequestProto_ErrorCode_ErrorCode_MAX + 1;

const ::google::protobuf::EnumDescriptor* ErrorReportRequestProto_ErrorCode_descriptor();
inline const ::std::string& ErrorReportRequestProto_ErrorCode_Name(ErrorReportRequestProto_ErrorCode value) {
  return ::google::protobuf::internal::NameOfEnum(
    ErrorReportRequestProto_ErrorCode_descriptor(), value);
}
inline bool ErrorReportRequestProto_ErrorCode_Parse(
    const ::std::string& name, ErrorReportRequestProto_ErrorCode* value) {
  return ::google::protobuf::internal::ParseNamedEnum<ErrorReportRequestProto_ErrorCode>(
    ErrorReportRequestProto_ErrorCode_descriptor(), name, value);
}
// ===================================================================

class DatanodeRegistrationProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.DatanodeRegistrationProto) */ {
 public:
  DatanodeRegistrationProto();
  virtual ~DatanodeRegistrationProto();

  DatanodeRegistrationProto(const DatanodeRegistrationProto& from);

  inline DatanodeRegistrationProto& operator=(const DatanodeRegistrationProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  DatanodeRegistrationProto(DatanodeRegistrationProto&& from) noexcept
    : DatanodeRegistrationProto() {
    *this = ::std::move(from);
  }

  inline DatanodeRegistrationProto& operator=(DatanodeRegistrationProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const DatanodeRegistrationProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const DatanodeRegistrationProto* internal_default_instance() {
    return reinterpret_cast<const DatanodeRegistrationProto*>(
               &_DatanodeRegistrationProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    0;

  void Swap(DatanodeRegistrationProto* other);
  friend void swap(DatanodeRegistrationProto& a, DatanodeRegistrationProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline DatanodeRegistrationProto* New() const final {
    return CreateMaybeMessage<DatanodeRegistrationProto>(NULL);
  }

  DatanodeRegistrationProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<DatanodeRegistrationProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const DatanodeRegistrationProto& from);
  void MergeFrom(const DatanodeRegistrationProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DatanodeRegistrationProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required string softwareVersion = 4;
  bool has_softwareversion() const;
  void clear_softwareversion();
  static const int kSoftwareVersionFieldNumber = 4;
  const ::std::string& softwareversion() const;
  void set_softwareversion(const ::std::string& value);
  #if LANG_CXX11
  void set_softwareversion(::std::string&& value);
  #endif
  void set_softwareversion(const char* value);
  void set_softwareversion(const char* value, size_t size);
  ::std::string* mutable_softwareversion();
  ::std::string* release_softwareversion();
  void set_allocated_softwareversion(::std::string* softwareversion);

  // required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
  bool has_datanodeid() const;
  void clear_datanodeid();
  static const int kDatanodeIDFieldNumber = 1;
  private:
  const ::hadoop::hdfs::DatanodeIDProto& _internal_datanodeid() const;
  public:
  const ::hadoop::hdfs::DatanodeIDProto& datanodeid() const;
  ::hadoop::hdfs::DatanodeIDProto* release_datanodeid();
  ::hadoop::hdfs::DatanodeIDProto* mutable_datanodeid();
  void set_allocated_datanodeid(::hadoop::hdfs::DatanodeIDProto* datanodeid);

  // required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
  bool has_storageinfo() const;
  void clear_storageinfo();
  static const int kStorageInfoFieldNumber = 2;
  private:
  const ::hadoop::hdfs::StorageInfoProto& _internal_storageinfo() const;
  public:
  const ::hadoop::hdfs::StorageInfoProto& storageinfo() const;
  ::hadoop::hdfs::StorageInfoProto* release_storageinfo();
  ::hadoop::hdfs::StorageInfoProto* mutable_storageinfo();
  void set_allocated_storageinfo(::hadoop::hdfs::StorageInfoProto* storageinfo);

  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
  bool has_keys() const;
  void clear_keys();
  static const int kKeysFieldNumber = 3;
  private:
  const ::hadoop::hdfs::ExportedBlockKeysProto& _internal_keys() const;
  public:
  const ::hadoop::hdfs::ExportedBlockKeysProto& keys() const;
  ::hadoop::hdfs::ExportedBlockKeysProto* release_keys();
  ::hadoop::hdfs::ExportedBlockKeysProto* mutable_keys();
  void set_allocated_keys(::hadoop::hdfs::ExportedBlockKeysProto* keys);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.DatanodeRegistrationProto)
 private:
  void set_has_datanodeid();
  void clear_has_datanodeid();
  void set_has_storageinfo();
  void clear_has_storageinfo();
  void set_has_keys();
  void clear_has_keys();
  void set_has_softwareversion();
  void clear_has_softwareversion();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr softwareversion_;
  ::hadoop::hdfs::DatanodeIDProto* datanodeid_;
  ::hadoop::hdfs::StorageInfoProto* storageinfo_;
  ::hadoop::hdfs::ExportedBlockKeysProto* keys_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class DatanodeCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.DatanodeCommandProto) */ {
 public:
  DatanodeCommandProto();
  virtual ~DatanodeCommandProto();

  DatanodeCommandProto(const DatanodeCommandProto& from);

  inline DatanodeCommandProto& operator=(const DatanodeCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  DatanodeCommandProto(DatanodeCommandProto&& from) noexcept
    : DatanodeCommandProto() {
    *this = ::std::move(from);
  }

  inline DatanodeCommandProto& operator=(DatanodeCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const DatanodeCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const DatanodeCommandProto* internal_default_instance() {
    return reinterpret_cast<const DatanodeCommandProto*>(
               &_DatanodeCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    1;

  void Swap(DatanodeCommandProto* other);
  friend void swap(DatanodeCommandProto& a, DatanodeCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline DatanodeCommandProto* New() const final {
    return CreateMaybeMessage<DatanodeCommandProto>(NULL);
  }

  DatanodeCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<DatanodeCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const DatanodeCommandProto& from);
  void MergeFrom(const DatanodeCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(DatanodeCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef DatanodeCommandProto_Type Type;
  static const Type BalancerBandwidthCommand =
    DatanodeCommandProto_Type_BalancerBandwidthCommand;
  static const Type BlockCommand =
    DatanodeCommandProto_Type_BlockCommand;
  static const Type BlockRecoveryCommand =
    DatanodeCommandProto_Type_BlockRecoveryCommand;
  static const Type FinalizeCommand =
    DatanodeCommandProto_Type_FinalizeCommand;
  static const Type KeyUpdateCommand =
    DatanodeCommandProto_Type_KeyUpdateCommand;
  static const Type RegisterCommand =
    DatanodeCommandProto_Type_RegisterCommand;
  static const Type UnusedUpgradeCommand =
    DatanodeCommandProto_Type_UnusedUpgradeCommand;
  static const Type NullDatanodeCommand =
    DatanodeCommandProto_Type_NullDatanodeCommand;
  static const Type BlockIdCommand =
    DatanodeCommandProto_Type_BlockIdCommand;
  static const Type BlockECReconstructionCommand =
    DatanodeCommandProto_Type_BlockECReconstructionCommand;
  static inline bool Type_IsValid(int value) {
    return DatanodeCommandProto_Type_IsValid(value);
  }
  static const Type Type_MIN =
    DatanodeCommandProto_Type_Type_MIN;
  static const Type Type_MAX =
    DatanodeCommandProto_Type_Type_MAX;
  static const int Type_ARRAYSIZE =
    DatanodeCommandProto_Type_Type_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Type_descriptor() {
    return DatanodeCommandProto_Type_descriptor();
  }
  static inline const ::std::string& Type_Name(Type value) {
    return DatanodeCommandProto_Type_Name(value);
  }
  static inline bool Type_Parse(const ::std::string& name,
      Type* value) {
    return DatanodeCommandProto_Type_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
  bool has_balancercmd() const;
  void clear_balancercmd();
  static const int kBalancerCmdFieldNumber = 2;
  private:
  const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto& _internal_balancercmd() const;
  public:
  const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto& balancercmd() const;
  ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* release_balancercmd();
  ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* mutable_balancercmd();
  void set_allocated_balancercmd(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* balancercmd);

  // optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
  bool has_blkcmd() const;
  void clear_blkcmd();
  static const int kBlkCmdFieldNumber = 3;
  private:
  const ::hadoop::hdfs::datanode::BlockCommandProto& _internal_blkcmd() const;
  public:
  const ::hadoop::hdfs::datanode::BlockCommandProto& blkcmd() const;
  ::hadoop::hdfs::datanode::BlockCommandProto* release_blkcmd();
  ::hadoop::hdfs::datanode::BlockCommandProto* mutable_blkcmd();
  void set_allocated_blkcmd(::hadoop::hdfs::datanode::BlockCommandProto* blkcmd);

  // optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
  bool has_recoverycmd() const;
  void clear_recoverycmd();
  static const int kRecoveryCmdFieldNumber = 4;
  private:
  const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto& _internal_recoverycmd() const;
  public:
  const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto& recoverycmd() const;
  ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* release_recoverycmd();
  ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* mutable_recoverycmd();
  void set_allocated_recoverycmd(::hadoop::hdfs::datanode::BlockRecoveryCommandProto* recoverycmd);

  // optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
  bool has_finalizecmd() const;
  void clear_finalizecmd();
  static const int kFinalizeCmdFieldNumber = 5;
  private:
  const ::hadoop::hdfs::datanode::FinalizeCommandProto& _internal_finalizecmd() const;
  public:
  const ::hadoop::hdfs::datanode::FinalizeCommandProto& finalizecmd() const;
  ::hadoop::hdfs::datanode::FinalizeCommandProto* release_finalizecmd();
  ::hadoop::hdfs::datanode::FinalizeCommandProto* mutable_finalizecmd();
  void set_allocated_finalizecmd(::hadoop::hdfs::datanode::FinalizeCommandProto* finalizecmd);

  // optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
  bool has_keyupdatecmd() const;
  void clear_keyupdatecmd();
  static const int kKeyUpdateCmdFieldNumber = 6;
  private:
  const ::hadoop::hdfs::datanode::KeyUpdateCommandProto& _internal_keyupdatecmd() const;
  public:
  const ::hadoop::hdfs::datanode::KeyUpdateCommandProto& keyupdatecmd() const;
  ::hadoop::hdfs::datanode::KeyUpdateCommandProto* release_keyupdatecmd();
  ::hadoop::hdfs::datanode::KeyUpdateCommandProto* mutable_keyupdatecmd();
  void set_allocated_keyupdatecmd(::hadoop::hdfs::datanode::KeyUpdateCommandProto* keyupdatecmd);

  // optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
  bool has_registercmd() const;
  void clear_registercmd();
  static const int kRegisterCmdFieldNumber = 7;
  private:
  const ::hadoop::hdfs::datanode::RegisterCommandProto& _internal_registercmd() const;
  public:
  const ::hadoop::hdfs::datanode::RegisterCommandProto& registercmd() const;
  ::hadoop::hdfs::datanode::RegisterCommandProto* release_registercmd();
  ::hadoop::hdfs::datanode::RegisterCommandProto* mutable_registercmd();
  void set_allocated_registercmd(::hadoop::hdfs::datanode::RegisterCommandProto* registercmd);

  // optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
  bool has_blkidcmd() const;
  void clear_blkidcmd();
  static const int kBlkIdCmdFieldNumber = 8;
  private:
  const ::hadoop::hdfs::datanode::BlockIdCommandProto& _internal_blkidcmd() const;
  public:
  const ::hadoop::hdfs::datanode::BlockIdCommandProto& blkidcmd() const;
  ::hadoop::hdfs::datanode::BlockIdCommandProto* release_blkidcmd();
  ::hadoop::hdfs::datanode::BlockIdCommandProto* mutable_blkidcmd();
  void set_allocated_blkidcmd(::hadoop::hdfs::datanode::BlockIdCommandProto* blkidcmd);

  // optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
  bool has_blkecreconstructioncmd() const;
  void clear_blkecreconstructioncmd();
  static const int kBlkECReconstructionCmdFieldNumber = 9;
  private:
  const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto& _internal_blkecreconstructioncmd() const;
  public:
  const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto& blkecreconstructioncmd() const;
  ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* release_blkecreconstructioncmd();
  ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* mutable_blkecreconstructioncmd();
  void set_allocated_blkecreconstructioncmd(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* blkecreconstructioncmd);

  // required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
  bool has_cmdtype() const;
  void clear_cmdtype();
  static const int kCmdTypeFieldNumber = 1;
  ::hadoop::hdfs::datanode::DatanodeCommandProto_Type cmdtype() const;
  void set_cmdtype(::hadoop::hdfs::datanode::DatanodeCommandProto_Type value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.DatanodeCommandProto)
 private:
  void set_has_cmdtype();
  void clear_has_cmdtype();
  void set_has_balancercmd();
  void clear_has_balancercmd();
  void set_has_blkcmd();
  void clear_has_blkcmd();
  void set_has_recoverycmd();
  void clear_has_recoverycmd();
  void set_has_finalizecmd();
  void clear_has_finalizecmd();
  void set_has_keyupdatecmd();
  void clear_has_keyupdatecmd();
  void set_has_registercmd();
  void clear_has_registercmd();
  void set_has_blkidcmd();
  void clear_has_blkidcmd();
  void set_has_blkecreconstructioncmd();
  void clear_has_blkecreconstructioncmd();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* balancercmd_;
  ::hadoop::hdfs::datanode::BlockCommandProto* blkcmd_;
  ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* recoverycmd_;
  ::hadoop::hdfs::datanode::FinalizeCommandProto* finalizecmd_;
  ::hadoop::hdfs::datanode::KeyUpdateCommandProto* keyupdatecmd_;
  ::hadoop::hdfs::datanode::RegisterCommandProto* registercmd_;
  ::hadoop::hdfs::datanode::BlockIdCommandProto* blkidcmd_;
  ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* blkecreconstructioncmd_;
  int cmdtype_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BalancerBandwidthCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BalancerBandwidthCommandProto) */ {
 public:
  BalancerBandwidthCommandProto();
  virtual ~BalancerBandwidthCommandProto();

  BalancerBandwidthCommandProto(const BalancerBandwidthCommandProto& from);

  inline BalancerBandwidthCommandProto& operator=(const BalancerBandwidthCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BalancerBandwidthCommandProto(BalancerBandwidthCommandProto&& from) noexcept
    : BalancerBandwidthCommandProto() {
    *this = ::std::move(from);
  }

  inline BalancerBandwidthCommandProto& operator=(BalancerBandwidthCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BalancerBandwidthCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BalancerBandwidthCommandProto* internal_default_instance() {
    return reinterpret_cast<const BalancerBandwidthCommandProto*>(
               &_BalancerBandwidthCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    2;

  void Swap(BalancerBandwidthCommandProto* other);
  friend void swap(BalancerBandwidthCommandProto& a, BalancerBandwidthCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BalancerBandwidthCommandProto* New() const final {
    return CreateMaybeMessage<BalancerBandwidthCommandProto>(NULL);
  }

  BalancerBandwidthCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BalancerBandwidthCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BalancerBandwidthCommandProto& from);
  void MergeFrom(const BalancerBandwidthCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BalancerBandwidthCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required uint64 bandwidth = 1;
  bool has_bandwidth() const;
  void clear_bandwidth();
  static const int kBandwidthFieldNumber = 1;
  ::google::protobuf::uint64 bandwidth() const;
  void set_bandwidth(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BalancerBandwidthCommandProto)
 private:
  void set_has_bandwidth();
  void clear_has_bandwidth();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::uint64 bandwidth_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockCommandProto) */ {
 public:
  BlockCommandProto();
  virtual ~BlockCommandProto();

  BlockCommandProto(const BlockCommandProto& from);

  inline BlockCommandProto& operator=(const BlockCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockCommandProto(BlockCommandProto&& from) noexcept
    : BlockCommandProto() {
    *this = ::std::move(from);
  }

  inline BlockCommandProto& operator=(BlockCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockCommandProto* internal_default_instance() {
    return reinterpret_cast<const BlockCommandProto*>(
               &_BlockCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    3;

  void Swap(BlockCommandProto* other);
  friend void swap(BlockCommandProto& a, BlockCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockCommandProto* New() const final {
    return CreateMaybeMessage<BlockCommandProto>(NULL);
  }

  BlockCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockCommandProto& from);
  void MergeFrom(const BlockCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef BlockCommandProto_Action Action;
  static const Action TRANSFER =
    BlockCommandProto_Action_TRANSFER;
  static const Action INVALIDATE =
    BlockCommandProto_Action_INVALIDATE;
  static const Action SHUTDOWN =
    BlockCommandProto_Action_SHUTDOWN;
  static inline bool Action_IsValid(int value) {
    return BlockCommandProto_Action_IsValid(value);
  }
  static const Action Action_MIN =
    BlockCommandProto_Action_Action_MIN;
  static const Action Action_MAX =
    BlockCommandProto_Action_Action_MAX;
  static const int Action_ARRAYSIZE =
    BlockCommandProto_Action_Action_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Action_descriptor() {
    return BlockCommandProto_Action_descriptor();
  }
  static inline const ::std::string& Action_Name(Action value) {
    return BlockCommandProto_Action_Name(value);
  }
  static inline bool Action_Parse(const ::std::string& name,
      Action* value) {
    return BlockCommandProto_Action_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.BlockProto blocks = 3;
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 3;
  ::hadoop::hdfs::BlockProto* mutable_blocks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockProto >*
      mutable_blocks();
  const ::hadoop::hdfs::BlockProto& blocks(int index) const;
  ::hadoop::hdfs::BlockProto* add_blocks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockProto >&
      blocks() const;

  // repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
  int targets_size() const;
  void clear_targets();
  static const int kTargetsFieldNumber = 4;
  ::hadoop::hdfs::DatanodeInfosProto* mutable_targets(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeInfosProto >*
      mutable_targets();
  const ::hadoop::hdfs::DatanodeInfosProto& targets(int index) const;
  ::hadoop::hdfs::DatanodeInfosProto* add_targets();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeInfosProto >&
      targets() const;

  // repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
  int targetstorageuuids_size() const;
  void clear_targetstorageuuids();
  static const int kTargetStorageUuidsFieldNumber = 5;
  ::hadoop::hdfs::StorageUuidsProto* mutable_targetstorageuuids(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageUuidsProto >*
      mutable_targetstorageuuids();
  const ::hadoop::hdfs::StorageUuidsProto& targetstorageuuids(int index) const;
  ::hadoop::hdfs::StorageUuidsProto* add_targetstorageuuids();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageUuidsProto >&
      targetstorageuuids() const;

  // repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
  int targetstoragetypes_size() const;
  void clear_targetstoragetypes();
  static const int kTargetStorageTypesFieldNumber = 6;
  ::hadoop::hdfs::StorageTypesProto* mutable_targetstoragetypes(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageTypesProto >*
      mutable_targetstoragetypes();
  const ::hadoop::hdfs::StorageTypesProto& targetstoragetypes(int index) const;
  ::hadoop::hdfs::StorageTypesProto* add_targetstoragetypes();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageTypesProto >&
      targetstoragetypes() const;

  // required string blockPoolId = 2;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 2;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
  bool has_action() const;
  void clear_action();
  static const int kActionFieldNumber = 1;
  ::hadoop::hdfs::datanode::BlockCommandProto_Action action() const;
  void set_action(::hadoop::hdfs::datanode::BlockCommandProto_Action value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockCommandProto)
 private:
  void set_has_action();
  void clear_has_action();
  void set_has_blockpoolid();
  void clear_has_blockpoolid();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockProto > blocks_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeInfosProto > targets_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageUuidsProto > targetstorageuuids_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageTypesProto > targetstoragetypes_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  int action_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockIdCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockIdCommandProto) */ {
 public:
  BlockIdCommandProto();
  virtual ~BlockIdCommandProto();

  BlockIdCommandProto(const BlockIdCommandProto& from);

  inline BlockIdCommandProto& operator=(const BlockIdCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockIdCommandProto(BlockIdCommandProto&& from) noexcept
    : BlockIdCommandProto() {
    *this = ::std::move(from);
  }

  inline BlockIdCommandProto& operator=(BlockIdCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockIdCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockIdCommandProto* internal_default_instance() {
    return reinterpret_cast<const BlockIdCommandProto*>(
               &_BlockIdCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    4;

  void Swap(BlockIdCommandProto* other);
  friend void swap(BlockIdCommandProto& a, BlockIdCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockIdCommandProto* New() const final {
    return CreateMaybeMessage<BlockIdCommandProto>(NULL);
  }

  BlockIdCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockIdCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockIdCommandProto& from);
  void MergeFrom(const BlockIdCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockIdCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef BlockIdCommandProto_Action Action;
  static const Action CACHE =
    BlockIdCommandProto_Action_CACHE;
  static const Action UNCACHE =
    BlockIdCommandProto_Action_UNCACHE;
  static inline bool Action_IsValid(int value) {
    return BlockIdCommandProto_Action_IsValid(value);
  }
  static const Action Action_MIN =
    BlockIdCommandProto_Action_Action_MIN;
  static const Action Action_MAX =
    BlockIdCommandProto_Action_Action_MAX;
  static const int Action_ARRAYSIZE =
    BlockIdCommandProto_Action_Action_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  Action_descriptor() {
    return BlockIdCommandProto_Action_descriptor();
  }
  static inline const ::std::string& Action_Name(Action value) {
    return BlockIdCommandProto_Action_Name(value);
  }
  static inline bool Action_Parse(const ::std::string& name,
      Action* value) {
    return BlockIdCommandProto_Action_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // repeated uint64 blockIds = 3 [packed = true];
  int blockids_size() const;
  void clear_blockids();
  static const int kBlockIdsFieldNumber = 3;
  ::google::protobuf::uint64 blockids(int index) const;
  void set_blockids(int index, ::google::protobuf::uint64 value);
  void add_blockids(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      blockids() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_blockids();

  // required string blockPoolId = 2;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 2;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
  bool has_action() const;
  void clear_action();
  static const int kActionFieldNumber = 1;
  ::hadoop::hdfs::datanode::BlockIdCommandProto_Action action() const;
  void set_action(::hadoop::hdfs::datanode::BlockIdCommandProto_Action value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockIdCommandProto)
 private:
  void set_has_action();
  void clear_has_action();
  void set_has_blockpoolid();
  void clear_has_blockpoolid();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > blockids_;
  mutable int _blockids_cached_byte_size_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  int action_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockRecoveryCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockRecoveryCommandProto) */ {
 public:
  BlockRecoveryCommandProto();
  virtual ~BlockRecoveryCommandProto();

  BlockRecoveryCommandProto(const BlockRecoveryCommandProto& from);

  inline BlockRecoveryCommandProto& operator=(const BlockRecoveryCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockRecoveryCommandProto(BlockRecoveryCommandProto&& from) noexcept
    : BlockRecoveryCommandProto() {
    *this = ::std::move(from);
  }

  inline BlockRecoveryCommandProto& operator=(BlockRecoveryCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockRecoveryCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockRecoveryCommandProto* internal_default_instance() {
    return reinterpret_cast<const BlockRecoveryCommandProto*>(
               &_BlockRecoveryCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    5;

  void Swap(BlockRecoveryCommandProto* other);
  friend void swap(BlockRecoveryCommandProto& a, BlockRecoveryCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockRecoveryCommandProto* New() const final {
    return CreateMaybeMessage<BlockRecoveryCommandProto>(NULL);
  }

  BlockRecoveryCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockRecoveryCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockRecoveryCommandProto& from);
  void MergeFrom(const BlockRecoveryCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockRecoveryCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 1;
  ::hadoop::hdfs::RecoveringBlockProto* mutable_blocks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::RecoveringBlockProto >*
      mutable_blocks();
  const ::hadoop::hdfs::RecoveringBlockProto& blocks(int index) const;
  ::hadoop::hdfs::RecoveringBlockProto* add_blocks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::RecoveringBlockProto >&
      blocks() const;

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockRecoveryCommandProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::RecoveringBlockProto > blocks_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class FinalizeCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.FinalizeCommandProto) */ {
 public:
  FinalizeCommandProto();
  virtual ~FinalizeCommandProto();

  FinalizeCommandProto(const FinalizeCommandProto& from);

  inline FinalizeCommandProto& operator=(const FinalizeCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  FinalizeCommandProto(FinalizeCommandProto&& from) noexcept
    : FinalizeCommandProto() {
    *this = ::std::move(from);
  }

  inline FinalizeCommandProto& operator=(FinalizeCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const FinalizeCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const FinalizeCommandProto* internal_default_instance() {
    return reinterpret_cast<const FinalizeCommandProto*>(
               &_FinalizeCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    6;

  void Swap(FinalizeCommandProto* other);
  friend void swap(FinalizeCommandProto& a, FinalizeCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline FinalizeCommandProto* New() const final {
    return CreateMaybeMessage<FinalizeCommandProto>(NULL);
  }

  FinalizeCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<FinalizeCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const FinalizeCommandProto& from);
  void MergeFrom(const FinalizeCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(FinalizeCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required string blockPoolId = 1;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 1;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.FinalizeCommandProto)
 private:
  void set_has_blockpoolid();
  void clear_has_blockpoolid();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class KeyUpdateCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.KeyUpdateCommandProto) */ {
 public:
  KeyUpdateCommandProto();
  virtual ~KeyUpdateCommandProto();

  KeyUpdateCommandProto(const KeyUpdateCommandProto& from);

  inline KeyUpdateCommandProto& operator=(const KeyUpdateCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  KeyUpdateCommandProto(KeyUpdateCommandProto&& from) noexcept
    : KeyUpdateCommandProto() {
    *this = ::std::move(from);
  }

  inline KeyUpdateCommandProto& operator=(KeyUpdateCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const KeyUpdateCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const KeyUpdateCommandProto* internal_default_instance() {
    return reinterpret_cast<const KeyUpdateCommandProto*>(
               &_KeyUpdateCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    7;

  void Swap(KeyUpdateCommandProto* other);
  friend void swap(KeyUpdateCommandProto& a, KeyUpdateCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline KeyUpdateCommandProto* New() const final {
    return CreateMaybeMessage<KeyUpdateCommandProto>(NULL);
  }

  KeyUpdateCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<KeyUpdateCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const KeyUpdateCommandProto& from);
  void MergeFrom(const KeyUpdateCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(KeyUpdateCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
  bool has_keys() const;
  void clear_keys();
  static const int kKeysFieldNumber = 1;
  private:
  const ::hadoop::hdfs::ExportedBlockKeysProto& _internal_keys() const;
  public:
  const ::hadoop::hdfs::ExportedBlockKeysProto& keys() const;
  ::hadoop::hdfs::ExportedBlockKeysProto* release_keys();
  ::hadoop::hdfs::ExportedBlockKeysProto* mutable_keys();
  void set_allocated_keys(::hadoop::hdfs::ExportedBlockKeysProto* keys);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.KeyUpdateCommandProto)
 private:
  void set_has_keys();
  void clear_has_keys();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::ExportedBlockKeysProto* keys_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RegisterCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.RegisterCommandProto) */ {
 public:
  RegisterCommandProto();
  virtual ~RegisterCommandProto();

  RegisterCommandProto(const RegisterCommandProto& from);

  inline RegisterCommandProto& operator=(const RegisterCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  RegisterCommandProto(RegisterCommandProto&& from) noexcept
    : RegisterCommandProto() {
    *this = ::std::move(from);
  }

  inline RegisterCommandProto& operator=(RegisterCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const RegisterCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const RegisterCommandProto* internal_default_instance() {
    return reinterpret_cast<const RegisterCommandProto*>(
               &_RegisterCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    8;

  void Swap(RegisterCommandProto* other);
  friend void swap(RegisterCommandProto& a, RegisterCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline RegisterCommandProto* New() const final {
    return CreateMaybeMessage<RegisterCommandProto>(NULL);
  }

  RegisterCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<RegisterCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const RegisterCommandProto& from);
  void MergeFrom(const RegisterCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(RegisterCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.RegisterCommandProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockECReconstructionCommandProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockECReconstructionCommandProto) */ {
 public:
  BlockECReconstructionCommandProto();
  virtual ~BlockECReconstructionCommandProto();

  BlockECReconstructionCommandProto(const BlockECReconstructionCommandProto& from);

  inline BlockECReconstructionCommandProto& operator=(const BlockECReconstructionCommandProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockECReconstructionCommandProto(BlockECReconstructionCommandProto&& from) noexcept
    : BlockECReconstructionCommandProto() {
    *this = ::std::move(from);
  }

  inline BlockECReconstructionCommandProto& operator=(BlockECReconstructionCommandProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockECReconstructionCommandProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockECReconstructionCommandProto* internal_default_instance() {
    return reinterpret_cast<const BlockECReconstructionCommandProto*>(
               &_BlockECReconstructionCommandProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    9;

  void Swap(BlockECReconstructionCommandProto* other);
  friend void swap(BlockECReconstructionCommandProto& a, BlockECReconstructionCommandProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockECReconstructionCommandProto* New() const final {
    return CreateMaybeMessage<BlockECReconstructionCommandProto>(NULL);
  }

  BlockECReconstructionCommandProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockECReconstructionCommandProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockECReconstructionCommandProto& from);
  void MergeFrom(const BlockECReconstructionCommandProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockECReconstructionCommandProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
  int blockecreconstructioninfo_size() const;
  void clear_blockecreconstructioninfo();
  static const int kBlockECReconstructioninfoFieldNumber = 1;
  ::hadoop::hdfs::BlockECReconstructionInfoProto* mutable_blockecreconstructioninfo(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockECReconstructionInfoProto >*
      mutable_blockecreconstructioninfo();
  const ::hadoop::hdfs::BlockECReconstructionInfoProto& blockecreconstructioninfo(int index) const;
  ::hadoop::hdfs::BlockECReconstructionInfoProto* add_blockecreconstructioninfo();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockECReconstructionInfoProto >&
      blockecreconstructioninfo() const;

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockECReconstructionCommandProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockECReconstructionInfoProto > blockecreconstructioninfo_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RegisterDatanodeRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.RegisterDatanodeRequestProto) */ {
 public:
  RegisterDatanodeRequestProto();
  virtual ~RegisterDatanodeRequestProto();

  RegisterDatanodeRequestProto(const RegisterDatanodeRequestProto& from);

  inline RegisterDatanodeRequestProto& operator=(const RegisterDatanodeRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  RegisterDatanodeRequestProto(RegisterDatanodeRequestProto&& from) noexcept
    : RegisterDatanodeRequestProto() {
    *this = ::std::move(from);
  }

  inline RegisterDatanodeRequestProto& operator=(RegisterDatanodeRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const RegisterDatanodeRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const RegisterDatanodeRequestProto* internal_default_instance() {
    return reinterpret_cast<const RegisterDatanodeRequestProto*>(
               &_RegisterDatanodeRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    10;

  void Swap(RegisterDatanodeRequestProto* other);
  friend void swap(RegisterDatanodeRequestProto& a, RegisterDatanodeRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline RegisterDatanodeRequestProto* New() const final {
    return CreateMaybeMessage<RegisterDatanodeRequestProto>(NULL);
  }

  RegisterDatanodeRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<RegisterDatanodeRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const RegisterDatanodeRequestProto& from);
  void MergeFrom(const RegisterDatanodeRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(RegisterDatanodeRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.RegisterDatanodeRequestProto)
 private:
  void set_has_registration();
  void clear_has_registration();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class RegisterDatanodeResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.RegisterDatanodeResponseProto) */ {
 public:
  RegisterDatanodeResponseProto();
  virtual ~RegisterDatanodeResponseProto();

  RegisterDatanodeResponseProto(const RegisterDatanodeResponseProto& from);

  inline RegisterDatanodeResponseProto& operator=(const RegisterDatanodeResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  RegisterDatanodeResponseProto(RegisterDatanodeResponseProto&& from) noexcept
    : RegisterDatanodeResponseProto() {
    *this = ::std::move(from);
  }

  inline RegisterDatanodeResponseProto& operator=(RegisterDatanodeResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const RegisterDatanodeResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const RegisterDatanodeResponseProto* internal_default_instance() {
    return reinterpret_cast<const RegisterDatanodeResponseProto*>(
               &_RegisterDatanodeResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    11;

  void Swap(RegisterDatanodeResponseProto* other);
  friend void swap(RegisterDatanodeResponseProto& a, RegisterDatanodeResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline RegisterDatanodeResponseProto* New() const final {
    return CreateMaybeMessage<RegisterDatanodeResponseProto>(NULL);
  }

  RegisterDatanodeResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<RegisterDatanodeResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const RegisterDatanodeResponseProto& from);
  void MergeFrom(const RegisterDatanodeResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(RegisterDatanodeResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.RegisterDatanodeResponseProto)
 private:
  void set_has_registration();
  void clear_has_registration();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class VolumeFailureSummaryProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.VolumeFailureSummaryProto) */ {
 public:
  VolumeFailureSummaryProto();
  virtual ~VolumeFailureSummaryProto();

  VolumeFailureSummaryProto(const VolumeFailureSummaryProto& from);

  inline VolumeFailureSummaryProto& operator=(const VolumeFailureSummaryProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  VolumeFailureSummaryProto(VolumeFailureSummaryProto&& from) noexcept
    : VolumeFailureSummaryProto() {
    *this = ::std::move(from);
  }

  inline VolumeFailureSummaryProto& operator=(VolumeFailureSummaryProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const VolumeFailureSummaryProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const VolumeFailureSummaryProto* internal_default_instance() {
    return reinterpret_cast<const VolumeFailureSummaryProto*>(
               &_VolumeFailureSummaryProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    12;

  void Swap(VolumeFailureSummaryProto* other);
  friend void swap(VolumeFailureSummaryProto& a, VolumeFailureSummaryProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline VolumeFailureSummaryProto* New() const final {
    return CreateMaybeMessage<VolumeFailureSummaryProto>(NULL);
  }

  VolumeFailureSummaryProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<VolumeFailureSummaryProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const VolumeFailureSummaryProto& from);
  void MergeFrom(const VolumeFailureSummaryProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(VolumeFailureSummaryProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated string failedStorageLocations = 1;
  int failedstoragelocations_size() const;
  void clear_failedstoragelocations();
  static const int kFailedStorageLocationsFieldNumber = 1;
  const ::std::string& failedstoragelocations(int index) const;
  ::std::string* mutable_failedstoragelocations(int index);
  void set_failedstoragelocations(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_failedstoragelocations(int index, ::std::string&& value);
  #endif
  void set_failedstoragelocations(int index, const char* value);
  void set_failedstoragelocations(int index, const char* value, size_t size);
  ::std::string* add_failedstoragelocations();
  void add_failedstoragelocations(const ::std::string& value);
  #if LANG_CXX11
  void add_failedstoragelocations(::std::string&& value);
  #endif
  void add_failedstoragelocations(const char* value);
  void add_failedstoragelocations(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& failedstoragelocations() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_failedstoragelocations();

  // required uint64 lastVolumeFailureDate = 2;
  bool has_lastvolumefailuredate() const;
  void clear_lastvolumefailuredate();
  static const int kLastVolumeFailureDateFieldNumber = 2;
  ::google::protobuf::uint64 lastvolumefailuredate() const;
  void set_lastvolumefailuredate(::google::protobuf::uint64 value);

  // required uint64 estimatedCapacityLostTotal = 3;
  bool has_estimatedcapacitylosttotal() const;
  void clear_estimatedcapacitylosttotal();
  static const int kEstimatedCapacityLostTotalFieldNumber = 3;
  ::google::protobuf::uint64 estimatedcapacitylosttotal() const;
  void set_estimatedcapacitylosttotal(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.VolumeFailureSummaryProto)
 private:
  void set_has_lastvolumefailuredate();
  void clear_has_lastvolumefailuredate();
  void set_has_estimatedcapacitylosttotal();
  void clear_has_estimatedcapacitylosttotal();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::std::string> failedstoragelocations_;
  ::google::protobuf::uint64 lastvolumefailuredate_;
  ::google::protobuf::uint64 estimatedcapacitylosttotal_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class HeartbeatRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.HeartbeatRequestProto) */ {
 public:
  HeartbeatRequestProto();
  virtual ~HeartbeatRequestProto();

  HeartbeatRequestProto(const HeartbeatRequestProto& from);

  inline HeartbeatRequestProto& operator=(const HeartbeatRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  HeartbeatRequestProto(HeartbeatRequestProto&& from) noexcept
    : HeartbeatRequestProto() {
    *this = ::std::move(from);
  }

  inline HeartbeatRequestProto& operator=(HeartbeatRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const HeartbeatRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const HeartbeatRequestProto* internal_default_instance() {
    return reinterpret_cast<const HeartbeatRequestProto*>(
               &_HeartbeatRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    13;

  void Swap(HeartbeatRequestProto* other);
  friend void swap(HeartbeatRequestProto& a, HeartbeatRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline HeartbeatRequestProto* New() const final {
    return CreateMaybeMessage<HeartbeatRequestProto>(NULL);
  }

  HeartbeatRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<HeartbeatRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const HeartbeatRequestProto& from);
  void MergeFrom(const HeartbeatRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HeartbeatRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.StorageReportProto reports = 2;
  int reports_size() const;
  void clear_reports();
  static const int kReportsFieldNumber = 2;
  ::hadoop::hdfs::StorageReportProto* mutable_reports(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageReportProto >*
      mutable_reports();
  const ::hadoop::hdfs::StorageReportProto& reports(int index) const;
  ::hadoop::hdfs::StorageReportProto* add_reports();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageReportProto >&
      reports() const;

  // repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
  int slowpeers_size() const;
  void clear_slowpeers();
  static const int kSlowPeersFieldNumber = 10;
  ::hadoop::hdfs::datanode::SlowPeerReportProto* mutable_slowpeers(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowPeerReportProto >*
      mutable_slowpeers();
  const ::hadoop::hdfs::datanode::SlowPeerReportProto& slowpeers(int index) const;
  ::hadoop::hdfs::datanode::SlowPeerReportProto* add_slowpeers();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowPeerReportProto >&
      slowpeers() const;

  // repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
  int slowdisks_size() const;
  void clear_slowdisks();
  static const int kSlowDisksFieldNumber = 11;
  ::hadoop::hdfs::datanode::SlowDiskReportProto* mutable_slowdisks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowDiskReportProto >*
      mutable_slowdisks();
  const ::hadoop::hdfs::datanode::SlowDiskReportProto& slowdisks(int index) const;
  ::hadoop::hdfs::datanode::SlowDiskReportProto* add_slowdisks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowDiskReportProto >&
      slowdisks() const;

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
  bool has_volumefailuresummary() const;
  void clear_volumefailuresummary();
  static const int kVolumeFailureSummaryFieldNumber = 8;
  private:
  const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto& _internal_volumefailuresummary() const;
  public:
  const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto& volumefailuresummary() const;
  ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* release_volumefailuresummary();
  ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* mutable_volumefailuresummary();
  void set_allocated_volumefailuresummary(::hadoop::hdfs::datanode::VolumeFailureSummaryProto* volumefailuresummary);

  // optional uint32 xmitsInProgress = 3 [default = 0];
  bool has_xmitsinprogress() const;
  void clear_xmitsinprogress();
  static const int kXmitsInProgressFieldNumber = 3;
  ::google::protobuf::uint32 xmitsinprogress() const;
  void set_xmitsinprogress(::google::protobuf::uint32 value);

  // optional uint32 xceiverCount = 4 [default = 0];
  bool has_xceivercount() const;
  void clear_xceivercount();
  static const int kXceiverCountFieldNumber = 4;
  ::google::protobuf::uint32 xceivercount() const;
  void set_xceivercount(::google::protobuf::uint32 value);

  // optional uint64 cacheCapacity = 6 [default = 0];
  bool has_cachecapacity() const;
  void clear_cachecapacity();
  static const int kCacheCapacityFieldNumber = 6;
  ::google::protobuf::uint64 cachecapacity() const;
  void set_cachecapacity(::google::protobuf::uint64 value);

  // optional uint64 cacheUsed = 7 [default = 0];
  bool has_cacheused() const;
  void clear_cacheused();
  static const int kCacheUsedFieldNumber = 7;
  ::google::protobuf::uint64 cacheused() const;
  void set_cacheused(::google::protobuf::uint64 value);

  // optional uint32 failedVolumes = 5 [default = 0];
  bool has_failedvolumes() const;
  void clear_failedvolumes();
  static const int kFailedVolumesFieldNumber = 5;
  ::google::protobuf::uint32 failedvolumes() const;
  void set_failedvolumes(::google::protobuf::uint32 value);

  // optional bool requestFullBlockReportLease = 9 [default = false];
  bool has_requestfullblockreportlease() const;
  void clear_requestfullblockreportlease();
  static const int kRequestFullBlockReportLeaseFieldNumber = 9;
  bool requestfullblockreportlease() const;
  void set_requestfullblockreportlease(bool value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.HeartbeatRequestProto)
 private:
  void set_has_registration();
  void clear_has_registration();
  void set_has_xmitsinprogress();
  void clear_has_xmitsinprogress();
  void set_has_xceivercount();
  void clear_has_xceivercount();
  void set_has_failedvolumes();
  void clear_has_failedvolumes();
  void set_has_cachecapacity();
  void clear_has_cachecapacity();
  void set_has_cacheused();
  void clear_has_cacheused();
  void set_has_volumefailuresummary();
  void clear_has_volumefailuresummary();
  void set_has_requestfullblockreportlease();
  void clear_has_requestfullblockreportlease();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageReportProto > reports_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowPeerReportProto > slowpeers_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowDiskReportProto > slowdisks_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* volumefailuresummary_;
  ::google::protobuf::uint32 xmitsinprogress_;
  ::google::protobuf::uint32 xceivercount_;
  ::google::protobuf::uint64 cachecapacity_;
  ::google::protobuf::uint64 cacheused_;
  ::google::protobuf::uint32 failedvolumes_;
  bool requestfullblockreportlease_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class HeartbeatResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.HeartbeatResponseProto) */ {
 public:
  HeartbeatResponseProto();
  virtual ~HeartbeatResponseProto();

  HeartbeatResponseProto(const HeartbeatResponseProto& from);

  inline HeartbeatResponseProto& operator=(const HeartbeatResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  HeartbeatResponseProto(HeartbeatResponseProto&& from) noexcept
    : HeartbeatResponseProto() {
    *this = ::std::move(from);
  }

  inline HeartbeatResponseProto& operator=(HeartbeatResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const HeartbeatResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const HeartbeatResponseProto* internal_default_instance() {
    return reinterpret_cast<const HeartbeatResponseProto*>(
               &_HeartbeatResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    14;

  void Swap(HeartbeatResponseProto* other);
  friend void swap(HeartbeatResponseProto& a, HeartbeatResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline HeartbeatResponseProto* New() const final {
    return CreateMaybeMessage<HeartbeatResponseProto>(NULL);
  }

  HeartbeatResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<HeartbeatResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const HeartbeatResponseProto& from);
  void MergeFrom(const HeartbeatResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(HeartbeatResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
  int cmds_size() const;
  void clear_cmds();
  static const int kCmdsFieldNumber = 1;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* mutable_cmds(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::DatanodeCommandProto >*
      mutable_cmds();
  const ::hadoop::hdfs::datanode::DatanodeCommandProto& cmds(int index) const;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* add_cmds();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::DatanodeCommandProto >&
      cmds() const;

  // required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
  bool has_hastatus() const;
  void clear_hastatus();
  static const int kHaStatusFieldNumber = 2;
  private:
  const ::hadoop::hdfs::NNHAStatusHeartbeatProto& _internal_hastatus() const;
  public:
  const ::hadoop::hdfs::NNHAStatusHeartbeatProto& hastatus() const;
  ::hadoop::hdfs::NNHAStatusHeartbeatProto* release_hastatus();
  ::hadoop::hdfs::NNHAStatusHeartbeatProto* mutable_hastatus();
  void set_allocated_hastatus(::hadoop::hdfs::NNHAStatusHeartbeatProto* hastatus);

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
  bool has_rollingupgradestatus() const;
  void clear_rollingupgradestatus();
  static const int kRollingUpgradeStatusFieldNumber = 3;
  private:
  const ::hadoop::hdfs::RollingUpgradeStatusProto& _internal_rollingupgradestatus() const;
  public:
  const ::hadoop::hdfs::RollingUpgradeStatusProto& rollingupgradestatus() const;
  ::hadoop::hdfs::RollingUpgradeStatusProto* release_rollingupgradestatus();
  ::hadoop::hdfs::RollingUpgradeStatusProto* mutable_rollingupgradestatus();
  void set_allocated_rollingupgradestatus(::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatus);

  // optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
  bool has_rollingupgradestatusv2() const;
  void clear_rollingupgradestatusv2();
  static const int kRollingUpgradeStatusV2FieldNumber = 4;
  private:
  const ::hadoop::hdfs::RollingUpgradeStatusProto& _internal_rollingupgradestatusv2() const;
  public:
  const ::hadoop::hdfs::RollingUpgradeStatusProto& rollingupgradestatusv2() const;
  ::hadoop::hdfs::RollingUpgradeStatusProto* release_rollingupgradestatusv2();
  ::hadoop::hdfs::RollingUpgradeStatusProto* mutable_rollingupgradestatusv2();
  void set_allocated_rollingupgradestatusv2(::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatusv2);

  // optional uint64 fullBlockReportLeaseId = 5 [default = 0];
  bool has_fullblockreportleaseid() const;
  void clear_fullblockreportleaseid();
  static const int kFullBlockReportLeaseIdFieldNumber = 5;
  ::google::protobuf::uint64 fullblockreportleaseid() const;
  void set_fullblockreportleaseid(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.HeartbeatResponseProto)
 private:
  void set_has_hastatus();
  void clear_has_hastatus();
  void set_has_rollingupgradestatus();
  void clear_has_rollingupgradestatus();
  void set_has_rollingupgradestatusv2();
  void clear_has_rollingupgradestatusv2();
  void set_has_fullblockreportleaseid();
  void clear_has_fullblockreportleaseid();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::DatanodeCommandProto > cmds_;
  ::hadoop::hdfs::NNHAStatusHeartbeatProto* hastatus_;
  ::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatus_;
  ::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatusv2_;
  ::google::protobuf::uint64 fullblockreportleaseid_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockReportRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockReportRequestProto) */ {
 public:
  BlockReportRequestProto();
  virtual ~BlockReportRequestProto();

  BlockReportRequestProto(const BlockReportRequestProto& from);

  inline BlockReportRequestProto& operator=(const BlockReportRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockReportRequestProto(BlockReportRequestProto&& from) noexcept
    : BlockReportRequestProto() {
    *this = ::std::move(from);
  }

  inline BlockReportRequestProto& operator=(BlockReportRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockReportRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockReportRequestProto* internal_default_instance() {
    return reinterpret_cast<const BlockReportRequestProto*>(
               &_BlockReportRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    15;

  void Swap(BlockReportRequestProto* other);
  friend void swap(BlockReportRequestProto& a, BlockReportRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockReportRequestProto* New() const final {
    return CreateMaybeMessage<BlockReportRequestProto>(NULL);
  }

  BlockReportRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockReportRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockReportRequestProto& from);
  void MergeFrom(const BlockReportRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockReportRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
  int reports_size() const;
  void clear_reports();
  static const int kReportsFieldNumber = 3;
  ::hadoop::hdfs::datanode::StorageBlockReportProto* mutable_reports(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageBlockReportProto >*
      mutable_reports();
  const ::hadoop::hdfs::datanode::StorageBlockReportProto& reports(int index) const;
  ::hadoop::hdfs::datanode::StorageBlockReportProto* add_reports();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageBlockReportProto >&
      reports() const;

  // required string blockPoolId = 2;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 2;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
  bool has_context() const;
  void clear_context();
  static const int kContextFieldNumber = 4;
  private:
  const ::hadoop::hdfs::datanode::BlockReportContextProto& _internal_context() const;
  public:
  const ::hadoop::hdfs::datanode::BlockReportContextProto& context() const;
  ::hadoop::hdfs::datanode::BlockReportContextProto* release_context();
  ::hadoop::hdfs::datanode::BlockReportContextProto* mutable_context();
  void set_allocated_context(::hadoop::hdfs::datanode::BlockReportContextProto* context);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockReportRequestProto)
 private:
  void set_has_registration();
  void clear_has_registration();
  void set_has_blockpoolid();
  void clear_has_blockpoolid();
  void set_has_context();
  void clear_has_context();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageBlockReportProto > reports_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  ::hadoop::hdfs::datanode::BlockReportContextProto* context_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockReportContextProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockReportContextProto) */ {
 public:
  BlockReportContextProto();
  virtual ~BlockReportContextProto();

  BlockReportContextProto(const BlockReportContextProto& from);

  inline BlockReportContextProto& operator=(const BlockReportContextProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockReportContextProto(BlockReportContextProto&& from) noexcept
    : BlockReportContextProto() {
    *this = ::std::move(from);
  }

  inline BlockReportContextProto& operator=(BlockReportContextProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockReportContextProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockReportContextProto* internal_default_instance() {
    return reinterpret_cast<const BlockReportContextProto*>(
               &_BlockReportContextProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    16;

  void Swap(BlockReportContextProto* other);
  friend void swap(BlockReportContextProto& a, BlockReportContextProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockReportContextProto* New() const final {
    return CreateMaybeMessage<BlockReportContextProto>(NULL);
  }

  BlockReportContextProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockReportContextProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockReportContextProto& from);
  void MergeFrom(const BlockReportContextProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockReportContextProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // required int32 totalRpcs = 1;
  bool has_totalrpcs() const;
  void clear_totalrpcs();
  static const int kTotalRpcsFieldNumber = 1;
  ::google::protobuf::int32 totalrpcs() const;
  void set_totalrpcs(::google::protobuf::int32 value);

  // required int32 curRpc = 2;
  bool has_currpc() const;
  void clear_currpc();
  static const int kCurRpcFieldNumber = 2;
  ::google::protobuf::int32 currpc() const;
  void set_currpc(::google::protobuf::int32 value);

  // required int64 id = 3;
  bool has_id() const;
  void clear_id();
  static const int kIdFieldNumber = 3;
  ::google::protobuf::int64 id() const;
  void set_id(::google::protobuf::int64 value);

  // optional uint64 leaseId = 4 [default = 0];
  bool has_leaseid() const;
  void clear_leaseid();
  static const int kLeaseIdFieldNumber = 4;
  ::google::protobuf::uint64 leaseid() const;
  void set_leaseid(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockReportContextProto)
 private:
  void set_has_totalrpcs();
  void clear_has_totalrpcs();
  void set_has_currpc();
  void clear_has_currpc();
  void set_has_id();
  void clear_has_id();
  void set_has_leaseid();
  void clear_has_leaseid();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::int32 totalrpcs_;
  ::google::protobuf::int32 currpc_;
  ::google::protobuf::int64 id_;
  ::google::protobuf::uint64 leaseid_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StorageBlockReportProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.StorageBlockReportProto) */ {
 public:
  StorageBlockReportProto();
  virtual ~StorageBlockReportProto();

  StorageBlockReportProto(const StorageBlockReportProto& from);

  inline StorageBlockReportProto& operator=(const StorageBlockReportProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StorageBlockReportProto(StorageBlockReportProto&& from) noexcept
    : StorageBlockReportProto() {
    *this = ::std::move(from);
  }

  inline StorageBlockReportProto& operator=(StorageBlockReportProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const StorageBlockReportProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StorageBlockReportProto* internal_default_instance() {
    return reinterpret_cast<const StorageBlockReportProto*>(
               &_StorageBlockReportProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    17;

  void Swap(StorageBlockReportProto* other);
  friend void swap(StorageBlockReportProto& a, StorageBlockReportProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StorageBlockReportProto* New() const final {
    return CreateMaybeMessage<StorageBlockReportProto>(NULL);
  }

  StorageBlockReportProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StorageBlockReportProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StorageBlockReportProto& from);
  void MergeFrom(const StorageBlockReportProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StorageBlockReportProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 blocks = 2 [packed = true];
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 2;
  ::google::protobuf::uint64 blocks(int index) const;
  void set_blocks(int index, ::google::protobuf::uint64 value);
  void add_blocks(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      blocks() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_blocks();

  // repeated bytes blocksBuffers = 4;
  int blocksbuffers_size() const;
  void clear_blocksbuffers();
  static const int kBlocksBuffersFieldNumber = 4;
  const ::std::string& blocksbuffers(int index) const;
  ::std::string* mutable_blocksbuffers(int index);
  void set_blocksbuffers(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_blocksbuffers(int index, ::std::string&& value);
  #endif
  void set_blocksbuffers(int index, const char* value);
  void set_blocksbuffers(int index, const void* value, size_t size);
  ::std::string* add_blocksbuffers();
  void add_blocksbuffers(const ::std::string& value);
  #if LANG_CXX11
  void add_blocksbuffers(::std::string&& value);
  #endif
  void add_blocksbuffers(const char* value);
  void add_blocksbuffers(const void* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& blocksbuffers() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_blocksbuffers();

  // required .hadoop.hdfs.DatanodeStorageProto storage = 1;
  bool has_storage() const;
  void clear_storage();
  static const int kStorageFieldNumber = 1;
  private:
  const ::hadoop::hdfs::DatanodeStorageProto& _internal_storage() const;
  public:
  const ::hadoop::hdfs::DatanodeStorageProto& storage() const;
  ::hadoop::hdfs::DatanodeStorageProto* release_storage();
  ::hadoop::hdfs::DatanodeStorageProto* mutable_storage();
  void set_allocated_storage(::hadoop::hdfs::DatanodeStorageProto* storage);

  // optional uint64 numberOfBlocks = 3;
  bool has_numberofblocks() const;
  void clear_numberofblocks();
  static const int kNumberOfBlocksFieldNumber = 3;
  ::google::protobuf::uint64 numberofblocks() const;
  void set_numberofblocks(::google::protobuf::uint64 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.StorageBlockReportProto)
 private:
  void set_has_storage();
  void clear_has_storage();
  void set_has_numberofblocks();
  void clear_has_numberofblocks();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > blocks_;
  mutable int _blocks_cached_byte_size_;
  ::google::protobuf::RepeatedPtrField< ::std::string> blocksbuffers_;
  ::hadoop::hdfs::DatanodeStorageProto* storage_;
  ::google::protobuf::uint64 numberofblocks_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockReportResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockReportResponseProto) */ {
 public:
  BlockReportResponseProto();
  virtual ~BlockReportResponseProto();

  BlockReportResponseProto(const BlockReportResponseProto& from);

  inline BlockReportResponseProto& operator=(const BlockReportResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockReportResponseProto(BlockReportResponseProto&& from) noexcept
    : BlockReportResponseProto() {
    *this = ::std::move(from);
  }

  inline BlockReportResponseProto& operator=(BlockReportResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockReportResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockReportResponseProto* internal_default_instance() {
    return reinterpret_cast<const BlockReportResponseProto*>(
               &_BlockReportResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    18;

  void Swap(BlockReportResponseProto* other);
  friend void swap(BlockReportResponseProto& a, BlockReportResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockReportResponseProto* New() const final {
    return CreateMaybeMessage<BlockReportResponseProto>(NULL);
  }

  BlockReportResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockReportResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockReportResponseProto& from);
  void MergeFrom(const BlockReportResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockReportResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  bool has_cmd() const;
  void clear_cmd();
  static const int kCmdFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeCommandProto& _internal_cmd() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeCommandProto& cmd() const;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* release_cmd();
  ::hadoop::hdfs::datanode::DatanodeCommandProto* mutable_cmd();
  void set_allocated_cmd(::hadoop::hdfs::datanode::DatanodeCommandProto* cmd);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockReportResponseProto)
 private:
  void set_has_cmd();
  void clear_has_cmd();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* cmd_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CacheReportRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.CacheReportRequestProto) */ {
 public:
  CacheReportRequestProto();
  virtual ~CacheReportRequestProto();

  CacheReportRequestProto(const CacheReportRequestProto& from);

  inline CacheReportRequestProto& operator=(const CacheReportRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  CacheReportRequestProto(CacheReportRequestProto&& from) noexcept
    : CacheReportRequestProto() {
    *this = ::std::move(from);
  }

  inline CacheReportRequestProto& operator=(CacheReportRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const CacheReportRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const CacheReportRequestProto* internal_default_instance() {
    return reinterpret_cast<const CacheReportRequestProto*>(
               &_CacheReportRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    19;

  void Swap(CacheReportRequestProto* other);
  friend void swap(CacheReportRequestProto& a, CacheReportRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline CacheReportRequestProto* New() const final {
    return CreateMaybeMessage<CacheReportRequestProto>(NULL);
  }

  CacheReportRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<CacheReportRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const CacheReportRequestProto& from);
  void MergeFrom(const CacheReportRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CacheReportRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated uint64 blocks = 3 [packed = true];
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 3;
  ::google::protobuf::uint64 blocks(int index) const;
  void set_blocks(int index, ::google::protobuf::uint64 value);
  void add_blocks(::google::protobuf::uint64 value);
  const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
      blocks() const;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
      mutable_blocks();

  // required string blockPoolId = 2;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 2;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.CacheReportRequestProto)
 private:
  void set_has_registration();
  void clear_has_registration();
  void set_has_blockpoolid();
  void clear_has_blockpoolid();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedField< ::google::protobuf::uint64 > blocks_;
  mutable int _blocks_cached_byte_size_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CacheReportResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.CacheReportResponseProto) */ {
 public:
  CacheReportResponseProto();
  virtual ~CacheReportResponseProto();

  CacheReportResponseProto(const CacheReportResponseProto& from);

  inline CacheReportResponseProto& operator=(const CacheReportResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  CacheReportResponseProto(CacheReportResponseProto&& from) noexcept
    : CacheReportResponseProto() {
    *this = ::std::move(from);
  }

  inline CacheReportResponseProto& operator=(CacheReportResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const CacheReportResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const CacheReportResponseProto* internal_default_instance() {
    return reinterpret_cast<const CacheReportResponseProto*>(
               &_CacheReportResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    20;

  void Swap(CacheReportResponseProto* other);
  friend void swap(CacheReportResponseProto& a, CacheReportResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline CacheReportResponseProto* New() const final {
    return CreateMaybeMessage<CacheReportResponseProto>(NULL);
  }

  CacheReportResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<CacheReportResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const CacheReportResponseProto& from);
  void MergeFrom(const CacheReportResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CacheReportResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
  bool has_cmd() const;
  void clear_cmd();
  static const int kCmdFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeCommandProto& _internal_cmd() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeCommandProto& cmd() const;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* release_cmd();
  ::hadoop::hdfs::datanode::DatanodeCommandProto* mutable_cmd();
  void set_allocated_cmd(::hadoop::hdfs::datanode::DatanodeCommandProto* cmd);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.CacheReportResponseProto)
 private:
  void set_has_cmd();
  void clear_has_cmd();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::hadoop::hdfs::datanode::DatanodeCommandProto* cmd_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReceivedDeletedBlockInfoProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto) */ {
 public:
  ReceivedDeletedBlockInfoProto();
  virtual ~ReceivedDeletedBlockInfoProto();

  ReceivedDeletedBlockInfoProto(const ReceivedDeletedBlockInfoProto& from);

  inline ReceivedDeletedBlockInfoProto& operator=(const ReceivedDeletedBlockInfoProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ReceivedDeletedBlockInfoProto(ReceivedDeletedBlockInfoProto&& from) noexcept
    : ReceivedDeletedBlockInfoProto() {
    *this = ::std::move(from);
  }

  inline ReceivedDeletedBlockInfoProto& operator=(ReceivedDeletedBlockInfoProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const ReceivedDeletedBlockInfoProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ReceivedDeletedBlockInfoProto* internal_default_instance() {
    return reinterpret_cast<const ReceivedDeletedBlockInfoProto*>(
               &_ReceivedDeletedBlockInfoProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    21;

  void Swap(ReceivedDeletedBlockInfoProto* other);
  friend void swap(ReceivedDeletedBlockInfoProto& a, ReceivedDeletedBlockInfoProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ReceivedDeletedBlockInfoProto* New() const final {
    return CreateMaybeMessage<ReceivedDeletedBlockInfoProto>(NULL);
  }

  ReceivedDeletedBlockInfoProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ReceivedDeletedBlockInfoProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ReceivedDeletedBlockInfoProto& from);
  void MergeFrom(const ReceivedDeletedBlockInfoProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ReceivedDeletedBlockInfoProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ReceivedDeletedBlockInfoProto_BlockStatus BlockStatus;
  static const BlockStatus RECEIVING =
    ReceivedDeletedBlockInfoProto_BlockStatus_RECEIVING;
  static const BlockStatus RECEIVED =
    ReceivedDeletedBlockInfoProto_BlockStatus_RECEIVED;
  static const BlockStatus DELETED =
    ReceivedDeletedBlockInfoProto_BlockStatus_DELETED;
  static inline bool BlockStatus_IsValid(int value) {
    return ReceivedDeletedBlockInfoProto_BlockStatus_IsValid(value);
  }
  static const BlockStatus BlockStatus_MIN =
    ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_MIN;
  static const BlockStatus BlockStatus_MAX =
    ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_MAX;
  static const int BlockStatus_ARRAYSIZE =
    ReceivedDeletedBlockInfoProto_BlockStatus_BlockStatus_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  BlockStatus_descriptor() {
    return ReceivedDeletedBlockInfoProto_BlockStatus_descriptor();
  }
  static inline const ::std::string& BlockStatus_Name(BlockStatus value) {
    return ReceivedDeletedBlockInfoProto_BlockStatus_Name(value);
  }
  static inline bool BlockStatus_Parse(const ::std::string& name,
      BlockStatus* value) {
    return ReceivedDeletedBlockInfoProto_BlockStatus_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // optional string deleteHint = 2;
  bool has_deletehint() const;
  void clear_deletehint();
  static const int kDeleteHintFieldNumber = 2;
  const ::std::string& deletehint() const;
  void set_deletehint(const ::std::string& value);
  #if LANG_CXX11
  void set_deletehint(::std::string&& value);
  #endif
  void set_deletehint(const char* value);
  void set_deletehint(const char* value, size_t size);
  ::std::string* mutable_deletehint();
  ::std::string* release_deletehint();
  void set_allocated_deletehint(::std::string* deletehint);

  // required .hadoop.hdfs.BlockProto block = 1;
  bool has_block() const;
  void clear_block();
  static const int kBlockFieldNumber = 1;
  private:
  const ::hadoop::hdfs::BlockProto& _internal_block() const;
  public:
  const ::hadoop::hdfs::BlockProto& block() const;
  ::hadoop::hdfs::BlockProto* release_block();
  ::hadoop::hdfs::BlockProto* mutable_block();
  void set_allocated_block(::hadoop::hdfs::BlockProto* block);

  // required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
  bool has_status() const;
  void clear_status();
  static const int kStatusFieldNumber = 3;
  ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus status() const;
  void set_status(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto)
 private:
  void set_has_block();
  void clear_has_block();
  void set_has_status();
  void clear_has_status();
  void set_has_deletehint();
  void clear_has_deletehint();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr deletehint_;
  ::hadoop::hdfs::BlockProto* block_;
  int status_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class StorageReceivedDeletedBlocksProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto) */ {
 public:
  StorageReceivedDeletedBlocksProto();
  virtual ~StorageReceivedDeletedBlocksProto();

  StorageReceivedDeletedBlocksProto(const StorageReceivedDeletedBlocksProto& from);

  inline StorageReceivedDeletedBlocksProto& operator=(const StorageReceivedDeletedBlocksProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  StorageReceivedDeletedBlocksProto(StorageReceivedDeletedBlocksProto&& from) noexcept
    : StorageReceivedDeletedBlocksProto() {
    *this = ::std::move(from);
  }

  inline StorageReceivedDeletedBlocksProto& operator=(StorageReceivedDeletedBlocksProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const StorageReceivedDeletedBlocksProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const StorageReceivedDeletedBlocksProto* internal_default_instance() {
    return reinterpret_cast<const StorageReceivedDeletedBlocksProto*>(
               &_StorageReceivedDeletedBlocksProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    22;

  void Swap(StorageReceivedDeletedBlocksProto* other);
  friend void swap(StorageReceivedDeletedBlocksProto& a, StorageReceivedDeletedBlocksProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline StorageReceivedDeletedBlocksProto* New() const final {
    return CreateMaybeMessage<StorageReceivedDeletedBlocksProto>(NULL);
  }

  StorageReceivedDeletedBlocksProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<StorageReceivedDeletedBlocksProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const StorageReceivedDeletedBlocksProto& from);
  void MergeFrom(const StorageReceivedDeletedBlocksProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(StorageReceivedDeletedBlocksProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 2;
  ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* mutable_blocks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >*
      mutable_blocks();
  const ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto& blocks(int index) const;
  ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* add_blocks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >&
      blocks() const;

  // required string storageUuid = 1 [deprecated = true];
  GOOGLE_PROTOBUF_DEPRECATED_ATTR bool has_storageuuid() const;
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void clear_storageuuid();
  GOOGLE_PROTOBUF_DEPRECATED_ATTR static const int kStorageUuidFieldNumber = 1;
  GOOGLE_PROTOBUF_DEPRECATED_ATTR const ::std::string& storageuuid() const;
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_storageuuid(const ::std::string& value);
  #if LANG_CXX11
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_storageuuid(::std::string&& value);
  #endif
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_storageuuid(const char* value);
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_storageuuid(const char* value, size_t size);
  GOOGLE_PROTOBUF_DEPRECATED_ATTR ::std::string* mutable_storageuuid();
  GOOGLE_PROTOBUF_DEPRECATED_ATTR ::std::string* release_storageuuid();
  GOOGLE_PROTOBUF_DEPRECATED_ATTR void set_allocated_storageuuid(::std::string* storageuuid);

  // optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
  bool has_storage() const;
  void clear_storage();
  static const int kStorageFieldNumber = 3;
  private:
  const ::hadoop::hdfs::DatanodeStorageProto& _internal_storage() const;
  public:
  const ::hadoop::hdfs::DatanodeStorageProto& storage() const;
  ::hadoop::hdfs::DatanodeStorageProto* release_storage();
  ::hadoop::hdfs::DatanodeStorageProto* mutable_storage();
  void set_allocated_storage(::hadoop::hdfs::DatanodeStorageProto* storage);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto)
 private:
  void set_has_storageuuid();
  void clear_has_storageuuid();
  void set_has_storage();
  void clear_has_storage();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto > blocks_;
  ::google::protobuf::internal::ArenaStringPtr storageuuid_;
  ::hadoop::hdfs::DatanodeStorageProto* storage_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockReceivedAndDeletedRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto) */ {
 public:
  BlockReceivedAndDeletedRequestProto();
  virtual ~BlockReceivedAndDeletedRequestProto();

  BlockReceivedAndDeletedRequestProto(const BlockReceivedAndDeletedRequestProto& from);

  inline BlockReceivedAndDeletedRequestProto& operator=(const BlockReceivedAndDeletedRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockReceivedAndDeletedRequestProto(BlockReceivedAndDeletedRequestProto&& from) noexcept
    : BlockReceivedAndDeletedRequestProto() {
    *this = ::std::move(from);
  }

  inline BlockReceivedAndDeletedRequestProto& operator=(BlockReceivedAndDeletedRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockReceivedAndDeletedRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockReceivedAndDeletedRequestProto* internal_default_instance() {
    return reinterpret_cast<const BlockReceivedAndDeletedRequestProto*>(
               &_BlockReceivedAndDeletedRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    23;

  void Swap(BlockReceivedAndDeletedRequestProto* other);
  friend void swap(BlockReceivedAndDeletedRequestProto& a, BlockReceivedAndDeletedRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockReceivedAndDeletedRequestProto* New() const final {
    return CreateMaybeMessage<BlockReceivedAndDeletedRequestProto>(NULL);
  }

  BlockReceivedAndDeletedRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockReceivedAndDeletedRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockReceivedAndDeletedRequestProto& from);
  void MergeFrom(const BlockReceivedAndDeletedRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockReceivedAndDeletedRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 3;
  ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* mutable_blocks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >*
      mutable_blocks();
  const ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto& blocks(int index) const;
  ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* add_blocks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >&
      blocks() const;

  // required string blockPoolId = 2;
  bool has_blockpoolid() const;
  void clear_blockpoolid();
  static const int kBlockPoolIdFieldNumber = 2;
  const ::std::string& blockpoolid() const;
  void set_blockpoolid(const ::std::string& value);
  #if LANG_CXX11
  void set_blockpoolid(::std::string&& value);
  #endif
  void set_blockpoolid(const char* value);
  void set_blockpoolid(const char* value, size_t size);
  ::std::string* mutable_blockpoolid();
  ::std::string* release_blockpoolid();
  void set_allocated_blockpoolid(::std::string* blockpoolid);

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
  bool has_registration() const;
  void clear_registration();
  static const int kRegistrationFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registration() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registration() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registration();
  void set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto)
 private:
  void set_has_registration();
  void clear_has_registration();
  void set_has_blockpoolid();
  void clear_has_blockpoolid();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto > blocks_;
  ::google::protobuf::internal::ArenaStringPtr blockpoolid_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class BlockReceivedAndDeletedResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto) */ {
 public:
  BlockReceivedAndDeletedResponseProto();
  virtual ~BlockReceivedAndDeletedResponseProto();

  BlockReceivedAndDeletedResponseProto(const BlockReceivedAndDeletedResponseProto& from);

  inline BlockReceivedAndDeletedResponseProto& operator=(const BlockReceivedAndDeletedResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  BlockReceivedAndDeletedResponseProto(BlockReceivedAndDeletedResponseProto&& from) noexcept
    : BlockReceivedAndDeletedResponseProto() {
    *this = ::std::move(from);
  }

  inline BlockReceivedAndDeletedResponseProto& operator=(BlockReceivedAndDeletedResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const BlockReceivedAndDeletedResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const BlockReceivedAndDeletedResponseProto* internal_default_instance() {
    return reinterpret_cast<const BlockReceivedAndDeletedResponseProto*>(
               &_BlockReceivedAndDeletedResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    24;

  void Swap(BlockReceivedAndDeletedResponseProto* other);
  friend void swap(BlockReceivedAndDeletedResponseProto& a, BlockReceivedAndDeletedResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline BlockReceivedAndDeletedResponseProto* New() const final {
    return CreateMaybeMessage<BlockReceivedAndDeletedResponseProto>(NULL);
  }

  BlockReceivedAndDeletedResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<BlockReceivedAndDeletedResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const BlockReceivedAndDeletedResponseProto& from);
  void MergeFrom(const BlockReceivedAndDeletedResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(BlockReceivedAndDeletedResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.BlockReceivedAndDeletedResponseProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ErrorReportRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.ErrorReportRequestProto) */ {
 public:
  ErrorReportRequestProto();
  virtual ~ErrorReportRequestProto();

  ErrorReportRequestProto(const ErrorReportRequestProto& from);

  inline ErrorReportRequestProto& operator=(const ErrorReportRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ErrorReportRequestProto(ErrorReportRequestProto&& from) noexcept
    : ErrorReportRequestProto() {
    *this = ::std::move(from);
  }

  inline ErrorReportRequestProto& operator=(ErrorReportRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const ErrorReportRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ErrorReportRequestProto* internal_default_instance() {
    return reinterpret_cast<const ErrorReportRequestProto*>(
               &_ErrorReportRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    25;

  void Swap(ErrorReportRequestProto* other);
  friend void swap(ErrorReportRequestProto& a, ErrorReportRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ErrorReportRequestProto* New() const final {
    return CreateMaybeMessage<ErrorReportRequestProto>(NULL);
  }

  ErrorReportRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ErrorReportRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ErrorReportRequestProto& from);
  void MergeFrom(const ErrorReportRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ErrorReportRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  typedef ErrorReportRequestProto_ErrorCode ErrorCode;
  static const ErrorCode NOTIFY =
    ErrorReportRequestProto_ErrorCode_NOTIFY;
  static const ErrorCode DISK_ERROR =
    ErrorReportRequestProto_ErrorCode_DISK_ERROR;
  static const ErrorCode INVALID_BLOCK =
    ErrorReportRequestProto_ErrorCode_INVALID_BLOCK;
  static const ErrorCode FATAL_DISK_ERROR =
    ErrorReportRequestProto_ErrorCode_FATAL_DISK_ERROR;
  static inline bool ErrorCode_IsValid(int value) {
    return ErrorReportRequestProto_ErrorCode_IsValid(value);
  }
  static const ErrorCode ErrorCode_MIN =
    ErrorReportRequestProto_ErrorCode_ErrorCode_MIN;
  static const ErrorCode ErrorCode_MAX =
    ErrorReportRequestProto_ErrorCode_ErrorCode_MAX;
  static const int ErrorCode_ARRAYSIZE =
    ErrorReportRequestProto_ErrorCode_ErrorCode_ARRAYSIZE;
  static inline const ::google::protobuf::EnumDescriptor*
  ErrorCode_descriptor() {
    return ErrorReportRequestProto_ErrorCode_descriptor();
  }
  static inline const ::std::string& ErrorCode_Name(ErrorCode value) {
    return ErrorReportRequestProto_ErrorCode_Name(value);
  }
  static inline bool ErrorCode_Parse(const ::std::string& name,
      ErrorCode* value) {
    return ErrorReportRequestProto_ErrorCode_Parse(name, value);
  }

  // accessors -------------------------------------------------------

  // required string msg = 3;
  bool has_msg() const;
  void clear_msg();
  static const int kMsgFieldNumber = 3;
  const ::std::string& msg() const;
  void set_msg(const ::std::string& value);
  #if LANG_CXX11
  void set_msg(::std::string&& value);
  #endif
  void set_msg(const char* value);
  void set_msg(const char* value, size_t size);
  ::std::string* mutable_msg();
  ::std::string* release_msg();
  void set_allocated_msg(::std::string* msg);

  // required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
  bool has_registartion() const;
  void clear_registartion();
  static const int kRegistartionFieldNumber = 1;
  private:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& _internal_registartion() const;
  public:
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& registartion() const;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* release_registartion();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* mutable_registartion();
  void set_allocated_registartion(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registartion);

  // required uint32 errorCode = 2;
  bool has_errorcode() const;
  void clear_errorcode();
  static const int kErrorCodeFieldNumber = 2;
  ::google::protobuf::uint32 errorcode() const;
  void set_errorcode(::google::protobuf::uint32 value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.ErrorReportRequestProto)
 private:
  void set_has_registartion();
  void clear_has_registartion();
  void set_has_errorcode();
  void clear_has_errorcode();
  void set_has_msg();
  void clear_has_msg();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr msg_;
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* registartion_;
  ::google::protobuf::uint32 errorcode_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ErrorReportResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.ErrorReportResponseProto) */ {
 public:
  ErrorReportResponseProto();
  virtual ~ErrorReportResponseProto();

  ErrorReportResponseProto(const ErrorReportResponseProto& from);

  inline ErrorReportResponseProto& operator=(const ErrorReportResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ErrorReportResponseProto(ErrorReportResponseProto&& from) noexcept
    : ErrorReportResponseProto() {
    *this = ::std::move(from);
  }

  inline ErrorReportResponseProto& operator=(ErrorReportResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const ErrorReportResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ErrorReportResponseProto* internal_default_instance() {
    return reinterpret_cast<const ErrorReportResponseProto*>(
               &_ErrorReportResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    26;

  void Swap(ErrorReportResponseProto* other);
  friend void swap(ErrorReportResponseProto& a, ErrorReportResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ErrorReportResponseProto* New() const final {
    return CreateMaybeMessage<ErrorReportResponseProto>(NULL);
  }

  ErrorReportResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ErrorReportResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ErrorReportResponseProto& from);
  void MergeFrom(const ErrorReportResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ErrorReportResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.ErrorReportResponseProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReportBadBlocksRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.ReportBadBlocksRequestProto) */ {
 public:
  ReportBadBlocksRequestProto();
  virtual ~ReportBadBlocksRequestProto();

  ReportBadBlocksRequestProto(const ReportBadBlocksRequestProto& from);

  inline ReportBadBlocksRequestProto& operator=(const ReportBadBlocksRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ReportBadBlocksRequestProto(ReportBadBlocksRequestProto&& from) noexcept
    : ReportBadBlocksRequestProto() {
    *this = ::std::move(from);
  }

  inline ReportBadBlocksRequestProto& operator=(ReportBadBlocksRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const ReportBadBlocksRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ReportBadBlocksRequestProto* internal_default_instance() {
    return reinterpret_cast<const ReportBadBlocksRequestProto*>(
               &_ReportBadBlocksRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    27;

  void Swap(ReportBadBlocksRequestProto* other);
  friend void swap(ReportBadBlocksRequestProto& a, ReportBadBlocksRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ReportBadBlocksRequestProto* New() const final {
    return CreateMaybeMessage<ReportBadBlocksRequestProto>(NULL);
  }

  ReportBadBlocksRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ReportBadBlocksRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ReportBadBlocksRequestProto& from);
  void MergeFrom(const ReportBadBlocksRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ReportBadBlocksRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
  int blocks_size() const;
  void clear_blocks();
  static const int kBlocksFieldNumber = 1;
  ::hadoop::hdfs::LocatedBlockProto* mutable_blocks(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::LocatedBlockProto >*
      mutable_blocks();
  const ::hadoop::hdfs::LocatedBlockProto& blocks(int index) const;
  ::hadoop::hdfs::LocatedBlockProto* add_blocks();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::LocatedBlockProto >&
      blocks() const;

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.ReportBadBlocksRequestProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::LocatedBlockProto > blocks_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class ReportBadBlocksResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.ReportBadBlocksResponseProto) */ {
 public:
  ReportBadBlocksResponseProto();
  virtual ~ReportBadBlocksResponseProto();

  ReportBadBlocksResponseProto(const ReportBadBlocksResponseProto& from);

  inline ReportBadBlocksResponseProto& operator=(const ReportBadBlocksResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  ReportBadBlocksResponseProto(ReportBadBlocksResponseProto&& from) noexcept
    : ReportBadBlocksResponseProto() {
    *this = ::std::move(from);
  }

  inline ReportBadBlocksResponseProto& operator=(ReportBadBlocksResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const ReportBadBlocksResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const ReportBadBlocksResponseProto* internal_default_instance() {
    return reinterpret_cast<const ReportBadBlocksResponseProto*>(
               &_ReportBadBlocksResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    28;

  void Swap(ReportBadBlocksResponseProto* other);
  friend void swap(ReportBadBlocksResponseProto& a, ReportBadBlocksResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline ReportBadBlocksResponseProto* New() const final {
    return CreateMaybeMessage<ReportBadBlocksResponseProto>(NULL);
  }

  ReportBadBlocksResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<ReportBadBlocksResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const ReportBadBlocksResponseProto& from);
  void MergeFrom(const ReportBadBlocksResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(ReportBadBlocksResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.ReportBadBlocksResponseProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CommitBlockSynchronizationRequestProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto) */ {
 public:
  CommitBlockSynchronizationRequestProto();
  virtual ~CommitBlockSynchronizationRequestProto();

  CommitBlockSynchronizationRequestProto(const CommitBlockSynchronizationRequestProto& from);

  inline CommitBlockSynchronizationRequestProto& operator=(const CommitBlockSynchronizationRequestProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  CommitBlockSynchronizationRequestProto(CommitBlockSynchronizationRequestProto&& from) noexcept
    : CommitBlockSynchronizationRequestProto() {
    *this = ::std::move(from);
  }

  inline CommitBlockSynchronizationRequestProto& operator=(CommitBlockSynchronizationRequestProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const CommitBlockSynchronizationRequestProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const CommitBlockSynchronizationRequestProto* internal_default_instance() {
    return reinterpret_cast<const CommitBlockSynchronizationRequestProto*>(
               &_CommitBlockSynchronizationRequestProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    29;

  void Swap(CommitBlockSynchronizationRequestProto* other);
  friend void swap(CommitBlockSynchronizationRequestProto& a, CommitBlockSynchronizationRequestProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline CommitBlockSynchronizationRequestProto* New() const final {
    return CreateMaybeMessage<CommitBlockSynchronizationRequestProto>(NULL);
  }

  CommitBlockSynchronizationRequestProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<CommitBlockSynchronizationRequestProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const CommitBlockSynchronizationRequestProto& from);
  void MergeFrom(const CommitBlockSynchronizationRequestProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CommitBlockSynchronizationRequestProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
  int newtaragets_size() const;
  void clear_newtaragets();
  static const int kNewTaragetsFieldNumber = 6;
  ::hadoop::hdfs::DatanodeIDProto* mutable_newtaragets(int index);
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeIDProto >*
      mutable_newtaragets();
  const ::hadoop::hdfs::DatanodeIDProto& newtaragets(int index) const;
  ::hadoop::hdfs::DatanodeIDProto* add_newtaragets();
  const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeIDProto >&
      newtaragets() const;

  // repeated string newTargetStorages = 7;
  int newtargetstorages_size() const;
  void clear_newtargetstorages();
  static const int kNewTargetStoragesFieldNumber = 7;
  const ::std::string& newtargetstorages(int index) const;
  ::std::string* mutable_newtargetstorages(int index);
  void set_newtargetstorages(int index, const ::std::string& value);
  #if LANG_CXX11
  void set_newtargetstorages(int index, ::std::string&& value);
  #endif
  void set_newtargetstorages(int index, const char* value);
  void set_newtargetstorages(int index, const char* value, size_t size);
  ::std::string* add_newtargetstorages();
  void add_newtargetstorages(const ::std::string& value);
  #if LANG_CXX11
  void add_newtargetstorages(::std::string&& value);
  #endif
  void add_newtargetstorages(const char* value);
  void add_newtargetstorages(const char* value, size_t size);
  const ::google::protobuf::RepeatedPtrField< ::std::string>& newtargetstorages() const;
  ::google::protobuf::RepeatedPtrField< ::std::string>* mutable_newtargetstorages();

  // required .hadoop.hdfs.ExtendedBlockProto block = 1;
  bool has_block() const;
  void clear_block();
  static const int kBlockFieldNumber = 1;
  private:
  const ::hadoop::hdfs::ExtendedBlockProto& _internal_block() const;
  public:
  const ::hadoop::hdfs::ExtendedBlockProto& block() const;
  ::hadoop::hdfs::ExtendedBlockProto* release_block();
  ::hadoop::hdfs::ExtendedBlockProto* mutable_block();
  void set_allocated_block(::hadoop::hdfs::ExtendedBlockProto* block);

  // required uint64 newGenStamp = 2;
  bool has_newgenstamp() const;
  void clear_newgenstamp();
  static const int kNewGenStampFieldNumber = 2;
  ::google::protobuf::uint64 newgenstamp() const;
  void set_newgenstamp(::google::protobuf::uint64 value);

  // required uint64 newLength = 3;
  bool has_newlength() const;
  void clear_newlength();
  static const int kNewLengthFieldNumber = 3;
  ::google::protobuf::uint64 newlength() const;
  void set_newlength(::google::protobuf::uint64 value);

  // required bool closeFile = 4;
  bool has_closefile() const;
  void clear_closefile();
  static const int kCloseFileFieldNumber = 4;
  bool closefile() const;
  void set_closefile(bool value);

  // required bool deleteBlock = 5;
  bool has_deleteblock() const;
  void clear_deleteblock();
  static const int kDeleteBlockFieldNumber = 5;
  bool deleteblock() const;
  void set_deleteblock(bool value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto)
 private:
  void set_has_block();
  void clear_has_block();
  void set_has_newgenstamp();
  void clear_has_newgenstamp();
  void set_has_newlength();
  void clear_has_newlength();
  void set_has_closefile();
  void clear_has_closefile();
  void set_has_deleteblock();
  void clear_has_deleteblock();

  // helper for ByteSizeLong()
  size_t RequiredFieldsByteSizeFallback() const;

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeIDProto > newtaragets_;
  ::google::protobuf::RepeatedPtrField< ::std::string> newtargetstorages_;
  ::hadoop::hdfs::ExtendedBlockProto* block_;
  ::google::protobuf::uint64 newgenstamp_;
  ::google::protobuf::uint64 newlength_;
  bool closefile_;
  bool deleteblock_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class CommitBlockSynchronizationResponseProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto) */ {
 public:
  CommitBlockSynchronizationResponseProto();
  virtual ~CommitBlockSynchronizationResponseProto();

  CommitBlockSynchronizationResponseProto(const CommitBlockSynchronizationResponseProto& from);

  inline CommitBlockSynchronizationResponseProto& operator=(const CommitBlockSynchronizationResponseProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  CommitBlockSynchronizationResponseProto(CommitBlockSynchronizationResponseProto&& from) noexcept
    : CommitBlockSynchronizationResponseProto() {
    *this = ::std::move(from);
  }

  inline CommitBlockSynchronizationResponseProto& operator=(CommitBlockSynchronizationResponseProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const CommitBlockSynchronizationResponseProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const CommitBlockSynchronizationResponseProto* internal_default_instance() {
    return reinterpret_cast<const CommitBlockSynchronizationResponseProto*>(
               &_CommitBlockSynchronizationResponseProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    30;

  void Swap(CommitBlockSynchronizationResponseProto* other);
  friend void swap(CommitBlockSynchronizationResponseProto& a, CommitBlockSynchronizationResponseProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline CommitBlockSynchronizationResponseProto* New() const final {
    return CreateMaybeMessage<CommitBlockSynchronizationResponseProto>(NULL);
  }

  CommitBlockSynchronizationResponseProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<CommitBlockSynchronizationResponseProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const CommitBlockSynchronizationResponseProto& from);
  void MergeFrom(const CommitBlockSynchronizationResponseProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(CommitBlockSynchronizationResponseProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.CommitBlockSynchronizationResponseProto)
 private:

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SlowPeerReportProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.SlowPeerReportProto) */ {
 public:
  SlowPeerReportProto();
  virtual ~SlowPeerReportProto();

  SlowPeerReportProto(const SlowPeerReportProto& from);

  inline SlowPeerReportProto& operator=(const SlowPeerReportProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SlowPeerReportProto(SlowPeerReportProto&& from) noexcept
    : SlowPeerReportProto() {
    *this = ::std::move(from);
  }

  inline SlowPeerReportProto& operator=(SlowPeerReportProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const SlowPeerReportProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SlowPeerReportProto* internal_default_instance() {
    return reinterpret_cast<const SlowPeerReportProto*>(
               &_SlowPeerReportProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    31;

  void Swap(SlowPeerReportProto* other);
  friend void swap(SlowPeerReportProto& a, SlowPeerReportProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SlowPeerReportProto* New() const final {
    return CreateMaybeMessage<SlowPeerReportProto>(NULL);
  }

  SlowPeerReportProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SlowPeerReportProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SlowPeerReportProto& from);
  void MergeFrom(const SlowPeerReportProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SlowPeerReportProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional string dataNodeId = 1;
  bool has_datanodeid() const;
  void clear_datanodeid();
  static const int kDataNodeIdFieldNumber = 1;
  const ::std::string& datanodeid() const;
  void set_datanodeid(const ::std::string& value);
  #if LANG_CXX11
  void set_datanodeid(::std::string&& value);
  #endif
  void set_datanodeid(const char* value);
  void set_datanodeid(const char* value, size_t size);
  ::std::string* mutable_datanodeid();
  ::std::string* release_datanodeid();
  void set_allocated_datanodeid(::std::string* datanodeid);

  // optional double aggregateLatency = 2;
  bool has_aggregatelatency() const;
  void clear_aggregatelatency();
  static const int kAggregateLatencyFieldNumber = 2;
  double aggregatelatency() const;
  void set_aggregatelatency(double value);

  // optional double median = 3;
  bool has_median() const;
  void clear_median();
  static const int kMedianFieldNumber = 3;
  double median() const;
  void set_median(double value);

  // optional double mad = 4;
  bool has_mad() const;
  void clear_mad();
  static const int kMadFieldNumber = 4;
  double mad() const;
  void set_mad(double value);

  // optional double upperLimitLatency = 5;
  bool has_upperlimitlatency() const;
  void clear_upperlimitlatency();
  static const int kUpperLimitLatencyFieldNumber = 5;
  double upperlimitlatency() const;
  void set_upperlimitlatency(double value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.SlowPeerReportProto)
 private:
  void set_has_datanodeid();
  void clear_has_datanodeid();
  void set_has_aggregatelatency();
  void clear_has_aggregatelatency();
  void set_has_median();
  void clear_has_median();
  void set_has_mad();
  void clear_has_mad();
  void set_has_upperlimitlatency();
  void clear_has_upperlimitlatency();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr datanodeid_;
  double aggregatelatency_;
  double median_;
  double mad_;
  double upperlimitlatency_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// -------------------------------------------------------------------

class SlowDiskReportProto : public ::google::protobuf::Message /* @@protoc_insertion_point(class_definition:hadoop.hdfs.datanode.SlowDiskReportProto) */ {
 public:
  SlowDiskReportProto();
  virtual ~SlowDiskReportProto();

  SlowDiskReportProto(const SlowDiskReportProto& from);

  inline SlowDiskReportProto& operator=(const SlowDiskReportProto& from) {
    CopyFrom(from);
    return *this;
  }
  #if LANG_CXX11
  SlowDiskReportProto(SlowDiskReportProto&& from) noexcept
    : SlowDiskReportProto() {
    *this = ::std::move(from);
  }

  inline SlowDiskReportProto& operator=(SlowDiskReportProto&& from) noexcept {
    if (GetArenaNoVirtual() == from.GetArenaNoVirtual()) {
      if (this != &from) InternalSwap(&from);
    } else {
      CopyFrom(from);
    }
    return *this;
  }
  #endif
  inline const ::google::protobuf::UnknownFieldSet& unknown_fields() const {
    return _internal_metadata_.unknown_fields();
  }
  inline ::google::protobuf::UnknownFieldSet* mutable_unknown_fields() {
    return _internal_metadata_.mutable_unknown_fields();
  }

  static const ::google::protobuf::Descriptor* descriptor();
  static const SlowDiskReportProto& default_instance();

  static void InitAsDefaultInstance();  // FOR INTERNAL USE ONLY
  static inline const SlowDiskReportProto* internal_default_instance() {
    return reinterpret_cast<const SlowDiskReportProto*>(
               &_SlowDiskReportProto_default_instance_);
  }
  static constexpr int kIndexInFileMessages =
    32;

  void Swap(SlowDiskReportProto* other);
  friend void swap(SlowDiskReportProto& a, SlowDiskReportProto& b) {
    a.Swap(&b);
  }

  // implements Message ----------------------------------------------

  inline SlowDiskReportProto* New() const final {
    return CreateMaybeMessage<SlowDiskReportProto>(NULL);
  }

  SlowDiskReportProto* New(::google::protobuf::Arena* arena) const final {
    return CreateMaybeMessage<SlowDiskReportProto>(arena);
  }
  void CopyFrom(const ::google::protobuf::Message& from) final;
  void MergeFrom(const ::google::protobuf::Message& from) final;
  void CopyFrom(const SlowDiskReportProto& from);
  void MergeFrom(const SlowDiskReportProto& from);
  void Clear() final;
  bool IsInitialized() const final;

  size_t ByteSizeLong() const final;
  bool MergePartialFromCodedStream(
      ::google::protobuf::io::CodedInputStream* input) final;
  void SerializeWithCachedSizes(
      ::google::protobuf::io::CodedOutputStream* output) const final;
  ::google::protobuf::uint8* InternalSerializeWithCachedSizesToArray(
      bool deterministic, ::google::protobuf::uint8* target) const final;
  int GetCachedSize() const final { return _cached_size_.Get(); }

  private:
  void SharedCtor();
  void SharedDtor();
  void SetCachedSize(int size) const final;
  void InternalSwap(SlowDiskReportProto* other);
  private:
  inline ::google::protobuf::Arena* GetArenaNoVirtual() const {
    return NULL;
  }
  inline void* MaybeArenaPtr() const {
    return NULL;
  }
  public:

  ::google::protobuf::Metadata GetMetadata() const final;

  // nested types ----------------------------------------------------

  // accessors -------------------------------------------------------

  // optional string basePath = 1;
  bool has_basepath() const;
  void clear_basepath();
  static const int kBasePathFieldNumber = 1;
  const ::std::string& basepath() const;
  void set_basepath(const ::std::string& value);
  #if LANG_CXX11
  void set_basepath(::std::string&& value);
  #endif
  void set_basepath(const char* value);
  void set_basepath(const char* value, size_t size);
  ::std::string* mutable_basepath();
  ::std::string* release_basepath();
  void set_allocated_basepath(::std::string* basepath);

  // optional double meanMetadataOpLatency = 2;
  bool has_meanmetadataoplatency() const;
  void clear_meanmetadataoplatency();
  static const int kMeanMetadataOpLatencyFieldNumber = 2;
  double meanmetadataoplatency() const;
  void set_meanmetadataoplatency(double value);

  // optional double meanReadIoLatency = 3;
  bool has_meanreadiolatency() const;
  void clear_meanreadiolatency();
  static const int kMeanReadIoLatencyFieldNumber = 3;
  double meanreadiolatency() const;
  void set_meanreadiolatency(double value);

  // optional double meanWriteIoLatency = 4;
  bool has_meanwriteiolatency() const;
  void clear_meanwriteiolatency();
  static const int kMeanWriteIoLatencyFieldNumber = 4;
  double meanwriteiolatency() const;
  void set_meanwriteiolatency(double value);

  // @@protoc_insertion_point(class_scope:hadoop.hdfs.datanode.SlowDiskReportProto)
 private:
  void set_has_basepath();
  void clear_has_basepath();
  void set_has_meanmetadataoplatency();
  void clear_has_meanmetadataoplatency();
  void set_has_meanreadiolatency();
  void clear_has_meanreadiolatency();
  void set_has_meanwriteiolatency();
  void clear_has_meanwriteiolatency();

  ::google::protobuf::internal::InternalMetadataWithArena _internal_metadata_;
  ::google::protobuf::internal::HasBits<1> _has_bits_;
  mutable ::google::protobuf::internal::CachedSize _cached_size_;
  ::google::protobuf::internal::ArenaStringPtr basepath_;
  double meanmetadataoplatency_;
  double meanreadiolatency_;
  double meanwriteiolatency_;
  friend struct ::protobuf_DatanodeProtocol_2eproto::TableStruct;
};
// ===================================================================


// ===================================================================

#ifdef __GNUC__
  #pragma GCC diagnostic push
  #pragma GCC diagnostic ignored "-Wstrict-aliasing"
#endif  // __GNUC__
// DatanodeRegistrationProto

// required .hadoop.hdfs.DatanodeIDProto datanodeID = 1;
inline bool DatanodeRegistrationProto::has_datanodeid() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void DatanodeRegistrationProto::set_has_datanodeid() {
  _has_bits_[0] |= 0x00000002u;
}
inline void DatanodeRegistrationProto::clear_has_datanodeid() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::hadoop::hdfs::DatanodeIDProto& DatanodeRegistrationProto::_internal_datanodeid() const {
  return *datanodeid_;
}
inline const ::hadoop::hdfs::DatanodeIDProto& DatanodeRegistrationProto::datanodeid() const {
  const ::hadoop::hdfs::DatanodeIDProto* p = datanodeid_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeRegistrationProto.datanodeID)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::DatanodeIDProto*>(
      &::hadoop::hdfs::_DatanodeIDProto_default_instance_);
}
inline ::hadoop::hdfs::DatanodeIDProto* DatanodeRegistrationProto::release_datanodeid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeRegistrationProto.datanodeID)
  clear_has_datanodeid();
  ::hadoop::hdfs::DatanodeIDProto* temp = datanodeid_;
  datanodeid_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::DatanodeIDProto* DatanodeRegistrationProto::mutable_datanodeid() {
  set_has_datanodeid();
  if (datanodeid_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::DatanodeIDProto>(GetArenaNoVirtual());
    datanodeid_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeRegistrationProto.datanodeID)
  return datanodeid_;
}
inline void DatanodeRegistrationProto::set_allocated_datanodeid(::hadoop::hdfs::DatanodeIDProto* datanodeid) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(datanodeid_);
  }
  if (datanodeid) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      datanodeid = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, datanodeid, submessage_arena);
    }
    set_has_datanodeid();
  } else {
    clear_has_datanodeid();
  }
  datanodeid_ = datanodeid;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeRegistrationProto.datanodeID)
}

// required .hadoop.hdfs.StorageInfoProto storageInfo = 2;
inline bool DatanodeRegistrationProto::has_storageinfo() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void DatanodeRegistrationProto::set_has_storageinfo() {
  _has_bits_[0] |= 0x00000004u;
}
inline void DatanodeRegistrationProto::clear_has_storageinfo() {
  _has_bits_[0] &= ~0x00000004u;
}
inline const ::hadoop::hdfs::StorageInfoProto& DatanodeRegistrationProto::_internal_storageinfo() const {
  return *storageinfo_;
}
inline const ::hadoop::hdfs::StorageInfoProto& DatanodeRegistrationProto::storageinfo() const {
  const ::hadoop::hdfs::StorageInfoProto* p = storageinfo_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeRegistrationProto.storageInfo)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::StorageInfoProto*>(
      &::hadoop::hdfs::_StorageInfoProto_default_instance_);
}
inline ::hadoop::hdfs::StorageInfoProto* DatanodeRegistrationProto::release_storageinfo() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeRegistrationProto.storageInfo)
  clear_has_storageinfo();
  ::hadoop::hdfs::StorageInfoProto* temp = storageinfo_;
  storageinfo_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::StorageInfoProto* DatanodeRegistrationProto::mutable_storageinfo() {
  set_has_storageinfo();
  if (storageinfo_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::StorageInfoProto>(GetArenaNoVirtual());
    storageinfo_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeRegistrationProto.storageInfo)
  return storageinfo_;
}
inline void DatanodeRegistrationProto::set_allocated_storageinfo(::hadoop::hdfs::StorageInfoProto* storageinfo) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(storageinfo_);
  }
  if (storageinfo) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      storageinfo = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, storageinfo, submessage_arena);
    }
    set_has_storageinfo();
  } else {
    clear_has_storageinfo();
  }
  storageinfo_ = storageinfo;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeRegistrationProto.storageInfo)
}

// required .hadoop.hdfs.ExportedBlockKeysProto keys = 3;
inline bool DatanodeRegistrationProto::has_keys() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void DatanodeRegistrationProto::set_has_keys() {
  _has_bits_[0] |= 0x00000008u;
}
inline void DatanodeRegistrationProto::clear_has_keys() {
  _has_bits_[0] &= ~0x00000008u;
}
inline const ::hadoop::hdfs::ExportedBlockKeysProto& DatanodeRegistrationProto::_internal_keys() const {
  return *keys_;
}
inline const ::hadoop::hdfs::ExportedBlockKeysProto& DatanodeRegistrationProto::keys() const {
  const ::hadoop::hdfs::ExportedBlockKeysProto* p = keys_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeRegistrationProto.keys)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::ExportedBlockKeysProto*>(
      &::hadoop::hdfs::_ExportedBlockKeysProto_default_instance_);
}
inline ::hadoop::hdfs::ExportedBlockKeysProto* DatanodeRegistrationProto::release_keys() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeRegistrationProto.keys)
  clear_has_keys();
  ::hadoop::hdfs::ExportedBlockKeysProto* temp = keys_;
  keys_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::ExportedBlockKeysProto* DatanodeRegistrationProto::mutable_keys() {
  set_has_keys();
  if (keys_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::ExportedBlockKeysProto>(GetArenaNoVirtual());
    keys_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeRegistrationProto.keys)
  return keys_;
}
inline void DatanodeRegistrationProto::set_allocated_keys(::hadoop::hdfs::ExportedBlockKeysProto* keys) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(keys_);
  }
  if (keys) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      keys = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, keys, submessage_arena);
    }
    set_has_keys();
  } else {
    clear_has_keys();
  }
  keys_ = keys;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeRegistrationProto.keys)
}

// required string softwareVersion = 4;
inline bool DatanodeRegistrationProto::has_softwareversion() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void DatanodeRegistrationProto::set_has_softwareversion() {
  _has_bits_[0] |= 0x00000001u;
}
inline void DatanodeRegistrationProto::clear_has_softwareversion() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void DatanodeRegistrationProto::clear_softwareversion() {
  softwareversion_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_softwareversion();
}
inline const ::std::string& DatanodeRegistrationProto::softwareversion() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
  return softwareversion_.GetNoArena();
}
inline void DatanodeRegistrationProto::set_softwareversion(const ::std::string& value) {
  set_has_softwareversion();
  softwareversion_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
}
#if LANG_CXX11
inline void DatanodeRegistrationProto::set_softwareversion(::std::string&& value) {
  set_has_softwareversion();
  softwareversion_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
}
#endif
inline void DatanodeRegistrationProto::set_softwareversion(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_softwareversion();
  softwareversion_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
}
inline void DatanodeRegistrationProto::set_softwareversion(const char* value, size_t size) {
  set_has_softwareversion();
  softwareversion_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
}
inline ::std::string* DatanodeRegistrationProto::mutable_softwareversion() {
  set_has_softwareversion();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
  return softwareversion_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* DatanodeRegistrationProto::release_softwareversion() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
  if (!has_softwareversion()) {
    return NULL;
  }
  clear_has_softwareversion();
  return softwareversion_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void DatanodeRegistrationProto::set_allocated_softwareversion(::std::string* softwareversion) {
  if (softwareversion != NULL) {
    set_has_softwareversion();
  } else {
    clear_has_softwareversion();
  }
  softwareversion_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), softwareversion);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeRegistrationProto.softwareVersion)
}

// -------------------------------------------------------------------

// DatanodeCommandProto

// required .hadoop.hdfs.datanode.DatanodeCommandProto.Type cmdType = 1;
inline bool DatanodeCommandProto::has_cmdtype() const {
  return (_has_bits_[0] & 0x00000100u) != 0;
}
inline void DatanodeCommandProto::set_has_cmdtype() {
  _has_bits_[0] |= 0x00000100u;
}
inline void DatanodeCommandProto::clear_has_cmdtype() {
  _has_bits_[0] &= ~0x00000100u;
}
inline void DatanodeCommandProto::clear_cmdtype() {
  cmdtype_ = 0;
  clear_has_cmdtype();
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto_Type DatanodeCommandProto::cmdtype() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.cmdType)
  return static_cast< ::hadoop::hdfs::datanode::DatanodeCommandProto_Type >(cmdtype_);
}
inline void DatanodeCommandProto::set_cmdtype(::hadoop::hdfs::datanode::DatanodeCommandProto_Type value) {
  assert(::hadoop::hdfs::datanode::DatanodeCommandProto_Type_IsValid(value));
  set_has_cmdtype();
  cmdtype_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.DatanodeCommandProto.cmdType)
}

// optional .hadoop.hdfs.datanode.BalancerBandwidthCommandProto balancerCmd = 2;
inline bool DatanodeCommandProto::has_balancercmd() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void DatanodeCommandProto::set_has_balancercmd() {
  _has_bits_[0] |= 0x00000001u;
}
inline void DatanodeCommandProto::clear_has_balancercmd() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void DatanodeCommandProto::clear_balancercmd() {
  if (balancercmd_ != NULL) balancercmd_->Clear();
  clear_has_balancercmd();
}
inline const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto& DatanodeCommandProto::_internal_balancercmd() const {
  return *balancercmd_;
}
inline const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto& DatanodeCommandProto::balancercmd() const {
  const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* p = balancercmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.balancerCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto*>(
      &::hadoop::hdfs::datanode::_BalancerBandwidthCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* DatanodeCommandProto::release_balancercmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.balancerCmd)
  clear_has_balancercmd();
  ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* temp = balancercmd_;
  balancercmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* DatanodeCommandProto::mutable_balancercmd() {
  set_has_balancercmd();
  if (balancercmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BalancerBandwidthCommandProto>(GetArenaNoVirtual());
    balancercmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.balancerCmd)
  return balancercmd_;
}
inline void DatanodeCommandProto::set_allocated_balancercmd(::hadoop::hdfs::datanode::BalancerBandwidthCommandProto* balancercmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete balancercmd_;
  }
  if (balancercmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      balancercmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, balancercmd, submessage_arena);
    }
    set_has_balancercmd();
  } else {
    clear_has_balancercmd();
  }
  balancercmd_ = balancercmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.balancerCmd)
}

// optional .hadoop.hdfs.datanode.BlockCommandProto blkCmd = 3;
inline bool DatanodeCommandProto::has_blkcmd() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void DatanodeCommandProto::set_has_blkcmd() {
  _has_bits_[0] |= 0x00000002u;
}
inline void DatanodeCommandProto::clear_has_blkcmd() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void DatanodeCommandProto::clear_blkcmd() {
  if (blkcmd_ != NULL) blkcmd_->Clear();
  clear_has_blkcmd();
}
inline const ::hadoop::hdfs::datanode::BlockCommandProto& DatanodeCommandProto::_internal_blkcmd() const {
  return *blkcmd_;
}
inline const ::hadoop::hdfs::datanode::BlockCommandProto& DatanodeCommandProto::blkcmd() const {
  const ::hadoop::hdfs::datanode::BlockCommandProto* p = blkcmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.blkCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BlockCommandProto*>(
      &::hadoop::hdfs::datanode::_BlockCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BlockCommandProto* DatanodeCommandProto::release_blkcmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.blkCmd)
  clear_has_blkcmd();
  ::hadoop::hdfs::datanode::BlockCommandProto* temp = blkcmd_;
  blkcmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BlockCommandProto* DatanodeCommandProto::mutable_blkcmd() {
  set_has_blkcmd();
  if (blkcmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BlockCommandProto>(GetArenaNoVirtual());
    blkcmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.blkCmd)
  return blkcmd_;
}
inline void DatanodeCommandProto::set_allocated_blkcmd(::hadoop::hdfs::datanode::BlockCommandProto* blkcmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete blkcmd_;
  }
  if (blkcmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      blkcmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, blkcmd, submessage_arena);
    }
    set_has_blkcmd();
  } else {
    clear_has_blkcmd();
  }
  blkcmd_ = blkcmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.blkCmd)
}

// optional .hadoop.hdfs.datanode.BlockRecoveryCommandProto recoveryCmd = 4;
inline bool DatanodeCommandProto::has_recoverycmd() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void DatanodeCommandProto::set_has_recoverycmd() {
  _has_bits_[0] |= 0x00000004u;
}
inline void DatanodeCommandProto::clear_has_recoverycmd() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void DatanodeCommandProto::clear_recoverycmd() {
  if (recoverycmd_ != NULL) recoverycmd_->Clear();
  clear_has_recoverycmd();
}
inline const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto& DatanodeCommandProto::_internal_recoverycmd() const {
  return *recoverycmd_;
}
inline const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto& DatanodeCommandProto::recoverycmd() const {
  const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* p = recoverycmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.recoveryCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BlockRecoveryCommandProto*>(
      &::hadoop::hdfs::datanode::_BlockRecoveryCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* DatanodeCommandProto::release_recoverycmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.recoveryCmd)
  clear_has_recoverycmd();
  ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* temp = recoverycmd_;
  recoverycmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BlockRecoveryCommandProto* DatanodeCommandProto::mutable_recoverycmd() {
  set_has_recoverycmd();
  if (recoverycmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BlockRecoveryCommandProto>(GetArenaNoVirtual());
    recoverycmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.recoveryCmd)
  return recoverycmd_;
}
inline void DatanodeCommandProto::set_allocated_recoverycmd(::hadoop::hdfs::datanode::BlockRecoveryCommandProto* recoverycmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete recoverycmd_;
  }
  if (recoverycmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      recoverycmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, recoverycmd, submessage_arena);
    }
    set_has_recoverycmd();
  } else {
    clear_has_recoverycmd();
  }
  recoverycmd_ = recoverycmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.recoveryCmd)
}

// optional .hadoop.hdfs.datanode.FinalizeCommandProto finalizeCmd = 5;
inline bool DatanodeCommandProto::has_finalizecmd() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void DatanodeCommandProto::set_has_finalizecmd() {
  _has_bits_[0] |= 0x00000008u;
}
inline void DatanodeCommandProto::clear_has_finalizecmd() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void DatanodeCommandProto::clear_finalizecmd() {
  if (finalizecmd_ != NULL) finalizecmd_->Clear();
  clear_has_finalizecmd();
}
inline const ::hadoop::hdfs::datanode::FinalizeCommandProto& DatanodeCommandProto::_internal_finalizecmd() const {
  return *finalizecmd_;
}
inline const ::hadoop::hdfs::datanode::FinalizeCommandProto& DatanodeCommandProto::finalizecmd() const {
  const ::hadoop::hdfs::datanode::FinalizeCommandProto* p = finalizecmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.finalizeCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::FinalizeCommandProto*>(
      &::hadoop::hdfs::datanode::_FinalizeCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::FinalizeCommandProto* DatanodeCommandProto::release_finalizecmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.finalizeCmd)
  clear_has_finalizecmd();
  ::hadoop::hdfs::datanode::FinalizeCommandProto* temp = finalizecmd_;
  finalizecmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::FinalizeCommandProto* DatanodeCommandProto::mutable_finalizecmd() {
  set_has_finalizecmd();
  if (finalizecmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::FinalizeCommandProto>(GetArenaNoVirtual());
    finalizecmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.finalizeCmd)
  return finalizecmd_;
}
inline void DatanodeCommandProto::set_allocated_finalizecmd(::hadoop::hdfs::datanode::FinalizeCommandProto* finalizecmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete finalizecmd_;
  }
  if (finalizecmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      finalizecmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, finalizecmd, submessage_arena);
    }
    set_has_finalizecmd();
  } else {
    clear_has_finalizecmd();
  }
  finalizecmd_ = finalizecmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.finalizeCmd)
}

// optional .hadoop.hdfs.datanode.KeyUpdateCommandProto keyUpdateCmd = 6;
inline bool DatanodeCommandProto::has_keyupdatecmd() const {
  return (_has_bits_[0] & 0x00000010u) != 0;
}
inline void DatanodeCommandProto::set_has_keyupdatecmd() {
  _has_bits_[0] |= 0x00000010u;
}
inline void DatanodeCommandProto::clear_has_keyupdatecmd() {
  _has_bits_[0] &= ~0x00000010u;
}
inline void DatanodeCommandProto::clear_keyupdatecmd() {
  if (keyupdatecmd_ != NULL) keyupdatecmd_->Clear();
  clear_has_keyupdatecmd();
}
inline const ::hadoop::hdfs::datanode::KeyUpdateCommandProto& DatanodeCommandProto::_internal_keyupdatecmd() const {
  return *keyupdatecmd_;
}
inline const ::hadoop::hdfs::datanode::KeyUpdateCommandProto& DatanodeCommandProto::keyupdatecmd() const {
  const ::hadoop::hdfs::datanode::KeyUpdateCommandProto* p = keyupdatecmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.keyUpdateCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::KeyUpdateCommandProto*>(
      &::hadoop::hdfs::datanode::_KeyUpdateCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::KeyUpdateCommandProto* DatanodeCommandProto::release_keyupdatecmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.keyUpdateCmd)
  clear_has_keyupdatecmd();
  ::hadoop::hdfs::datanode::KeyUpdateCommandProto* temp = keyupdatecmd_;
  keyupdatecmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::KeyUpdateCommandProto* DatanodeCommandProto::mutable_keyupdatecmd() {
  set_has_keyupdatecmd();
  if (keyupdatecmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::KeyUpdateCommandProto>(GetArenaNoVirtual());
    keyupdatecmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.keyUpdateCmd)
  return keyupdatecmd_;
}
inline void DatanodeCommandProto::set_allocated_keyupdatecmd(::hadoop::hdfs::datanode::KeyUpdateCommandProto* keyupdatecmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete keyupdatecmd_;
  }
  if (keyupdatecmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      keyupdatecmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, keyupdatecmd, submessage_arena);
    }
    set_has_keyupdatecmd();
  } else {
    clear_has_keyupdatecmd();
  }
  keyupdatecmd_ = keyupdatecmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.keyUpdateCmd)
}

// optional .hadoop.hdfs.datanode.RegisterCommandProto registerCmd = 7;
inline bool DatanodeCommandProto::has_registercmd() const {
  return (_has_bits_[0] & 0x00000020u) != 0;
}
inline void DatanodeCommandProto::set_has_registercmd() {
  _has_bits_[0] |= 0x00000020u;
}
inline void DatanodeCommandProto::clear_has_registercmd() {
  _has_bits_[0] &= ~0x00000020u;
}
inline void DatanodeCommandProto::clear_registercmd() {
  if (registercmd_ != NULL) registercmd_->Clear();
  clear_has_registercmd();
}
inline const ::hadoop::hdfs::datanode::RegisterCommandProto& DatanodeCommandProto::_internal_registercmd() const {
  return *registercmd_;
}
inline const ::hadoop::hdfs::datanode::RegisterCommandProto& DatanodeCommandProto::registercmd() const {
  const ::hadoop::hdfs::datanode::RegisterCommandProto* p = registercmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.registerCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::RegisterCommandProto*>(
      &::hadoop::hdfs::datanode::_RegisterCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::RegisterCommandProto* DatanodeCommandProto::release_registercmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.registerCmd)
  clear_has_registercmd();
  ::hadoop::hdfs::datanode::RegisterCommandProto* temp = registercmd_;
  registercmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::RegisterCommandProto* DatanodeCommandProto::mutable_registercmd() {
  set_has_registercmd();
  if (registercmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::RegisterCommandProto>(GetArenaNoVirtual());
    registercmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.registerCmd)
  return registercmd_;
}
inline void DatanodeCommandProto::set_allocated_registercmd(::hadoop::hdfs::datanode::RegisterCommandProto* registercmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registercmd_;
  }
  if (registercmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registercmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registercmd, submessage_arena);
    }
    set_has_registercmd();
  } else {
    clear_has_registercmd();
  }
  registercmd_ = registercmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.registerCmd)
}

// optional .hadoop.hdfs.datanode.BlockIdCommandProto blkIdCmd = 8;
inline bool DatanodeCommandProto::has_blkidcmd() const {
  return (_has_bits_[0] & 0x00000040u) != 0;
}
inline void DatanodeCommandProto::set_has_blkidcmd() {
  _has_bits_[0] |= 0x00000040u;
}
inline void DatanodeCommandProto::clear_has_blkidcmd() {
  _has_bits_[0] &= ~0x00000040u;
}
inline void DatanodeCommandProto::clear_blkidcmd() {
  if (blkidcmd_ != NULL) blkidcmd_->Clear();
  clear_has_blkidcmd();
}
inline const ::hadoop::hdfs::datanode::BlockIdCommandProto& DatanodeCommandProto::_internal_blkidcmd() const {
  return *blkidcmd_;
}
inline const ::hadoop::hdfs::datanode::BlockIdCommandProto& DatanodeCommandProto::blkidcmd() const {
  const ::hadoop::hdfs::datanode::BlockIdCommandProto* p = blkidcmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.blkIdCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BlockIdCommandProto*>(
      &::hadoop::hdfs::datanode::_BlockIdCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BlockIdCommandProto* DatanodeCommandProto::release_blkidcmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.blkIdCmd)
  clear_has_blkidcmd();
  ::hadoop::hdfs::datanode::BlockIdCommandProto* temp = blkidcmd_;
  blkidcmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BlockIdCommandProto* DatanodeCommandProto::mutable_blkidcmd() {
  set_has_blkidcmd();
  if (blkidcmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BlockIdCommandProto>(GetArenaNoVirtual());
    blkidcmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.blkIdCmd)
  return blkidcmd_;
}
inline void DatanodeCommandProto::set_allocated_blkidcmd(::hadoop::hdfs::datanode::BlockIdCommandProto* blkidcmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete blkidcmd_;
  }
  if (blkidcmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      blkidcmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, blkidcmd, submessage_arena);
    }
    set_has_blkidcmd();
  } else {
    clear_has_blkidcmd();
  }
  blkidcmd_ = blkidcmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.blkIdCmd)
}

// optional .hadoop.hdfs.datanode.BlockECReconstructionCommandProto blkECReconstructionCmd = 9;
inline bool DatanodeCommandProto::has_blkecreconstructioncmd() const {
  return (_has_bits_[0] & 0x00000080u) != 0;
}
inline void DatanodeCommandProto::set_has_blkecreconstructioncmd() {
  _has_bits_[0] |= 0x00000080u;
}
inline void DatanodeCommandProto::clear_has_blkecreconstructioncmd() {
  _has_bits_[0] &= ~0x00000080u;
}
inline void DatanodeCommandProto::clear_blkecreconstructioncmd() {
  if (blkecreconstructioncmd_ != NULL) blkecreconstructioncmd_->Clear();
  clear_has_blkecreconstructioncmd();
}
inline const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto& DatanodeCommandProto::_internal_blkecreconstructioncmd() const {
  return *blkecreconstructioncmd_;
}
inline const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto& DatanodeCommandProto::blkecreconstructioncmd() const {
  const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* p = blkecreconstructioncmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.DatanodeCommandProto.blkECReconstructionCmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto*>(
      &::hadoop::hdfs::datanode::_BlockECReconstructionCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* DatanodeCommandProto::release_blkecreconstructioncmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.DatanodeCommandProto.blkECReconstructionCmd)
  clear_has_blkecreconstructioncmd();
  ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* temp = blkecreconstructioncmd_;
  blkecreconstructioncmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* DatanodeCommandProto::mutable_blkecreconstructioncmd() {
  set_has_blkecreconstructioncmd();
  if (blkecreconstructioncmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BlockECReconstructionCommandProto>(GetArenaNoVirtual());
    blkecreconstructioncmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.DatanodeCommandProto.blkECReconstructionCmd)
  return blkecreconstructioncmd_;
}
inline void DatanodeCommandProto::set_allocated_blkecreconstructioncmd(::hadoop::hdfs::datanode::BlockECReconstructionCommandProto* blkecreconstructioncmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete blkecreconstructioncmd_;
  }
  if (blkecreconstructioncmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      blkecreconstructioncmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, blkecreconstructioncmd, submessage_arena);
    }
    set_has_blkecreconstructioncmd();
  } else {
    clear_has_blkecreconstructioncmd();
  }
  blkecreconstructioncmd_ = blkecreconstructioncmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.DatanodeCommandProto.blkECReconstructionCmd)
}

// -------------------------------------------------------------------

// BalancerBandwidthCommandProto

// required uint64 bandwidth = 1;
inline bool BalancerBandwidthCommandProto::has_bandwidth() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BalancerBandwidthCommandProto::set_has_bandwidth() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BalancerBandwidthCommandProto::clear_has_bandwidth() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BalancerBandwidthCommandProto::clear_bandwidth() {
  bandwidth_ = GOOGLE_ULONGLONG(0);
  clear_has_bandwidth();
}
inline ::google::protobuf::uint64 BalancerBandwidthCommandProto::bandwidth() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BalancerBandwidthCommandProto.bandwidth)
  return bandwidth_;
}
inline void BalancerBandwidthCommandProto::set_bandwidth(::google::protobuf::uint64 value) {
  set_has_bandwidth();
  bandwidth_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BalancerBandwidthCommandProto.bandwidth)
}

// -------------------------------------------------------------------

// BlockCommandProto

// required .hadoop.hdfs.datanode.BlockCommandProto.Action action = 1;
inline bool BlockCommandProto::has_action() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void BlockCommandProto::set_has_action() {
  _has_bits_[0] |= 0x00000002u;
}
inline void BlockCommandProto::clear_has_action() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void BlockCommandProto::clear_action() {
  action_ = 1;
  clear_has_action();
}
inline ::hadoop::hdfs::datanode::BlockCommandProto_Action BlockCommandProto::action() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.action)
  return static_cast< ::hadoop::hdfs::datanode::BlockCommandProto_Action >(action_);
}
inline void BlockCommandProto::set_action(::hadoop::hdfs::datanode::BlockCommandProto_Action value) {
  assert(::hadoop::hdfs::datanode::BlockCommandProto_Action_IsValid(value));
  set_has_action();
  action_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockCommandProto.action)
}

// required string blockPoolId = 2;
inline bool BlockCommandProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockCommandProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockCommandProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockCommandProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& BlockCommandProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void BlockCommandProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
}
#if LANG_CXX11
inline void BlockCommandProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
}
#endif
inline void BlockCommandProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
}
inline void BlockCommandProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
}
inline ::std::string* BlockCommandProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* BlockCommandProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void BlockCommandProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockCommandProto.blockPoolId)
}

// repeated .hadoop.hdfs.BlockProto blocks = 3;
inline int BlockCommandProto::blocks_size() const {
  return blocks_.size();
}
inline ::hadoop::hdfs::BlockProto* BlockCommandProto::mutable_blocks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockCommandProto.blocks)
  return blocks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockProto >*
BlockCommandProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockCommandProto.blocks)
  return &blocks_;
}
inline const ::hadoop::hdfs::BlockProto& BlockCommandProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.blocks)
  return blocks_.Get(index);
}
inline ::hadoop::hdfs::BlockProto* BlockCommandProto::add_blocks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockCommandProto.blocks)
  return blocks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockProto >&
BlockCommandProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockCommandProto.blocks)
  return blocks_;
}

// repeated .hadoop.hdfs.DatanodeInfosProto targets = 4;
inline int BlockCommandProto::targets_size() const {
  return targets_.size();
}
inline ::hadoop::hdfs::DatanodeInfosProto* BlockCommandProto::mutable_targets(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockCommandProto.targets)
  return targets_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeInfosProto >*
BlockCommandProto::mutable_targets() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockCommandProto.targets)
  return &targets_;
}
inline const ::hadoop::hdfs::DatanodeInfosProto& BlockCommandProto::targets(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.targets)
  return targets_.Get(index);
}
inline ::hadoop::hdfs::DatanodeInfosProto* BlockCommandProto::add_targets() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockCommandProto.targets)
  return targets_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeInfosProto >&
BlockCommandProto::targets() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockCommandProto.targets)
  return targets_;
}

// repeated .hadoop.hdfs.StorageUuidsProto targetStorageUuids = 5;
inline int BlockCommandProto::targetstorageuuids_size() const {
  return targetstorageuuids_.size();
}
inline ::hadoop::hdfs::StorageUuidsProto* BlockCommandProto::mutable_targetstorageuuids(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockCommandProto.targetStorageUuids)
  return targetstorageuuids_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageUuidsProto >*
BlockCommandProto::mutable_targetstorageuuids() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockCommandProto.targetStorageUuids)
  return &targetstorageuuids_;
}
inline const ::hadoop::hdfs::StorageUuidsProto& BlockCommandProto::targetstorageuuids(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.targetStorageUuids)
  return targetstorageuuids_.Get(index);
}
inline ::hadoop::hdfs::StorageUuidsProto* BlockCommandProto::add_targetstorageuuids() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockCommandProto.targetStorageUuids)
  return targetstorageuuids_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageUuidsProto >&
BlockCommandProto::targetstorageuuids() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockCommandProto.targetStorageUuids)
  return targetstorageuuids_;
}

// repeated .hadoop.hdfs.StorageTypesProto targetStorageTypes = 6;
inline int BlockCommandProto::targetstoragetypes_size() const {
  return targetstoragetypes_.size();
}
inline ::hadoop::hdfs::StorageTypesProto* BlockCommandProto::mutable_targetstoragetypes(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockCommandProto.targetStorageTypes)
  return targetstoragetypes_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageTypesProto >*
BlockCommandProto::mutable_targetstoragetypes() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockCommandProto.targetStorageTypes)
  return &targetstoragetypes_;
}
inline const ::hadoop::hdfs::StorageTypesProto& BlockCommandProto::targetstoragetypes(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockCommandProto.targetStorageTypes)
  return targetstoragetypes_.Get(index);
}
inline ::hadoop::hdfs::StorageTypesProto* BlockCommandProto::add_targetstoragetypes() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockCommandProto.targetStorageTypes)
  return targetstoragetypes_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageTypesProto >&
BlockCommandProto::targetstoragetypes() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockCommandProto.targetStorageTypes)
  return targetstoragetypes_;
}

// -------------------------------------------------------------------

// BlockIdCommandProto

// required .hadoop.hdfs.datanode.BlockIdCommandProto.Action action = 1;
inline bool BlockIdCommandProto::has_action() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void BlockIdCommandProto::set_has_action() {
  _has_bits_[0] |= 0x00000002u;
}
inline void BlockIdCommandProto::clear_has_action() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void BlockIdCommandProto::clear_action() {
  action_ = 1;
  clear_has_action();
}
inline ::hadoop::hdfs::datanode::BlockIdCommandProto_Action BlockIdCommandProto::action() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockIdCommandProto.action)
  return static_cast< ::hadoop::hdfs::datanode::BlockIdCommandProto_Action >(action_);
}
inline void BlockIdCommandProto::set_action(::hadoop::hdfs::datanode::BlockIdCommandProto_Action value) {
  assert(::hadoop::hdfs::datanode::BlockIdCommandProto_Action_IsValid(value));
  set_has_action();
  action_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockIdCommandProto.action)
}

// required string blockPoolId = 2;
inline bool BlockIdCommandProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockIdCommandProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockIdCommandProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockIdCommandProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& BlockIdCommandProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void BlockIdCommandProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
}
#if LANG_CXX11
inline void BlockIdCommandProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
}
#endif
inline void BlockIdCommandProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
}
inline void BlockIdCommandProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
}
inline ::std::string* BlockIdCommandProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* BlockIdCommandProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void BlockIdCommandProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockIdCommandProto.blockPoolId)
}

// repeated uint64 blockIds = 3 [packed = true];
inline int BlockIdCommandProto::blockids_size() const {
  return blockids_.size();
}
inline void BlockIdCommandProto::clear_blockids() {
  blockids_.Clear();
}
inline ::google::protobuf::uint64 BlockIdCommandProto::blockids(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockIdCommandProto.blockIds)
  return blockids_.Get(index);
}
inline void BlockIdCommandProto::set_blockids(int index, ::google::protobuf::uint64 value) {
  blockids_.Set(index, value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockIdCommandProto.blockIds)
}
inline void BlockIdCommandProto::add_blockids(::google::protobuf::uint64 value) {
  blockids_.Add(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockIdCommandProto.blockIds)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
BlockIdCommandProto::blockids() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockIdCommandProto.blockIds)
  return blockids_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
BlockIdCommandProto::mutable_blockids() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockIdCommandProto.blockIds)
  return &blockids_;
}

// -------------------------------------------------------------------

// BlockRecoveryCommandProto

// repeated .hadoop.hdfs.RecoveringBlockProto blocks = 1;
inline int BlockRecoveryCommandProto::blocks_size() const {
  return blocks_.size();
}
inline ::hadoop::hdfs::RecoveringBlockProto* BlockRecoveryCommandProto::mutable_blocks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockRecoveryCommandProto.blocks)
  return blocks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::RecoveringBlockProto >*
BlockRecoveryCommandProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockRecoveryCommandProto.blocks)
  return &blocks_;
}
inline const ::hadoop::hdfs::RecoveringBlockProto& BlockRecoveryCommandProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockRecoveryCommandProto.blocks)
  return blocks_.Get(index);
}
inline ::hadoop::hdfs::RecoveringBlockProto* BlockRecoveryCommandProto::add_blocks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockRecoveryCommandProto.blocks)
  return blocks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::RecoveringBlockProto >&
BlockRecoveryCommandProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockRecoveryCommandProto.blocks)
  return blocks_;
}

// -------------------------------------------------------------------

// FinalizeCommandProto

// required string blockPoolId = 1;
inline bool FinalizeCommandProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void FinalizeCommandProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void FinalizeCommandProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void FinalizeCommandProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& FinalizeCommandProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void FinalizeCommandProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
}
#if LANG_CXX11
inline void FinalizeCommandProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
}
#endif
inline void FinalizeCommandProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
}
inline void FinalizeCommandProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
}
inline ::std::string* FinalizeCommandProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* FinalizeCommandProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void FinalizeCommandProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.FinalizeCommandProto.blockPoolId)
}

// -------------------------------------------------------------------

// KeyUpdateCommandProto

// required .hadoop.hdfs.ExportedBlockKeysProto keys = 1;
inline bool KeyUpdateCommandProto::has_keys() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void KeyUpdateCommandProto::set_has_keys() {
  _has_bits_[0] |= 0x00000001u;
}
inline void KeyUpdateCommandProto::clear_has_keys() {
  _has_bits_[0] &= ~0x00000001u;
}
inline const ::hadoop::hdfs::ExportedBlockKeysProto& KeyUpdateCommandProto::_internal_keys() const {
  return *keys_;
}
inline const ::hadoop::hdfs::ExportedBlockKeysProto& KeyUpdateCommandProto::keys() const {
  const ::hadoop::hdfs::ExportedBlockKeysProto* p = keys_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.KeyUpdateCommandProto.keys)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::ExportedBlockKeysProto*>(
      &::hadoop::hdfs::_ExportedBlockKeysProto_default_instance_);
}
inline ::hadoop::hdfs::ExportedBlockKeysProto* KeyUpdateCommandProto::release_keys() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.KeyUpdateCommandProto.keys)
  clear_has_keys();
  ::hadoop::hdfs::ExportedBlockKeysProto* temp = keys_;
  keys_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::ExportedBlockKeysProto* KeyUpdateCommandProto::mutable_keys() {
  set_has_keys();
  if (keys_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::ExportedBlockKeysProto>(GetArenaNoVirtual());
    keys_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.KeyUpdateCommandProto.keys)
  return keys_;
}
inline void KeyUpdateCommandProto::set_allocated_keys(::hadoop::hdfs::ExportedBlockKeysProto* keys) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(keys_);
  }
  if (keys) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      keys = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, keys, submessage_arena);
    }
    set_has_keys();
  } else {
    clear_has_keys();
  }
  keys_ = keys;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.KeyUpdateCommandProto.keys)
}

// -------------------------------------------------------------------

// RegisterCommandProto

// -------------------------------------------------------------------

// BlockECReconstructionCommandProto

// repeated .hadoop.hdfs.BlockECReconstructionInfoProto blockECReconstructioninfo = 1;
inline int BlockECReconstructionCommandProto::blockecreconstructioninfo_size() const {
  return blockecreconstructioninfo_.size();
}
inline ::hadoop::hdfs::BlockECReconstructionInfoProto* BlockECReconstructionCommandProto::mutable_blockecreconstructioninfo(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockECReconstructionCommandProto.blockECReconstructioninfo)
  return blockecreconstructioninfo_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockECReconstructionInfoProto >*
BlockECReconstructionCommandProto::mutable_blockecreconstructioninfo() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockECReconstructionCommandProto.blockECReconstructioninfo)
  return &blockecreconstructioninfo_;
}
inline const ::hadoop::hdfs::BlockECReconstructionInfoProto& BlockECReconstructionCommandProto::blockecreconstructioninfo(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockECReconstructionCommandProto.blockECReconstructioninfo)
  return blockecreconstructioninfo_.Get(index);
}
inline ::hadoop::hdfs::BlockECReconstructionInfoProto* BlockECReconstructionCommandProto::add_blockecreconstructioninfo() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockECReconstructionCommandProto.blockECReconstructioninfo)
  return blockecreconstructioninfo_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::BlockECReconstructionInfoProto >&
BlockECReconstructionCommandProto::blockecreconstructioninfo() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockECReconstructionCommandProto.blockECReconstructioninfo)
  return blockecreconstructioninfo_;
}

// -------------------------------------------------------------------

// RegisterDatanodeRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool RegisterDatanodeRequestProto::has_registration() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void RegisterDatanodeRequestProto::set_has_registration() {
  _has_bits_[0] |= 0x00000001u;
}
inline void RegisterDatanodeRequestProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void RegisterDatanodeRequestProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& RegisterDatanodeRequestProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& RegisterDatanodeRequestProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.RegisterDatanodeRequestProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* RegisterDatanodeRequestProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.RegisterDatanodeRequestProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* RegisterDatanodeRequestProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.RegisterDatanodeRequestProto.registration)
  return registration_;
}
inline void RegisterDatanodeRequestProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.RegisterDatanodeRequestProto.registration)
}

// -------------------------------------------------------------------

// RegisterDatanodeResponseProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool RegisterDatanodeResponseProto::has_registration() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void RegisterDatanodeResponseProto::set_has_registration() {
  _has_bits_[0] |= 0x00000001u;
}
inline void RegisterDatanodeResponseProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void RegisterDatanodeResponseProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& RegisterDatanodeResponseProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& RegisterDatanodeResponseProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.RegisterDatanodeResponseProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* RegisterDatanodeResponseProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.RegisterDatanodeResponseProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* RegisterDatanodeResponseProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.RegisterDatanodeResponseProto.registration)
  return registration_;
}
inline void RegisterDatanodeResponseProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.RegisterDatanodeResponseProto.registration)
}

// -------------------------------------------------------------------

// VolumeFailureSummaryProto

// repeated string failedStorageLocations = 1;
inline int VolumeFailureSummaryProto::failedstoragelocations_size() const {
  return failedstoragelocations_.size();
}
inline void VolumeFailureSummaryProto::clear_failedstoragelocations() {
  failedstoragelocations_.Clear();
}
inline const ::std::string& VolumeFailureSummaryProto::failedstoragelocations(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  return failedstoragelocations_.Get(index);
}
inline ::std::string* VolumeFailureSummaryProto::mutable_failedstoragelocations(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  return failedstoragelocations_.Mutable(index);
}
inline void VolumeFailureSummaryProto::set_failedstoragelocations(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  failedstoragelocations_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void VolumeFailureSummaryProto::set_failedstoragelocations(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  failedstoragelocations_.Mutable(index)->assign(std::move(value));
}
#endif
inline void VolumeFailureSummaryProto::set_failedstoragelocations(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  failedstoragelocations_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
inline void VolumeFailureSummaryProto::set_failedstoragelocations(int index, const char* value, size_t size) {
  failedstoragelocations_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
inline ::std::string* VolumeFailureSummaryProto::add_failedstoragelocations() {
  // @@protoc_insertion_point(field_add_mutable:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  return failedstoragelocations_.Add();
}
inline void VolumeFailureSummaryProto::add_failedstoragelocations(const ::std::string& value) {
  failedstoragelocations_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
#if LANG_CXX11
inline void VolumeFailureSummaryProto::add_failedstoragelocations(::std::string&& value) {
  failedstoragelocations_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
#endif
inline void VolumeFailureSummaryProto::add_failedstoragelocations(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  failedstoragelocations_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
inline void VolumeFailureSummaryProto::add_failedstoragelocations(const char* value, size_t size) {
  failedstoragelocations_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
VolumeFailureSummaryProto::failedstoragelocations() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  return failedstoragelocations_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
VolumeFailureSummaryProto::mutable_failedstoragelocations() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.VolumeFailureSummaryProto.failedStorageLocations)
  return &failedstoragelocations_;
}

// required uint64 lastVolumeFailureDate = 2;
inline bool VolumeFailureSummaryProto::has_lastvolumefailuredate() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void VolumeFailureSummaryProto::set_has_lastvolumefailuredate() {
  _has_bits_[0] |= 0x00000001u;
}
inline void VolumeFailureSummaryProto::clear_has_lastvolumefailuredate() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void VolumeFailureSummaryProto::clear_lastvolumefailuredate() {
  lastvolumefailuredate_ = GOOGLE_ULONGLONG(0);
  clear_has_lastvolumefailuredate();
}
inline ::google::protobuf::uint64 VolumeFailureSummaryProto::lastvolumefailuredate() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.VolumeFailureSummaryProto.lastVolumeFailureDate)
  return lastvolumefailuredate_;
}
inline void VolumeFailureSummaryProto::set_lastvolumefailuredate(::google::protobuf::uint64 value) {
  set_has_lastvolumefailuredate();
  lastvolumefailuredate_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.VolumeFailureSummaryProto.lastVolumeFailureDate)
}

// required uint64 estimatedCapacityLostTotal = 3;
inline bool VolumeFailureSummaryProto::has_estimatedcapacitylosttotal() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void VolumeFailureSummaryProto::set_has_estimatedcapacitylosttotal() {
  _has_bits_[0] |= 0x00000002u;
}
inline void VolumeFailureSummaryProto::clear_has_estimatedcapacitylosttotal() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void VolumeFailureSummaryProto::clear_estimatedcapacitylosttotal() {
  estimatedcapacitylosttotal_ = GOOGLE_ULONGLONG(0);
  clear_has_estimatedcapacitylosttotal();
}
inline ::google::protobuf::uint64 VolumeFailureSummaryProto::estimatedcapacitylosttotal() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.VolumeFailureSummaryProto.estimatedCapacityLostTotal)
  return estimatedcapacitylosttotal_;
}
inline void VolumeFailureSummaryProto::set_estimatedcapacitylosttotal(::google::protobuf::uint64 value) {
  set_has_estimatedcapacitylosttotal();
  estimatedcapacitylosttotal_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.VolumeFailureSummaryProto.estimatedCapacityLostTotal)
}

// -------------------------------------------------------------------

// HeartbeatRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool HeartbeatRequestProto::has_registration() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void HeartbeatRequestProto::set_has_registration() {
  _has_bits_[0] |= 0x00000001u;
}
inline void HeartbeatRequestProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void HeartbeatRequestProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& HeartbeatRequestProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& HeartbeatRequestProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* HeartbeatRequestProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.HeartbeatRequestProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* HeartbeatRequestProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatRequestProto.registration)
  return registration_;
}
inline void HeartbeatRequestProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.HeartbeatRequestProto.registration)
}

// repeated .hadoop.hdfs.StorageReportProto reports = 2;
inline int HeartbeatRequestProto::reports_size() const {
  return reports_.size();
}
inline ::hadoop::hdfs::StorageReportProto* HeartbeatRequestProto::mutable_reports(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatRequestProto.reports)
  return reports_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageReportProto >*
HeartbeatRequestProto::mutable_reports() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.HeartbeatRequestProto.reports)
  return &reports_;
}
inline const ::hadoop::hdfs::StorageReportProto& HeartbeatRequestProto::reports(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.reports)
  return reports_.Get(index);
}
inline ::hadoop::hdfs::StorageReportProto* HeartbeatRequestProto::add_reports() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.HeartbeatRequestProto.reports)
  return reports_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::StorageReportProto >&
HeartbeatRequestProto::reports() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.HeartbeatRequestProto.reports)
  return reports_;
}

// optional uint32 xmitsInProgress = 3 [default = 0];
inline bool HeartbeatRequestProto::has_xmitsinprogress() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void HeartbeatRequestProto::set_has_xmitsinprogress() {
  _has_bits_[0] |= 0x00000004u;
}
inline void HeartbeatRequestProto::clear_has_xmitsinprogress() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void HeartbeatRequestProto::clear_xmitsinprogress() {
  xmitsinprogress_ = 0u;
  clear_has_xmitsinprogress();
}
inline ::google::protobuf::uint32 HeartbeatRequestProto::xmitsinprogress() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.xmitsInProgress)
  return xmitsinprogress_;
}
inline void HeartbeatRequestProto::set_xmitsinprogress(::google::protobuf::uint32 value) {
  set_has_xmitsinprogress();
  xmitsinprogress_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.xmitsInProgress)
}

// optional uint32 xceiverCount = 4 [default = 0];
inline bool HeartbeatRequestProto::has_xceivercount() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void HeartbeatRequestProto::set_has_xceivercount() {
  _has_bits_[0] |= 0x00000008u;
}
inline void HeartbeatRequestProto::clear_has_xceivercount() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void HeartbeatRequestProto::clear_xceivercount() {
  xceivercount_ = 0u;
  clear_has_xceivercount();
}
inline ::google::protobuf::uint32 HeartbeatRequestProto::xceivercount() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.xceiverCount)
  return xceivercount_;
}
inline void HeartbeatRequestProto::set_xceivercount(::google::protobuf::uint32 value) {
  set_has_xceivercount();
  xceivercount_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.xceiverCount)
}

// optional uint32 failedVolumes = 5 [default = 0];
inline bool HeartbeatRequestProto::has_failedvolumes() const {
  return (_has_bits_[0] & 0x00000040u) != 0;
}
inline void HeartbeatRequestProto::set_has_failedvolumes() {
  _has_bits_[0] |= 0x00000040u;
}
inline void HeartbeatRequestProto::clear_has_failedvolumes() {
  _has_bits_[0] &= ~0x00000040u;
}
inline void HeartbeatRequestProto::clear_failedvolumes() {
  failedvolumes_ = 0u;
  clear_has_failedvolumes();
}
inline ::google::protobuf::uint32 HeartbeatRequestProto::failedvolumes() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.failedVolumes)
  return failedvolumes_;
}
inline void HeartbeatRequestProto::set_failedvolumes(::google::protobuf::uint32 value) {
  set_has_failedvolumes();
  failedvolumes_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.failedVolumes)
}

// optional uint64 cacheCapacity = 6 [default = 0];
inline bool HeartbeatRequestProto::has_cachecapacity() const {
  return (_has_bits_[0] & 0x00000010u) != 0;
}
inline void HeartbeatRequestProto::set_has_cachecapacity() {
  _has_bits_[0] |= 0x00000010u;
}
inline void HeartbeatRequestProto::clear_has_cachecapacity() {
  _has_bits_[0] &= ~0x00000010u;
}
inline void HeartbeatRequestProto::clear_cachecapacity() {
  cachecapacity_ = GOOGLE_ULONGLONG(0);
  clear_has_cachecapacity();
}
inline ::google::protobuf::uint64 HeartbeatRequestProto::cachecapacity() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.cacheCapacity)
  return cachecapacity_;
}
inline void HeartbeatRequestProto::set_cachecapacity(::google::protobuf::uint64 value) {
  set_has_cachecapacity();
  cachecapacity_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.cacheCapacity)
}

// optional uint64 cacheUsed = 7 [default = 0];
inline bool HeartbeatRequestProto::has_cacheused() const {
  return (_has_bits_[0] & 0x00000020u) != 0;
}
inline void HeartbeatRequestProto::set_has_cacheused() {
  _has_bits_[0] |= 0x00000020u;
}
inline void HeartbeatRequestProto::clear_has_cacheused() {
  _has_bits_[0] &= ~0x00000020u;
}
inline void HeartbeatRequestProto::clear_cacheused() {
  cacheused_ = GOOGLE_ULONGLONG(0);
  clear_has_cacheused();
}
inline ::google::protobuf::uint64 HeartbeatRequestProto::cacheused() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.cacheUsed)
  return cacheused_;
}
inline void HeartbeatRequestProto::set_cacheused(::google::protobuf::uint64 value) {
  set_has_cacheused();
  cacheused_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.cacheUsed)
}

// optional .hadoop.hdfs.datanode.VolumeFailureSummaryProto volumeFailureSummary = 8;
inline bool HeartbeatRequestProto::has_volumefailuresummary() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void HeartbeatRequestProto::set_has_volumefailuresummary() {
  _has_bits_[0] |= 0x00000002u;
}
inline void HeartbeatRequestProto::clear_has_volumefailuresummary() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void HeartbeatRequestProto::clear_volumefailuresummary() {
  if (volumefailuresummary_ != NULL) volumefailuresummary_->Clear();
  clear_has_volumefailuresummary();
}
inline const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto& HeartbeatRequestProto::_internal_volumefailuresummary() const {
  return *volumefailuresummary_;
}
inline const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto& HeartbeatRequestProto::volumefailuresummary() const {
  const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* p = volumefailuresummary_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.volumeFailureSummary)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::VolumeFailureSummaryProto*>(
      &::hadoop::hdfs::datanode::_VolumeFailureSummaryProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* HeartbeatRequestProto::release_volumefailuresummary() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.HeartbeatRequestProto.volumeFailureSummary)
  clear_has_volumefailuresummary();
  ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* temp = volumefailuresummary_;
  volumefailuresummary_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::VolumeFailureSummaryProto* HeartbeatRequestProto::mutable_volumefailuresummary() {
  set_has_volumefailuresummary();
  if (volumefailuresummary_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::VolumeFailureSummaryProto>(GetArenaNoVirtual());
    volumefailuresummary_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatRequestProto.volumeFailureSummary)
  return volumefailuresummary_;
}
inline void HeartbeatRequestProto::set_allocated_volumefailuresummary(::hadoop::hdfs::datanode::VolumeFailureSummaryProto* volumefailuresummary) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete volumefailuresummary_;
  }
  if (volumefailuresummary) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      volumefailuresummary = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, volumefailuresummary, submessage_arena);
    }
    set_has_volumefailuresummary();
  } else {
    clear_has_volumefailuresummary();
  }
  volumefailuresummary_ = volumefailuresummary;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.HeartbeatRequestProto.volumeFailureSummary)
}

// optional bool requestFullBlockReportLease = 9 [default = false];
inline bool HeartbeatRequestProto::has_requestfullblockreportlease() const {
  return (_has_bits_[0] & 0x00000080u) != 0;
}
inline void HeartbeatRequestProto::set_has_requestfullblockreportlease() {
  _has_bits_[0] |= 0x00000080u;
}
inline void HeartbeatRequestProto::clear_has_requestfullblockreportlease() {
  _has_bits_[0] &= ~0x00000080u;
}
inline void HeartbeatRequestProto::clear_requestfullblockreportlease() {
  requestfullblockreportlease_ = false;
  clear_has_requestfullblockreportlease();
}
inline bool HeartbeatRequestProto::requestfullblockreportlease() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.requestFullBlockReportLease)
  return requestfullblockreportlease_;
}
inline void HeartbeatRequestProto::set_requestfullblockreportlease(bool value) {
  set_has_requestfullblockreportlease();
  requestfullblockreportlease_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatRequestProto.requestFullBlockReportLease)
}

// repeated .hadoop.hdfs.datanode.SlowPeerReportProto slowPeers = 10;
inline int HeartbeatRequestProto::slowpeers_size() const {
  return slowpeers_.size();
}
inline void HeartbeatRequestProto::clear_slowpeers() {
  slowpeers_.Clear();
}
inline ::hadoop::hdfs::datanode::SlowPeerReportProto* HeartbeatRequestProto::mutable_slowpeers(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatRequestProto.slowPeers)
  return slowpeers_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowPeerReportProto >*
HeartbeatRequestProto::mutable_slowpeers() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.HeartbeatRequestProto.slowPeers)
  return &slowpeers_;
}
inline const ::hadoop::hdfs::datanode::SlowPeerReportProto& HeartbeatRequestProto::slowpeers(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.slowPeers)
  return slowpeers_.Get(index);
}
inline ::hadoop::hdfs::datanode::SlowPeerReportProto* HeartbeatRequestProto::add_slowpeers() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.HeartbeatRequestProto.slowPeers)
  return slowpeers_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowPeerReportProto >&
HeartbeatRequestProto::slowpeers() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.HeartbeatRequestProto.slowPeers)
  return slowpeers_;
}

// repeated .hadoop.hdfs.datanode.SlowDiskReportProto slowDisks = 11;
inline int HeartbeatRequestProto::slowdisks_size() const {
  return slowdisks_.size();
}
inline void HeartbeatRequestProto::clear_slowdisks() {
  slowdisks_.Clear();
}
inline ::hadoop::hdfs::datanode::SlowDiskReportProto* HeartbeatRequestProto::mutable_slowdisks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatRequestProto.slowDisks)
  return slowdisks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowDiskReportProto >*
HeartbeatRequestProto::mutable_slowdisks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.HeartbeatRequestProto.slowDisks)
  return &slowdisks_;
}
inline const ::hadoop::hdfs::datanode::SlowDiskReportProto& HeartbeatRequestProto::slowdisks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatRequestProto.slowDisks)
  return slowdisks_.Get(index);
}
inline ::hadoop::hdfs::datanode::SlowDiskReportProto* HeartbeatRequestProto::add_slowdisks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.HeartbeatRequestProto.slowDisks)
  return slowdisks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::SlowDiskReportProto >&
HeartbeatRequestProto::slowdisks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.HeartbeatRequestProto.slowDisks)
  return slowdisks_;
}

// -------------------------------------------------------------------

// HeartbeatResponseProto

// repeated .hadoop.hdfs.datanode.DatanodeCommandProto cmds = 1;
inline int HeartbeatResponseProto::cmds_size() const {
  return cmds_.size();
}
inline void HeartbeatResponseProto::clear_cmds() {
  cmds_.Clear();
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* HeartbeatResponseProto::mutable_cmds(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatResponseProto.cmds)
  return cmds_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::DatanodeCommandProto >*
HeartbeatResponseProto::mutable_cmds() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.HeartbeatResponseProto.cmds)
  return &cmds_;
}
inline const ::hadoop::hdfs::datanode::DatanodeCommandProto& HeartbeatResponseProto::cmds(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatResponseProto.cmds)
  return cmds_.Get(index);
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* HeartbeatResponseProto::add_cmds() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.HeartbeatResponseProto.cmds)
  return cmds_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::DatanodeCommandProto >&
HeartbeatResponseProto::cmds() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.HeartbeatResponseProto.cmds)
  return cmds_;
}

// required .hadoop.hdfs.NNHAStatusHeartbeatProto haStatus = 2;
inline bool HeartbeatResponseProto::has_hastatus() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void HeartbeatResponseProto::set_has_hastatus() {
  _has_bits_[0] |= 0x00000001u;
}
inline void HeartbeatResponseProto::clear_has_hastatus() {
  _has_bits_[0] &= ~0x00000001u;
}
inline const ::hadoop::hdfs::NNHAStatusHeartbeatProto& HeartbeatResponseProto::_internal_hastatus() const {
  return *hastatus_;
}
inline const ::hadoop::hdfs::NNHAStatusHeartbeatProto& HeartbeatResponseProto::hastatus() const {
  const ::hadoop::hdfs::NNHAStatusHeartbeatProto* p = hastatus_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatResponseProto.haStatus)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::NNHAStatusHeartbeatProto*>(
      &::hadoop::hdfs::_NNHAStatusHeartbeatProto_default_instance_);
}
inline ::hadoop::hdfs::NNHAStatusHeartbeatProto* HeartbeatResponseProto::release_hastatus() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.HeartbeatResponseProto.haStatus)
  clear_has_hastatus();
  ::hadoop::hdfs::NNHAStatusHeartbeatProto* temp = hastatus_;
  hastatus_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::NNHAStatusHeartbeatProto* HeartbeatResponseProto::mutable_hastatus() {
  set_has_hastatus();
  if (hastatus_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::NNHAStatusHeartbeatProto>(GetArenaNoVirtual());
    hastatus_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatResponseProto.haStatus)
  return hastatus_;
}
inline void HeartbeatResponseProto::set_allocated_hastatus(::hadoop::hdfs::NNHAStatusHeartbeatProto* hastatus) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(hastatus_);
  }
  if (hastatus) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      hastatus = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, hastatus, submessage_arena);
    }
    set_has_hastatus();
  } else {
    clear_has_hastatus();
  }
  hastatus_ = hastatus;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.HeartbeatResponseProto.haStatus)
}

// optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatus = 3;
inline bool HeartbeatResponseProto::has_rollingupgradestatus() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void HeartbeatResponseProto::set_has_rollingupgradestatus() {
  _has_bits_[0] |= 0x00000002u;
}
inline void HeartbeatResponseProto::clear_has_rollingupgradestatus() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::hadoop::hdfs::RollingUpgradeStatusProto& HeartbeatResponseProto::_internal_rollingupgradestatus() const {
  return *rollingupgradestatus_;
}
inline const ::hadoop::hdfs::RollingUpgradeStatusProto& HeartbeatResponseProto::rollingupgradestatus() const {
  const ::hadoop::hdfs::RollingUpgradeStatusProto* p = rollingupgradestatus_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatus)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::RollingUpgradeStatusProto*>(
      &::hadoop::hdfs::_RollingUpgradeStatusProto_default_instance_);
}
inline ::hadoop::hdfs::RollingUpgradeStatusProto* HeartbeatResponseProto::release_rollingupgradestatus() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatus)
  clear_has_rollingupgradestatus();
  ::hadoop::hdfs::RollingUpgradeStatusProto* temp = rollingupgradestatus_;
  rollingupgradestatus_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::RollingUpgradeStatusProto* HeartbeatResponseProto::mutable_rollingupgradestatus() {
  set_has_rollingupgradestatus();
  if (rollingupgradestatus_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::RollingUpgradeStatusProto>(GetArenaNoVirtual());
    rollingupgradestatus_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatus)
  return rollingupgradestatus_;
}
inline void HeartbeatResponseProto::set_allocated_rollingupgradestatus(::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatus) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(rollingupgradestatus_);
  }
  if (rollingupgradestatus) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      rollingupgradestatus = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, rollingupgradestatus, submessage_arena);
    }
    set_has_rollingupgradestatus();
  } else {
    clear_has_rollingupgradestatus();
  }
  rollingupgradestatus_ = rollingupgradestatus;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatus)
}

// optional .hadoop.hdfs.RollingUpgradeStatusProto rollingUpgradeStatusV2 = 4;
inline bool HeartbeatResponseProto::has_rollingupgradestatusv2() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void HeartbeatResponseProto::set_has_rollingupgradestatusv2() {
  _has_bits_[0] |= 0x00000004u;
}
inline void HeartbeatResponseProto::clear_has_rollingupgradestatusv2() {
  _has_bits_[0] &= ~0x00000004u;
}
inline const ::hadoop::hdfs::RollingUpgradeStatusProto& HeartbeatResponseProto::_internal_rollingupgradestatusv2() const {
  return *rollingupgradestatusv2_;
}
inline const ::hadoop::hdfs::RollingUpgradeStatusProto& HeartbeatResponseProto::rollingupgradestatusv2() const {
  const ::hadoop::hdfs::RollingUpgradeStatusProto* p = rollingupgradestatusv2_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatusV2)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::RollingUpgradeStatusProto*>(
      &::hadoop::hdfs::_RollingUpgradeStatusProto_default_instance_);
}
inline ::hadoop::hdfs::RollingUpgradeStatusProto* HeartbeatResponseProto::release_rollingupgradestatusv2() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatusV2)
  clear_has_rollingupgradestatusv2();
  ::hadoop::hdfs::RollingUpgradeStatusProto* temp = rollingupgradestatusv2_;
  rollingupgradestatusv2_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::RollingUpgradeStatusProto* HeartbeatResponseProto::mutable_rollingupgradestatusv2() {
  set_has_rollingupgradestatusv2();
  if (rollingupgradestatusv2_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::RollingUpgradeStatusProto>(GetArenaNoVirtual());
    rollingupgradestatusv2_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatusV2)
  return rollingupgradestatusv2_;
}
inline void HeartbeatResponseProto::set_allocated_rollingupgradestatusv2(::hadoop::hdfs::RollingUpgradeStatusProto* rollingupgradestatusv2) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(rollingupgradestatusv2_);
  }
  if (rollingupgradestatusv2) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      rollingupgradestatusv2 = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, rollingupgradestatusv2, submessage_arena);
    }
    set_has_rollingupgradestatusv2();
  } else {
    clear_has_rollingupgradestatusv2();
  }
  rollingupgradestatusv2_ = rollingupgradestatusv2;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.HeartbeatResponseProto.rollingUpgradeStatusV2)
}

// optional uint64 fullBlockReportLeaseId = 5 [default = 0];
inline bool HeartbeatResponseProto::has_fullblockreportleaseid() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void HeartbeatResponseProto::set_has_fullblockreportleaseid() {
  _has_bits_[0] |= 0x00000008u;
}
inline void HeartbeatResponseProto::clear_has_fullblockreportleaseid() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void HeartbeatResponseProto::clear_fullblockreportleaseid() {
  fullblockreportleaseid_ = GOOGLE_ULONGLONG(0);
  clear_has_fullblockreportleaseid();
}
inline ::google::protobuf::uint64 HeartbeatResponseProto::fullblockreportleaseid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.HeartbeatResponseProto.fullBlockReportLeaseId)
  return fullblockreportleaseid_;
}
inline void HeartbeatResponseProto::set_fullblockreportleaseid(::google::protobuf::uint64 value) {
  set_has_fullblockreportleaseid();
  fullblockreportleaseid_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.HeartbeatResponseProto.fullBlockReportLeaseId)
}

// -------------------------------------------------------------------

// BlockReportRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool BlockReportRequestProto::has_registration() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void BlockReportRequestProto::set_has_registration() {
  _has_bits_[0] |= 0x00000002u;
}
inline void BlockReportRequestProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void BlockReportRequestProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& BlockReportRequestProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& BlockReportRequestProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportRequestProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* BlockReportRequestProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReportRequestProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* BlockReportRequestProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReportRequestProto.registration)
  return registration_;
}
inline void BlockReportRequestProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReportRequestProto.registration)
}

// required string blockPoolId = 2;
inline bool BlockReportRequestProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockReportRequestProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockReportRequestProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockReportRequestProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& BlockReportRequestProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void BlockReportRequestProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
}
#if LANG_CXX11
inline void BlockReportRequestProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
}
#endif
inline void BlockReportRequestProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
}
inline void BlockReportRequestProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
}
inline ::std::string* BlockReportRequestProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* BlockReportRequestProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void BlockReportRequestProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReportRequestProto.blockPoolId)
}

// repeated .hadoop.hdfs.datanode.StorageBlockReportProto reports = 3;
inline int BlockReportRequestProto::reports_size() const {
  return reports_.size();
}
inline void BlockReportRequestProto::clear_reports() {
  reports_.Clear();
}
inline ::hadoop::hdfs::datanode::StorageBlockReportProto* BlockReportRequestProto::mutable_reports(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReportRequestProto.reports)
  return reports_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageBlockReportProto >*
BlockReportRequestProto::mutable_reports() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockReportRequestProto.reports)
  return &reports_;
}
inline const ::hadoop::hdfs::datanode::StorageBlockReportProto& BlockReportRequestProto::reports(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportRequestProto.reports)
  return reports_.Get(index);
}
inline ::hadoop::hdfs::datanode::StorageBlockReportProto* BlockReportRequestProto::add_reports() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockReportRequestProto.reports)
  return reports_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageBlockReportProto >&
BlockReportRequestProto::reports() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockReportRequestProto.reports)
  return reports_;
}

// optional .hadoop.hdfs.datanode.BlockReportContextProto context = 4;
inline bool BlockReportRequestProto::has_context() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void BlockReportRequestProto::set_has_context() {
  _has_bits_[0] |= 0x00000004u;
}
inline void BlockReportRequestProto::clear_has_context() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void BlockReportRequestProto::clear_context() {
  if (context_ != NULL) context_->Clear();
  clear_has_context();
}
inline const ::hadoop::hdfs::datanode::BlockReportContextProto& BlockReportRequestProto::_internal_context() const {
  return *context_;
}
inline const ::hadoop::hdfs::datanode::BlockReportContextProto& BlockReportRequestProto::context() const {
  const ::hadoop::hdfs::datanode::BlockReportContextProto* p = context_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportRequestProto.context)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::BlockReportContextProto*>(
      &::hadoop::hdfs::datanode::_BlockReportContextProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::BlockReportContextProto* BlockReportRequestProto::release_context() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReportRequestProto.context)
  clear_has_context();
  ::hadoop::hdfs::datanode::BlockReportContextProto* temp = context_;
  context_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::BlockReportContextProto* BlockReportRequestProto::mutable_context() {
  set_has_context();
  if (context_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::BlockReportContextProto>(GetArenaNoVirtual());
    context_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReportRequestProto.context)
  return context_;
}
inline void BlockReportRequestProto::set_allocated_context(::hadoop::hdfs::datanode::BlockReportContextProto* context) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete context_;
  }
  if (context) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      context = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, context, submessage_arena);
    }
    set_has_context();
  } else {
    clear_has_context();
  }
  context_ = context;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReportRequestProto.context)
}

// -------------------------------------------------------------------

// BlockReportContextProto

// required int32 totalRpcs = 1;
inline bool BlockReportContextProto::has_totalrpcs() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockReportContextProto::set_has_totalrpcs() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockReportContextProto::clear_has_totalrpcs() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockReportContextProto::clear_totalrpcs() {
  totalrpcs_ = 0;
  clear_has_totalrpcs();
}
inline ::google::protobuf::int32 BlockReportContextProto::totalrpcs() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportContextProto.totalRpcs)
  return totalrpcs_;
}
inline void BlockReportContextProto::set_totalrpcs(::google::protobuf::int32 value) {
  set_has_totalrpcs();
  totalrpcs_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReportContextProto.totalRpcs)
}

// required int32 curRpc = 2;
inline bool BlockReportContextProto::has_currpc() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void BlockReportContextProto::set_has_currpc() {
  _has_bits_[0] |= 0x00000002u;
}
inline void BlockReportContextProto::clear_has_currpc() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void BlockReportContextProto::clear_currpc() {
  currpc_ = 0;
  clear_has_currpc();
}
inline ::google::protobuf::int32 BlockReportContextProto::currpc() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportContextProto.curRpc)
  return currpc_;
}
inline void BlockReportContextProto::set_currpc(::google::protobuf::int32 value) {
  set_has_currpc();
  currpc_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReportContextProto.curRpc)
}

// required int64 id = 3;
inline bool BlockReportContextProto::has_id() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void BlockReportContextProto::set_has_id() {
  _has_bits_[0] |= 0x00000004u;
}
inline void BlockReportContextProto::clear_has_id() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void BlockReportContextProto::clear_id() {
  id_ = GOOGLE_LONGLONG(0);
  clear_has_id();
}
inline ::google::protobuf::int64 BlockReportContextProto::id() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportContextProto.id)
  return id_;
}
inline void BlockReportContextProto::set_id(::google::protobuf::int64 value) {
  set_has_id();
  id_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReportContextProto.id)
}

// optional uint64 leaseId = 4 [default = 0];
inline bool BlockReportContextProto::has_leaseid() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void BlockReportContextProto::set_has_leaseid() {
  _has_bits_[0] |= 0x00000008u;
}
inline void BlockReportContextProto::clear_has_leaseid() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void BlockReportContextProto::clear_leaseid() {
  leaseid_ = GOOGLE_ULONGLONG(0);
  clear_has_leaseid();
}
inline ::google::protobuf::uint64 BlockReportContextProto::leaseid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportContextProto.leaseId)
  return leaseid_;
}
inline void BlockReportContextProto::set_leaseid(::google::protobuf::uint64 value) {
  set_has_leaseid();
  leaseid_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReportContextProto.leaseId)
}

// -------------------------------------------------------------------

// StorageBlockReportProto

// required .hadoop.hdfs.DatanodeStorageProto storage = 1;
inline bool StorageBlockReportProto::has_storage() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void StorageBlockReportProto::set_has_storage() {
  _has_bits_[0] |= 0x00000001u;
}
inline void StorageBlockReportProto::clear_has_storage() {
  _has_bits_[0] &= ~0x00000001u;
}
inline const ::hadoop::hdfs::DatanodeStorageProto& StorageBlockReportProto::_internal_storage() const {
  return *storage_;
}
inline const ::hadoop::hdfs::DatanodeStorageProto& StorageBlockReportProto::storage() const {
  const ::hadoop::hdfs::DatanodeStorageProto* p = storage_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageBlockReportProto.storage)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::DatanodeStorageProto*>(
      &::hadoop::hdfs::_DatanodeStorageProto_default_instance_);
}
inline ::hadoop::hdfs::DatanodeStorageProto* StorageBlockReportProto::release_storage() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.StorageBlockReportProto.storage)
  clear_has_storage();
  ::hadoop::hdfs::DatanodeStorageProto* temp = storage_;
  storage_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::DatanodeStorageProto* StorageBlockReportProto::mutable_storage() {
  set_has_storage();
  if (storage_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::DatanodeStorageProto>(GetArenaNoVirtual());
    storage_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.StorageBlockReportProto.storage)
  return storage_;
}
inline void StorageBlockReportProto::set_allocated_storage(::hadoop::hdfs::DatanodeStorageProto* storage) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(storage_);
  }
  if (storage) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      storage = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, storage, submessage_arena);
    }
    set_has_storage();
  } else {
    clear_has_storage();
  }
  storage_ = storage;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.StorageBlockReportProto.storage)
}

// repeated uint64 blocks = 2 [packed = true];
inline int StorageBlockReportProto::blocks_size() const {
  return blocks_.size();
}
inline void StorageBlockReportProto::clear_blocks() {
  blocks_.Clear();
}
inline ::google::protobuf::uint64 StorageBlockReportProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageBlockReportProto.blocks)
  return blocks_.Get(index);
}
inline void StorageBlockReportProto::set_blocks(int index, ::google::protobuf::uint64 value) {
  blocks_.Set(index, value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.StorageBlockReportProto.blocks)
}
inline void StorageBlockReportProto::add_blocks(::google::protobuf::uint64 value) {
  blocks_.Add(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.StorageBlockReportProto.blocks)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
StorageBlockReportProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.StorageBlockReportProto.blocks)
  return blocks_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
StorageBlockReportProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.StorageBlockReportProto.blocks)
  return &blocks_;
}

// optional uint64 numberOfBlocks = 3;
inline bool StorageBlockReportProto::has_numberofblocks() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void StorageBlockReportProto::set_has_numberofblocks() {
  _has_bits_[0] |= 0x00000002u;
}
inline void StorageBlockReportProto::clear_has_numberofblocks() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void StorageBlockReportProto::clear_numberofblocks() {
  numberofblocks_ = GOOGLE_ULONGLONG(0);
  clear_has_numberofblocks();
}
inline ::google::protobuf::uint64 StorageBlockReportProto::numberofblocks() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageBlockReportProto.numberOfBlocks)
  return numberofblocks_;
}
inline void StorageBlockReportProto::set_numberofblocks(::google::protobuf::uint64 value) {
  set_has_numberofblocks();
  numberofblocks_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.StorageBlockReportProto.numberOfBlocks)
}

// repeated bytes blocksBuffers = 4;
inline int StorageBlockReportProto::blocksbuffers_size() const {
  return blocksbuffers_.size();
}
inline void StorageBlockReportProto::clear_blocksbuffers() {
  blocksbuffers_.Clear();
}
inline const ::std::string& StorageBlockReportProto::blocksbuffers(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  return blocksbuffers_.Get(index);
}
inline ::std::string* StorageBlockReportProto::mutable_blocksbuffers(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  return blocksbuffers_.Mutable(index);
}
inline void StorageBlockReportProto::set_blocksbuffers(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  blocksbuffers_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void StorageBlockReportProto::set_blocksbuffers(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  blocksbuffers_.Mutable(index)->assign(std::move(value));
}
#endif
inline void StorageBlockReportProto::set_blocksbuffers(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  blocksbuffers_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
inline void StorageBlockReportProto::set_blocksbuffers(int index, const void* value, size_t size) {
  blocksbuffers_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
inline ::std::string* StorageBlockReportProto::add_blocksbuffers() {
  // @@protoc_insertion_point(field_add_mutable:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  return blocksbuffers_.Add();
}
inline void StorageBlockReportProto::add_blocksbuffers(const ::std::string& value) {
  blocksbuffers_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
#if LANG_CXX11
inline void StorageBlockReportProto::add_blocksbuffers(::std::string&& value) {
  blocksbuffers_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
#endif
inline void StorageBlockReportProto::add_blocksbuffers(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  blocksbuffers_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
inline void StorageBlockReportProto::add_blocksbuffers(const void* value, size_t size) {
  blocksbuffers_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
StorageBlockReportProto::blocksbuffers() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  return blocksbuffers_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
StorageBlockReportProto::mutable_blocksbuffers() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.StorageBlockReportProto.blocksBuffers)
  return &blocksbuffers_;
}

// -------------------------------------------------------------------

// BlockReportResponseProto

// optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
inline bool BlockReportResponseProto::has_cmd() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockReportResponseProto::set_has_cmd() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockReportResponseProto::clear_has_cmd() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockReportResponseProto::clear_cmd() {
  if (cmd_ != NULL) cmd_->Clear();
  clear_has_cmd();
}
inline const ::hadoop::hdfs::datanode::DatanodeCommandProto& BlockReportResponseProto::_internal_cmd() const {
  return *cmd_;
}
inline const ::hadoop::hdfs::datanode::DatanodeCommandProto& BlockReportResponseProto::cmd() const {
  const ::hadoop::hdfs::datanode::DatanodeCommandProto* p = cmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReportResponseProto.cmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeCommandProto*>(
      &::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* BlockReportResponseProto::release_cmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReportResponseProto.cmd)
  clear_has_cmd();
  ::hadoop::hdfs::datanode::DatanodeCommandProto* temp = cmd_;
  cmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* BlockReportResponseProto::mutable_cmd() {
  set_has_cmd();
  if (cmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeCommandProto>(GetArenaNoVirtual());
    cmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReportResponseProto.cmd)
  return cmd_;
}
inline void BlockReportResponseProto::set_allocated_cmd(::hadoop::hdfs::datanode::DatanodeCommandProto* cmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete cmd_;
  }
  if (cmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      cmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, cmd, submessage_arena);
    }
    set_has_cmd();
  } else {
    clear_has_cmd();
  }
  cmd_ = cmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReportResponseProto.cmd)
}

// -------------------------------------------------------------------

// CacheReportRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool CacheReportRequestProto::has_registration() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void CacheReportRequestProto::set_has_registration() {
  _has_bits_[0] |= 0x00000002u;
}
inline void CacheReportRequestProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void CacheReportRequestProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& CacheReportRequestProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& CacheReportRequestProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CacheReportRequestProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* CacheReportRequestProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.CacheReportRequestProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* CacheReportRequestProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CacheReportRequestProto.registration)
  return registration_;
}
inline void CacheReportRequestProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.CacheReportRequestProto.registration)
}

// required string blockPoolId = 2;
inline bool CacheReportRequestProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void CacheReportRequestProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void CacheReportRequestProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void CacheReportRequestProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& CacheReportRequestProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void CacheReportRequestProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
}
#if LANG_CXX11
inline void CacheReportRequestProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
}
#endif
inline void CacheReportRequestProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
}
inline void CacheReportRequestProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
}
inline ::std::string* CacheReportRequestProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* CacheReportRequestProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void CacheReportRequestProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.CacheReportRequestProto.blockPoolId)
}

// repeated uint64 blocks = 3 [packed = true];
inline int CacheReportRequestProto::blocks_size() const {
  return blocks_.size();
}
inline void CacheReportRequestProto::clear_blocks() {
  blocks_.Clear();
}
inline ::google::protobuf::uint64 CacheReportRequestProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CacheReportRequestProto.blocks)
  return blocks_.Get(index);
}
inline void CacheReportRequestProto::set_blocks(int index, ::google::protobuf::uint64 value) {
  blocks_.Set(index, value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CacheReportRequestProto.blocks)
}
inline void CacheReportRequestProto::add_blocks(::google::protobuf::uint64 value) {
  blocks_.Add(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.CacheReportRequestProto.blocks)
}
inline const ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >&
CacheReportRequestProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.CacheReportRequestProto.blocks)
  return blocks_;
}
inline ::google::protobuf::RepeatedField< ::google::protobuf::uint64 >*
CacheReportRequestProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.CacheReportRequestProto.blocks)
  return &blocks_;
}

// -------------------------------------------------------------------

// CacheReportResponseProto

// optional .hadoop.hdfs.datanode.DatanodeCommandProto cmd = 1;
inline bool CacheReportResponseProto::has_cmd() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void CacheReportResponseProto::set_has_cmd() {
  _has_bits_[0] |= 0x00000001u;
}
inline void CacheReportResponseProto::clear_has_cmd() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void CacheReportResponseProto::clear_cmd() {
  if (cmd_ != NULL) cmd_->Clear();
  clear_has_cmd();
}
inline const ::hadoop::hdfs::datanode::DatanodeCommandProto& CacheReportResponseProto::_internal_cmd() const {
  return *cmd_;
}
inline const ::hadoop::hdfs::datanode::DatanodeCommandProto& CacheReportResponseProto::cmd() const {
  const ::hadoop::hdfs::datanode::DatanodeCommandProto* p = cmd_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CacheReportResponseProto.cmd)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeCommandProto*>(
      &::hadoop::hdfs::datanode::_DatanodeCommandProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* CacheReportResponseProto::release_cmd() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.CacheReportResponseProto.cmd)
  clear_has_cmd();
  ::hadoop::hdfs::datanode::DatanodeCommandProto* temp = cmd_;
  cmd_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeCommandProto* CacheReportResponseProto::mutable_cmd() {
  set_has_cmd();
  if (cmd_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeCommandProto>(GetArenaNoVirtual());
    cmd_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CacheReportResponseProto.cmd)
  return cmd_;
}
inline void CacheReportResponseProto::set_allocated_cmd(::hadoop::hdfs::datanode::DatanodeCommandProto* cmd) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete cmd_;
  }
  if (cmd) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      cmd = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, cmd, submessage_arena);
    }
    set_has_cmd();
  } else {
    clear_has_cmd();
  }
  cmd_ = cmd;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.CacheReportResponseProto.cmd)
}

// -------------------------------------------------------------------

// ReceivedDeletedBlockInfoProto

// required .hadoop.hdfs.BlockProto block = 1;
inline bool ReceivedDeletedBlockInfoProto::has_block() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void ReceivedDeletedBlockInfoProto::set_has_block() {
  _has_bits_[0] |= 0x00000002u;
}
inline void ReceivedDeletedBlockInfoProto::clear_has_block() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::hadoop::hdfs::BlockProto& ReceivedDeletedBlockInfoProto::_internal_block() const {
  return *block_;
}
inline const ::hadoop::hdfs::BlockProto& ReceivedDeletedBlockInfoProto::block() const {
  const ::hadoop::hdfs::BlockProto* p = block_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.block)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::BlockProto*>(
      &::hadoop::hdfs::_BlockProto_default_instance_);
}
inline ::hadoop::hdfs::BlockProto* ReceivedDeletedBlockInfoProto::release_block() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.block)
  clear_has_block();
  ::hadoop::hdfs::BlockProto* temp = block_;
  block_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::BlockProto* ReceivedDeletedBlockInfoProto::mutable_block() {
  set_has_block();
  if (block_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::BlockProto>(GetArenaNoVirtual());
    block_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.block)
  return block_;
}
inline void ReceivedDeletedBlockInfoProto::set_allocated_block(::hadoop::hdfs::BlockProto* block) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(block_);
  }
  if (block) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      block = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, block, submessage_arena);
    }
    set_has_block();
  } else {
    clear_has_block();
  }
  block_ = block;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.block)
}

// required .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.BlockStatus status = 3;
inline bool ReceivedDeletedBlockInfoProto::has_status() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void ReceivedDeletedBlockInfoProto::set_has_status() {
  _has_bits_[0] |= 0x00000004u;
}
inline void ReceivedDeletedBlockInfoProto::clear_has_status() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void ReceivedDeletedBlockInfoProto::clear_status() {
  status_ = 1;
  clear_has_status();
}
inline ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus ReceivedDeletedBlockInfoProto::status() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.status)
  return static_cast< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus >(status_);
}
inline void ReceivedDeletedBlockInfoProto::set_status(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus value) {
  assert(::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus_IsValid(value));
  set_has_status();
  status_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.status)
}

// optional string deleteHint = 2;
inline bool ReceivedDeletedBlockInfoProto::has_deletehint() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void ReceivedDeletedBlockInfoProto::set_has_deletehint() {
  _has_bits_[0] |= 0x00000001u;
}
inline void ReceivedDeletedBlockInfoProto::clear_has_deletehint() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void ReceivedDeletedBlockInfoProto::clear_deletehint() {
  deletehint_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_deletehint();
}
inline const ::std::string& ReceivedDeletedBlockInfoProto::deletehint() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
  return deletehint_.GetNoArena();
}
inline void ReceivedDeletedBlockInfoProto::set_deletehint(const ::std::string& value) {
  set_has_deletehint();
  deletehint_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
}
#if LANG_CXX11
inline void ReceivedDeletedBlockInfoProto::set_deletehint(::std::string&& value) {
  set_has_deletehint();
  deletehint_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
}
#endif
inline void ReceivedDeletedBlockInfoProto::set_deletehint(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_deletehint();
  deletehint_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
}
inline void ReceivedDeletedBlockInfoProto::set_deletehint(const char* value, size_t size) {
  set_has_deletehint();
  deletehint_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
}
inline ::std::string* ReceivedDeletedBlockInfoProto::mutable_deletehint() {
  set_has_deletehint();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
  return deletehint_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ReceivedDeletedBlockInfoProto::release_deletehint() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
  if (!has_deletehint()) {
    return NULL;
  }
  clear_has_deletehint();
  return deletehint_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ReceivedDeletedBlockInfoProto::set_allocated_deletehint(::std::string* deletehint) {
  if (deletehint != NULL) {
    set_has_deletehint();
  } else {
    clear_has_deletehint();
  }
  deletehint_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), deletehint);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto.deleteHint)
}

// -------------------------------------------------------------------

// StorageReceivedDeletedBlocksProto

// required string storageUuid = 1 [deprecated = true];
inline bool StorageReceivedDeletedBlocksProto::has_storageuuid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void StorageReceivedDeletedBlocksProto::set_has_storageuuid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void StorageReceivedDeletedBlocksProto::clear_has_storageuuid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void StorageReceivedDeletedBlocksProto::clear_storageuuid() {
  storageuuid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_storageuuid();
}
inline const ::std::string& StorageReceivedDeletedBlocksProto::storageuuid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
  return storageuuid_.GetNoArena();
}
inline void StorageReceivedDeletedBlocksProto::set_storageuuid(const ::std::string& value) {
  set_has_storageuuid();
  storageuuid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
}
#if LANG_CXX11
inline void StorageReceivedDeletedBlocksProto::set_storageuuid(::std::string&& value) {
  set_has_storageuuid();
  storageuuid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
}
#endif
inline void StorageReceivedDeletedBlocksProto::set_storageuuid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_storageuuid();
  storageuuid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
}
inline void StorageReceivedDeletedBlocksProto::set_storageuuid(const char* value, size_t size) {
  set_has_storageuuid();
  storageuuid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
}
inline ::std::string* StorageReceivedDeletedBlocksProto::mutable_storageuuid() {
  set_has_storageuuid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
  return storageuuid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* StorageReceivedDeletedBlocksProto::release_storageuuid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
  if (!has_storageuuid()) {
    return NULL;
  }
  clear_has_storageuuid();
  return storageuuid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void StorageReceivedDeletedBlocksProto::set_allocated_storageuuid(::std::string* storageuuid) {
  if (storageuuid != NULL) {
    set_has_storageuuid();
  } else {
    clear_has_storageuuid();
  }
  storageuuid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), storageuuid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storageUuid)
}

// repeated .hadoop.hdfs.datanode.ReceivedDeletedBlockInfoProto blocks = 2;
inline int StorageReceivedDeletedBlocksProto::blocks_size() const {
  return blocks_.size();
}
inline void StorageReceivedDeletedBlocksProto::clear_blocks() {
  blocks_.Clear();
}
inline ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* StorageReceivedDeletedBlocksProto::mutable_blocks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.blocks)
  return blocks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >*
StorageReceivedDeletedBlocksProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.blocks)
  return &blocks_;
}
inline const ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto& StorageReceivedDeletedBlocksProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.blocks)
  return blocks_.Get(index);
}
inline ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto* StorageReceivedDeletedBlocksProto::add_blocks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.blocks)
  return blocks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto >&
StorageReceivedDeletedBlocksProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.blocks)
  return blocks_;
}

// optional .hadoop.hdfs.DatanodeStorageProto storage = 3;
inline bool StorageReceivedDeletedBlocksProto::has_storage() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void StorageReceivedDeletedBlocksProto::set_has_storage() {
  _has_bits_[0] |= 0x00000002u;
}
inline void StorageReceivedDeletedBlocksProto::clear_has_storage() {
  _has_bits_[0] &= ~0x00000002u;
}
inline const ::hadoop::hdfs::DatanodeStorageProto& StorageReceivedDeletedBlocksProto::_internal_storage() const {
  return *storage_;
}
inline const ::hadoop::hdfs::DatanodeStorageProto& StorageReceivedDeletedBlocksProto::storage() const {
  const ::hadoop::hdfs::DatanodeStorageProto* p = storage_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storage)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::DatanodeStorageProto*>(
      &::hadoop::hdfs::_DatanodeStorageProto_default_instance_);
}
inline ::hadoop::hdfs::DatanodeStorageProto* StorageReceivedDeletedBlocksProto::release_storage() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storage)
  clear_has_storage();
  ::hadoop::hdfs::DatanodeStorageProto* temp = storage_;
  storage_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::DatanodeStorageProto* StorageReceivedDeletedBlocksProto::mutable_storage() {
  set_has_storage();
  if (storage_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::DatanodeStorageProto>(GetArenaNoVirtual());
    storage_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storage)
  return storage_;
}
inline void StorageReceivedDeletedBlocksProto::set_allocated_storage(::hadoop::hdfs::DatanodeStorageProto* storage) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(storage_);
  }
  if (storage) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      storage = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, storage, submessage_arena);
    }
    set_has_storage();
  } else {
    clear_has_storage();
  }
  storage_ = storage;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto.storage)
}

// -------------------------------------------------------------------

// BlockReceivedAndDeletedRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registration = 1;
inline bool BlockReceivedAndDeletedRequestProto::has_registration() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void BlockReceivedAndDeletedRequestProto::set_has_registration() {
  _has_bits_[0] |= 0x00000002u;
}
inline void BlockReceivedAndDeletedRequestProto::clear_has_registration() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void BlockReceivedAndDeletedRequestProto::clear_registration() {
  if (registration_ != NULL) registration_->Clear();
  clear_has_registration();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& BlockReceivedAndDeletedRequestProto::_internal_registration() const {
  return *registration_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& BlockReceivedAndDeletedRequestProto::registration() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registration_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.registration)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* BlockReceivedAndDeletedRequestProto::release_registration() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.registration)
  clear_has_registration();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registration_;
  registration_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* BlockReceivedAndDeletedRequestProto::mutable_registration() {
  set_has_registration();
  if (registration_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registration_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.registration)
  return registration_;
}
inline void BlockReceivedAndDeletedRequestProto::set_allocated_registration(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registration) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registration_;
  }
  if (registration) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registration = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registration, submessage_arena);
    }
    set_has_registration();
  } else {
    clear_has_registration();
  }
  registration_ = registration;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.registration)
}

// required string blockPoolId = 2;
inline bool BlockReceivedAndDeletedRequestProto::has_blockpoolid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void BlockReceivedAndDeletedRequestProto::set_has_blockpoolid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void BlockReceivedAndDeletedRequestProto::clear_has_blockpoolid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void BlockReceivedAndDeletedRequestProto::clear_blockpoolid() {
  blockpoolid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_blockpoolid();
}
inline const ::std::string& BlockReceivedAndDeletedRequestProto::blockpoolid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
  return blockpoolid_.GetNoArena();
}
inline void BlockReceivedAndDeletedRequestProto::set_blockpoolid(const ::std::string& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
}
#if LANG_CXX11
inline void BlockReceivedAndDeletedRequestProto::set_blockpoolid(::std::string&& value) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
}
#endif
inline void BlockReceivedAndDeletedRequestProto::set_blockpoolid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
}
inline void BlockReceivedAndDeletedRequestProto::set_blockpoolid(const char* value, size_t size) {
  set_has_blockpoolid();
  blockpoolid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
}
inline ::std::string* BlockReceivedAndDeletedRequestProto::mutable_blockpoolid() {
  set_has_blockpoolid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
  return blockpoolid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* BlockReceivedAndDeletedRequestProto::release_blockpoolid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
  if (!has_blockpoolid()) {
    return NULL;
  }
  clear_has_blockpoolid();
  return blockpoolid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void BlockReceivedAndDeletedRequestProto::set_allocated_blockpoolid(::std::string* blockpoolid) {
  if (blockpoolid != NULL) {
    set_has_blockpoolid();
  } else {
    clear_has_blockpoolid();
  }
  blockpoolid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), blockpoolid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blockPoolId)
}

// repeated .hadoop.hdfs.datanode.StorageReceivedDeletedBlocksProto blocks = 3;
inline int BlockReceivedAndDeletedRequestProto::blocks_size() const {
  return blocks_.size();
}
inline void BlockReceivedAndDeletedRequestProto::clear_blocks() {
  blocks_.Clear();
}
inline ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* BlockReceivedAndDeletedRequestProto::mutable_blocks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blocks)
  return blocks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >*
BlockReceivedAndDeletedRequestProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blocks)
  return &blocks_;
}
inline const ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto& BlockReceivedAndDeletedRequestProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blocks)
  return blocks_.Get(index);
}
inline ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto* BlockReceivedAndDeletedRequestProto::add_blocks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blocks)
  return blocks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::datanode::StorageReceivedDeletedBlocksProto >&
BlockReceivedAndDeletedRequestProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.BlockReceivedAndDeletedRequestProto.blocks)
  return blocks_;
}

// -------------------------------------------------------------------

// BlockReceivedAndDeletedResponseProto

// -------------------------------------------------------------------

// ErrorReportRequestProto

// required .hadoop.hdfs.datanode.DatanodeRegistrationProto registartion = 1;
inline bool ErrorReportRequestProto::has_registartion() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void ErrorReportRequestProto::set_has_registartion() {
  _has_bits_[0] |= 0x00000002u;
}
inline void ErrorReportRequestProto::clear_has_registartion() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void ErrorReportRequestProto::clear_registartion() {
  if (registartion_ != NULL) registartion_->Clear();
  clear_has_registartion();
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& ErrorReportRequestProto::_internal_registartion() const {
  return *registartion_;
}
inline const ::hadoop::hdfs::datanode::DatanodeRegistrationProto& ErrorReportRequestProto::registartion() const {
  const ::hadoop::hdfs::datanode::DatanodeRegistrationProto* p = registartion_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ErrorReportRequestProto.registartion)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::datanode::DatanodeRegistrationProto*>(
      &::hadoop::hdfs::datanode::_DatanodeRegistrationProto_default_instance_);
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* ErrorReportRequestProto::release_registartion() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.ErrorReportRequestProto.registartion)
  clear_has_registartion();
  ::hadoop::hdfs::datanode::DatanodeRegistrationProto* temp = registartion_;
  registartion_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::datanode::DatanodeRegistrationProto* ErrorReportRequestProto::mutable_registartion() {
  set_has_registartion();
  if (registartion_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::datanode::DatanodeRegistrationProto>(GetArenaNoVirtual());
    registartion_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.ErrorReportRequestProto.registartion)
  return registartion_;
}
inline void ErrorReportRequestProto::set_allocated_registartion(::hadoop::hdfs::datanode::DatanodeRegistrationProto* registartion) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete registartion_;
  }
  if (registartion) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      registartion = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, registartion, submessage_arena);
    }
    set_has_registartion();
  } else {
    clear_has_registartion();
  }
  registartion_ = registartion;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.ErrorReportRequestProto.registartion)
}

// required uint32 errorCode = 2;
inline bool ErrorReportRequestProto::has_errorcode() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void ErrorReportRequestProto::set_has_errorcode() {
  _has_bits_[0] |= 0x00000004u;
}
inline void ErrorReportRequestProto::clear_has_errorcode() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void ErrorReportRequestProto::clear_errorcode() {
  errorcode_ = 0u;
  clear_has_errorcode();
}
inline ::google::protobuf::uint32 ErrorReportRequestProto::errorcode() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ErrorReportRequestProto.errorCode)
  return errorcode_;
}
inline void ErrorReportRequestProto::set_errorcode(::google::protobuf::uint32 value) {
  set_has_errorcode();
  errorcode_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.ErrorReportRequestProto.errorCode)
}

// required string msg = 3;
inline bool ErrorReportRequestProto::has_msg() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void ErrorReportRequestProto::set_has_msg() {
  _has_bits_[0] |= 0x00000001u;
}
inline void ErrorReportRequestProto::clear_has_msg() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void ErrorReportRequestProto::clear_msg() {
  msg_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_msg();
}
inline const ::std::string& ErrorReportRequestProto::msg() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
  return msg_.GetNoArena();
}
inline void ErrorReportRequestProto::set_msg(const ::std::string& value) {
  set_has_msg();
  msg_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
}
#if LANG_CXX11
inline void ErrorReportRequestProto::set_msg(::std::string&& value) {
  set_has_msg();
  msg_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
}
#endif
inline void ErrorReportRequestProto::set_msg(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_msg();
  msg_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
}
inline void ErrorReportRequestProto::set_msg(const char* value, size_t size) {
  set_has_msg();
  msg_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
}
inline ::std::string* ErrorReportRequestProto::mutable_msg() {
  set_has_msg();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
  return msg_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* ErrorReportRequestProto::release_msg() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
  if (!has_msg()) {
    return NULL;
  }
  clear_has_msg();
  return msg_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void ErrorReportRequestProto::set_allocated_msg(::std::string* msg) {
  if (msg != NULL) {
    set_has_msg();
  } else {
    clear_has_msg();
  }
  msg_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), msg);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.ErrorReportRequestProto.msg)
}

// -------------------------------------------------------------------

// ErrorReportResponseProto

// -------------------------------------------------------------------

// ReportBadBlocksRequestProto

// repeated .hadoop.hdfs.LocatedBlockProto blocks = 1;
inline int ReportBadBlocksRequestProto::blocks_size() const {
  return blocks_.size();
}
inline ::hadoop::hdfs::LocatedBlockProto* ReportBadBlocksRequestProto::mutable_blocks(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.ReportBadBlocksRequestProto.blocks)
  return blocks_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::LocatedBlockProto >*
ReportBadBlocksRequestProto::mutable_blocks() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.ReportBadBlocksRequestProto.blocks)
  return &blocks_;
}
inline const ::hadoop::hdfs::LocatedBlockProto& ReportBadBlocksRequestProto::blocks(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.ReportBadBlocksRequestProto.blocks)
  return blocks_.Get(index);
}
inline ::hadoop::hdfs::LocatedBlockProto* ReportBadBlocksRequestProto::add_blocks() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.ReportBadBlocksRequestProto.blocks)
  return blocks_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::LocatedBlockProto >&
ReportBadBlocksRequestProto::blocks() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.ReportBadBlocksRequestProto.blocks)
  return blocks_;
}

// -------------------------------------------------------------------

// ReportBadBlocksResponseProto

// -------------------------------------------------------------------

// CommitBlockSynchronizationRequestProto

// required .hadoop.hdfs.ExtendedBlockProto block = 1;
inline bool CommitBlockSynchronizationRequestProto::has_block() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void CommitBlockSynchronizationRequestProto::set_has_block() {
  _has_bits_[0] |= 0x00000001u;
}
inline void CommitBlockSynchronizationRequestProto::clear_has_block() {
  _has_bits_[0] &= ~0x00000001u;
}
inline const ::hadoop::hdfs::ExtendedBlockProto& CommitBlockSynchronizationRequestProto::_internal_block() const {
  return *block_;
}
inline const ::hadoop::hdfs::ExtendedBlockProto& CommitBlockSynchronizationRequestProto::block() const {
  const ::hadoop::hdfs::ExtendedBlockProto* p = block_;
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.block)
  return p != NULL ? *p : *reinterpret_cast<const ::hadoop::hdfs::ExtendedBlockProto*>(
      &::hadoop::hdfs::_ExtendedBlockProto_default_instance_);
}
inline ::hadoop::hdfs::ExtendedBlockProto* CommitBlockSynchronizationRequestProto::release_block() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.block)
  clear_has_block();
  ::hadoop::hdfs::ExtendedBlockProto* temp = block_;
  block_ = NULL;
  return temp;
}
inline ::hadoop::hdfs::ExtendedBlockProto* CommitBlockSynchronizationRequestProto::mutable_block() {
  set_has_block();
  if (block_ == NULL) {
    auto* p = CreateMaybeMessage<::hadoop::hdfs::ExtendedBlockProto>(GetArenaNoVirtual());
    block_ = p;
  }
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.block)
  return block_;
}
inline void CommitBlockSynchronizationRequestProto::set_allocated_block(::hadoop::hdfs::ExtendedBlockProto* block) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  if (message_arena == NULL) {
    delete reinterpret_cast< ::google::protobuf::MessageLite*>(block_);
  }
  if (block) {
    ::google::protobuf::Arena* submessage_arena = NULL;
    if (message_arena != submessage_arena) {
      block = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, block, submessage_arena);
    }
    set_has_block();
  } else {
    clear_has_block();
  }
  block_ = block;
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.block)
}

// required uint64 newGenStamp = 2;
inline bool CommitBlockSynchronizationRequestProto::has_newgenstamp() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void CommitBlockSynchronizationRequestProto::set_has_newgenstamp() {
  _has_bits_[0] |= 0x00000002u;
}
inline void CommitBlockSynchronizationRequestProto::clear_has_newgenstamp() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void CommitBlockSynchronizationRequestProto::clear_newgenstamp() {
  newgenstamp_ = GOOGLE_ULONGLONG(0);
  clear_has_newgenstamp();
}
inline ::google::protobuf::uint64 CommitBlockSynchronizationRequestProto::newgenstamp() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newGenStamp)
  return newgenstamp_;
}
inline void CommitBlockSynchronizationRequestProto::set_newgenstamp(::google::protobuf::uint64 value) {
  set_has_newgenstamp();
  newgenstamp_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newGenStamp)
}

// required uint64 newLength = 3;
inline bool CommitBlockSynchronizationRequestProto::has_newlength() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void CommitBlockSynchronizationRequestProto::set_has_newlength() {
  _has_bits_[0] |= 0x00000004u;
}
inline void CommitBlockSynchronizationRequestProto::clear_has_newlength() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void CommitBlockSynchronizationRequestProto::clear_newlength() {
  newlength_ = GOOGLE_ULONGLONG(0);
  clear_has_newlength();
}
inline ::google::protobuf::uint64 CommitBlockSynchronizationRequestProto::newlength() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newLength)
  return newlength_;
}
inline void CommitBlockSynchronizationRequestProto::set_newlength(::google::protobuf::uint64 value) {
  set_has_newlength();
  newlength_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newLength)
}

// required bool closeFile = 4;
inline bool CommitBlockSynchronizationRequestProto::has_closefile() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void CommitBlockSynchronizationRequestProto::set_has_closefile() {
  _has_bits_[0] |= 0x00000008u;
}
inline void CommitBlockSynchronizationRequestProto::clear_has_closefile() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void CommitBlockSynchronizationRequestProto::clear_closefile() {
  closefile_ = false;
  clear_has_closefile();
}
inline bool CommitBlockSynchronizationRequestProto::closefile() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.closeFile)
  return closefile_;
}
inline void CommitBlockSynchronizationRequestProto::set_closefile(bool value) {
  set_has_closefile();
  closefile_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.closeFile)
}

// required bool deleteBlock = 5;
inline bool CommitBlockSynchronizationRequestProto::has_deleteblock() const {
  return (_has_bits_[0] & 0x00000010u) != 0;
}
inline void CommitBlockSynchronizationRequestProto::set_has_deleteblock() {
  _has_bits_[0] |= 0x00000010u;
}
inline void CommitBlockSynchronizationRequestProto::clear_has_deleteblock() {
  _has_bits_[0] &= ~0x00000010u;
}
inline void CommitBlockSynchronizationRequestProto::clear_deleteblock() {
  deleteblock_ = false;
  clear_has_deleteblock();
}
inline bool CommitBlockSynchronizationRequestProto::deleteblock() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.deleteBlock)
  return deleteblock_;
}
inline void CommitBlockSynchronizationRequestProto::set_deleteblock(bool value) {
  set_has_deleteblock();
  deleteblock_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.deleteBlock)
}

// repeated .hadoop.hdfs.DatanodeIDProto newTaragets = 6;
inline int CommitBlockSynchronizationRequestProto::newtaragets_size() const {
  return newtaragets_.size();
}
inline ::hadoop::hdfs::DatanodeIDProto* CommitBlockSynchronizationRequestProto::mutable_newtaragets(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTaragets)
  return newtaragets_.Mutable(index);
}
inline ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeIDProto >*
CommitBlockSynchronizationRequestProto::mutable_newtaragets() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTaragets)
  return &newtaragets_;
}
inline const ::hadoop::hdfs::DatanodeIDProto& CommitBlockSynchronizationRequestProto::newtaragets(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTaragets)
  return newtaragets_.Get(index);
}
inline ::hadoop::hdfs::DatanodeIDProto* CommitBlockSynchronizationRequestProto::add_newtaragets() {
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTaragets)
  return newtaragets_.Add();
}
inline const ::google::protobuf::RepeatedPtrField< ::hadoop::hdfs::DatanodeIDProto >&
CommitBlockSynchronizationRequestProto::newtaragets() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTaragets)
  return newtaragets_;
}

// repeated string newTargetStorages = 7;
inline int CommitBlockSynchronizationRequestProto::newtargetstorages_size() const {
  return newtargetstorages_.size();
}
inline void CommitBlockSynchronizationRequestProto::clear_newtargetstorages() {
  newtargetstorages_.Clear();
}
inline const ::std::string& CommitBlockSynchronizationRequestProto::newtargetstorages(int index) const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  return newtargetstorages_.Get(index);
}
inline ::std::string* CommitBlockSynchronizationRequestProto::mutable_newtargetstorages(int index) {
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  return newtargetstorages_.Mutable(index);
}
inline void CommitBlockSynchronizationRequestProto::set_newtargetstorages(int index, const ::std::string& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  newtargetstorages_.Mutable(index)->assign(value);
}
#if LANG_CXX11
inline void CommitBlockSynchronizationRequestProto::set_newtargetstorages(int index, ::std::string&& value) {
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  newtargetstorages_.Mutable(index)->assign(std::move(value));
}
#endif
inline void CommitBlockSynchronizationRequestProto::set_newtargetstorages(int index, const char* value) {
  GOOGLE_DCHECK(value != NULL);
  newtargetstorages_.Mutable(index)->assign(value);
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
inline void CommitBlockSynchronizationRequestProto::set_newtargetstorages(int index, const char* value, size_t size) {
  newtargetstorages_.Mutable(index)->assign(
    reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
inline ::std::string* CommitBlockSynchronizationRequestProto::add_newtargetstorages() {
  // @@protoc_insertion_point(field_add_mutable:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  return newtargetstorages_.Add();
}
inline void CommitBlockSynchronizationRequestProto::add_newtargetstorages(const ::std::string& value) {
  newtargetstorages_.Add()->assign(value);
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
#if LANG_CXX11
inline void CommitBlockSynchronizationRequestProto::add_newtargetstorages(::std::string&& value) {
  newtargetstorages_.Add(std::move(value));
  // @@protoc_insertion_point(field_add:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
#endif
inline void CommitBlockSynchronizationRequestProto::add_newtargetstorages(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  newtargetstorages_.Add()->assign(value);
  // @@protoc_insertion_point(field_add_char:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
inline void CommitBlockSynchronizationRequestProto::add_newtargetstorages(const char* value, size_t size) {
  newtargetstorages_.Add()->assign(reinterpret_cast<const char*>(value), size);
  // @@protoc_insertion_point(field_add_pointer:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
}
inline const ::google::protobuf::RepeatedPtrField< ::std::string>&
CommitBlockSynchronizationRequestProto::newtargetstorages() const {
  // @@protoc_insertion_point(field_list:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  return newtargetstorages_;
}
inline ::google::protobuf::RepeatedPtrField< ::std::string>*
CommitBlockSynchronizationRequestProto::mutable_newtargetstorages() {
  // @@protoc_insertion_point(field_mutable_list:hadoop.hdfs.datanode.CommitBlockSynchronizationRequestProto.newTargetStorages)
  return &newtargetstorages_;
}

// -------------------------------------------------------------------

// CommitBlockSynchronizationResponseProto

// -------------------------------------------------------------------

// SlowPeerReportProto

// optional string dataNodeId = 1;
inline bool SlowPeerReportProto::has_datanodeid() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void SlowPeerReportProto::set_has_datanodeid() {
  _has_bits_[0] |= 0x00000001u;
}
inline void SlowPeerReportProto::clear_has_datanodeid() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void SlowPeerReportProto::clear_datanodeid() {
  datanodeid_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_datanodeid();
}
inline const ::std::string& SlowPeerReportProto::datanodeid() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
  return datanodeid_.GetNoArena();
}
inline void SlowPeerReportProto::set_datanodeid(const ::std::string& value) {
  set_has_datanodeid();
  datanodeid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
}
#if LANG_CXX11
inline void SlowPeerReportProto::set_datanodeid(::std::string&& value) {
  set_has_datanodeid();
  datanodeid_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
}
#endif
inline void SlowPeerReportProto::set_datanodeid(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_datanodeid();
  datanodeid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
}
inline void SlowPeerReportProto::set_datanodeid(const char* value, size_t size) {
  set_has_datanodeid();
  datanodeid_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
}
inline ::std::string* SlowPeerReportProto::mutable_datanodeid() {
  set_has_datanodeid();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
  return datanodeid_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* SlowPeerReportProto::release_datanodeid() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
  if (!has_datanodeid()) {
    return NULL;
  }
  clear_has_datanodeid();
  return datanodeid_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void SlowPeerReportProto::set_allocated_datanodeid(::std::string* datanodeid) {
  if (datanodeid != NULL) {
    set_has_datanodeid();
  } else {
    clear_has_datanodeid();
  }
  datanodeid_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), datanodeid);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.SlowPeerReportProto.dataNodeId)
}

// optional double aggregateLatency = 2;
inline bool SlowPeerReportProto::has_aggregatelatency() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void SlowPeerReportProto::set_has_aggregatelatency() {
  _has_bits_[0] |= 0x00000002u;
}
inline void SlowPeerReportProto::clear_has_aggregatelatency() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void SlowPeerReportProto::clear_aggregatelatency() {
  aggregatelatency_ = 0;
  clear_has_aggregatelatency();
}
inline double SlowPeerReportProto::aggregatelatency() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowPeerReportProto.aggregateLatency)
  return aggregatelatency_;
}
inline void SlowPeerReportProto::set_aggregatelatency(double value) {
  set_has_aggregatelatency();
  aggregatelatency_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowPeerReportProto.aggregateLatency)
}

// optional double median = 3;
inline bool SlowPeerReportProto::has_median() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void SlowPeerReportProto::set_has_median() {
  _has_bits_[0] |= 0x00000004u;
}
inline void SlowPeerReportProto::clear_has_median() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void SlowPeerReportProto::clear_median() {
  median_ = 0;
  clear_has_median();
}
inline double SlowPeerReportProto::median() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowPeerReportProto.median)
  return median_;
}
inline void SlowPeerReportProto::set_median(double value) {
  set_has_median();
  median_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowPeerReportProto.median)
}

// optional double mad = 4;
inline bool SlowPeerReportProto::has_mad() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void SlowPeerReportProto::set_has_mad() {
  _has_bits_[0] |= 0x00000008u;
}
inline void SlowPeerReportProto::clear_has_mad() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void SlowPeerReportProto::clear_mad() {
  mad_ = 0;
  clear_has_mad();
}
inline double SlowPeerReportProto::mad() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowPeerReportProto.mad)
  return mad_;
}
inline void SlowPeerReportProto::set_mad(double value) {
  set_has_mad();
  mad_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowPeerReportProto.mad)
}

// optional double upperLimitLatency = 5;
inline bool SlowPeerReportProto::has_upperlimitlatency() const {
  return (_has_bits_[0] & 0x00000010u) != 0;
}
inline void SlowPeerReportProto::set_has_upperlimitlatency() {
  _has_bits_[0] |= 0x00000010u;
}
inline void SlowPeerReportProto::clear_has_upperlimitlatency() {
  _has_bits_[0] &= ~0x00000010u;
}
inline void SlowPeerReportProto::clear_upperlimitlatency() {
  upperlimitlatency_ = 0;
  clear_has_upperlimitlatency();
}
inline double SlowPeerReportProto::upperlimitlatency() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowPeerReportProto.upperLimitLatency)
  return upperlimitlatency_;
}
inline void SlowPeerReportProto::set_upperlimitlatency(double value) {
  set_has_upperlimitlatency();
  upperlimitlatency_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowPeerReportProto.upperLimitLatency)
}

// -------------------------------------------------------------------

// SlowDiskReportProto

// optional string basePath = 1;
inline bool SlowDiskReportProto::has_basepath() const {
  return (_has_bits_[0] & 0x00000001u) != 0;
}
inline void SlowDiskReportProto::set_has_basepath() {
  _has_bits_[0] |= 0x00000001u;
}
inline void SlowDiskReportProto::clear_has_basepath() {
  _has_bits_[0] &= ~0x00000001u;
}
inline void SlowDiskReportProto::clear_basepath() {
  basepath_.ClearToEmptyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  clear_has_basepath();
}
inline const ::std::string& SlowDiskReportProto::basepath() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
  return basepath_.GetNoArena();
}
inline void SlowDiskReportProto::set_basepath(const ::std::string& value) {
  set_has_basepath();
  basepath_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), value);
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
}
#if LANG_CXX11
inline void SlowDiskReportProto::set_basepath(::std::string&& value) {
  set_has_basepath();
  basepath_.SetNoArena(
    &::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::move(value));
  // @@protoc_insertion_point(field_set_rvalue:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
}
#endif
inline void SlowDiskReportProto::set_basepath(const char* value) {
  GOOGLE_DCHECK(value != NULL);
  set_has_basepath();
  basepath_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), ::std::string(value));
  // @@protoc_insertion_point(field_set_char:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
}
inline void SlowDiskReportProto::set_basepath(const char* value, size_t size) {
  set_has_basepath();
  basepath_.SetNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(),
      ::std::string(reinterpret_cast<const char*>(value), size));
  // @@protoc_insertion_point(field_set_pointer:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
}
inline ::std::string* SlowDiskReportProto::mutable_basepath() {
  set_has_basepath();
  // @@protoc_insertion_point(field_mutable:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
  return basepath_.MutableNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline ::std::string* SlowDiskReportProto::release_basepath() {
  // @@protoc_insertion_point(field_release:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
  if (!has_basepath()) {
    return NULL;
  }
  clear_has_basepath();
  return basepath_.ReleaseNonDefaultNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
}
inline void SlowDiskReportProto::set_allocated_basepath(::std::string* basepath) {
  if (basepath != NULL) {
    set_has_basepath();
  } else {
    clear_has_basepath();
  }
  basepath_.SetAllocatedNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), basepath);
  // @@protoc_insertion_point(field_set_allocated:hadoop.hdfs.datanode.SlowDiskReportProto.basePath)
}

// optional double meanMetadataOpLatency = 2;
inline bool SlowDiskReportProto::has_meanmetadataoplatency() const {
  return (_has_bits_[0] & 0x00000002u) != 0;
}
inline void SlowDiskReportProto::set_has_meanmetadataoplatency() {
  _has_bits_[0] |= 0x00000002u;
}
inline void SlowDiskReportProto::clear_has_meanmetadataoplatency() {
  _has_bits_[0] &= ~0x00000002u;
}
inline void SlowDiskReportProto::clear_meanmetadataoplatency() {
  meanmetadataoplatency_ = 0;
  clear_has_meanmetadataoplatency();
}
inline double SlowDiskReportProto::meanmetadataoplatency() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowDiskReportProto.meanMetadataOpLatency)
  return meanmetadataoplatency_;
}
inline void SlowDiskReportProto::set_meanmetadataoplatency(double value) {
  set_has_meanmetadataoplatency();
  meanmetadataoplatency_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowDiskReportProto.meanMetadataOpLatency)
}

// optional double meanReadIoLatency = 3;
inline bool SlowDiskReportProto::has_meanreadiolatency() const {
  return (_has_bits_[0] & 0x00000004u) != 0;
}
inline void SlowDiskReportProto::set_has_meanreadiolatency() {
  _has_bits_[0] |= 0x00000004u;
}
inline void SlowDiskReportProto::clear_has_meanreadiolatency() {
  _has_bits_[0] &= ~0x00000004u;
}
inline void SlowDiskReportProto::clear_meanreadiolatency() {
  meanreadiolatency_ = 0;
  clear_has_meanreadiolatency();
}
inline double SlowDiskReportProto::meanreadiolatency() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowDiskReportProto.meanReadIoLatency)
  return meanreadiolatency_;
}
inline void SlowDiskReportProto::set_meanreadiolatency(double value) {
  set_has_meanreadiolatency();
  meanreadiolatency_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowDiskReportProto.meanReadIoLatency)
}

// optional double meanWriteIoLatency = 4;
inline bool SlowDiskReportProto::has_meanwriteiolatency() const {
  return (_has_bits_[0] & 0x00000008u) != 0;
}
inline void SlowDiskReportProto::set_has_meanwriteiolatency() {
  _has_bits_[0] |= 0x00000008u;
}
inline void SlowDiskReportProto::clear_has_meanwriteiolatency() {
  _has_bits_[0] &= ~0x00000008u;
}
inline void SlowDiskReportProto::clear_meanwriteiolatency() {
  meanwriteiolatency_ = 0;
  clear_has_meanwriteiolatency();
}
inline double SlowDiskReportProto::meanwriteiolatency() const {
  // @@protoc_insertion_point(field_get:hadoop.hdfs.datanode.SlowDiskReportProto.meanWriteIoLatency)
  return meanwriteiolatency_;
}
inline void SlowDiskReportProto::set_meanwriteiolatency(double value) {
  set_has_meanwriteiolatency();
  meanwriteiolatency_ = value;
  // @@protoc_insertion_point(field_set:hadoop.hdfs.datanode.SlowDiskReportProto.meanWriteIoLatency)
}

#ifdef __GNUC__
  #pragma GCC diagnostic pop
#endif  // __GNUC__
// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------

// -------------------------------------------------------------------


// @@protoc_insertion_point(namespace_scope)

}  // namespace datanode
}  // namespace hdfs
}  // namespace hadoop

namespace google {
namespace protobuf {

template <> struct is_proto_enum< ::hadoop::hdfs::datanode::DatanodeCommandProto_Type> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::hadoop::hdfs::datanode::DatanodeCommandProto_Type>() {
  return ::hadoop::hdfs::datanode::DatanodeCommandProto_Type_descriptor();
}
template <> struct is_proto_enum< ::hadoop::hdfs::datanode::BlockCommandProto_Action> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::hadoop::hdfs::datanode::BlockCommandProto_Action>() {
  return ::hadoop::hdfs::datanode::BlockCommandProto_Action_descriptor();
}
template <> struct is_proto_enum< ::hadoop::hdfs::datanode::BlockIdCommandProto_Action> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::hadoop::hdfs::datanode::BlockIdCommandProto_Action>() {
  return ::hadoop::hdfs::datanode::BlockIdCommandProto_Action_descriptor();
}
template <> struct is_proto_enum< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus>() {
  return ::hadoop::hdfs::datanode::ReceivedDeletedBlockInfoProto_BlockStatus_descriptor();
}
template <> struct is_proto_enum< ::hadoop::hdfs::datanode::ErrorReportRequestProto_ErrorCode> : ::std::true_type {};
template <>
inline const EnumDescriptor* GetEnumDescriptor< ::hadoop::hdfs::datanode::ErrorReportRequestProto_ErrorCode>() {
  return ::hadoop::hdfs::datanode::ErrorReportRequestProto_ErrorCode_descriptor();
}

}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)

#endif  // PROTOBUF_INCLUDED_DatanodeProtocol_2eproto
