
## 1. Stage 0 的整体目标（已经达成）

**定位：**

* 不管 HDFS 原生协议（NameNode/DataNode、hrpc、DataTransferProtocol）；
* 先用一套 **自研 Internal RPC 协议** 把整条链路打通：

  * `gwctl（测试客户端）`
  * ⬇
  * `InternalRpcServer + InternalGatewayServiceImpl`
  * ⬇
  * `CephFsAdapter + XattrMetadataStore`
  * ⬇
  * `CephFS 集群`

**效果：**

* 已经可以用简单命令：

  * `gwctl create /path`
  * `gwctl put local.bin /path`
  * `gwctl cat /path`
  * `gwctl ls /dir`
  * `gwctl rm /path`
* 实现了 **“HDFS 语义 → CephFS 文件 + xattr 元数据”** 的基本映射。

---

## 2. 协议层（Internal RPC）已实现内容

### 2.1 Proto 定义：`gateway_internal.proto`

完成的 message & service：

* **基础类型**

  * `RpcStatus`：统一返回码（`code` + `message`）
  * `FileHandle`：`file_id` + `path`
  * `BlockHandle`：`file_id` + `index` + `path`
  * `FileStatusProto`：路径、长度、block_size、replication、mode、时间戳等

* **RPC 接口**

  * `CreateFile(CreateFileRequest) → CreateFileResponse`
  * `GetFileInfo(GetFileInfoRequest) → GetFileInfoResponse`
  * `ListStatus(ListStatusRequest) → ListStatusResponse`
  * `Delete(DeleteRequest) → DeleteResponse`
  * `AllocateBlock(AllocateBlockRequest) → AllocateBlockResponse`
  * `GetBlockLocations(GetBlockLocationsRequest) → GetBlockLocationsResponse`
  * `WriteBlock(WriteBlockRequest) → WriteBlockResponse`
  * `ReadBlock(ReadBlockRequest) → ReadBlockResponse`
  * `Complete(CompleteRequest) → CompleteResponse`

语义上刻意贴 HDFS：

* `CreateFile` ~ HDFS `create`
* `AllocateBlock` ~ HDFS `addBlock`
* `GetBlockLocations` ~ HDFS `getBlockLocations`
* `WriteBlock/ReadBlock` ~ HDFS DataNode 上的 write/read 行为（简化版）
* `Complete` ~ HDFS `complete`

### 2.2 Internal RPC 线协议

自定义了一个简单二进制帧格式（用于 client/server 通信）：

* 每个请求/响应帧：

  ```text
  uint32 length   // 网络字节序，总长（method_id + reserved + payload）
  uint16 method   // MethodId 枚举值，网络字节序
  uint16 reserved // 保留字段，目前填 0
  bytes  payload  // 对应 Request / Response 的 protobuf 序列化
  ```

* `MethodId` 枚举包括：

  * `CREATE_FILE / GET_FILE_INFO / LIST_STATUS / DELETE_PATH / ALLOCATE_BLOCK / GET_BLOCK_LOCATIONS / WRITE_BLOCK / READ_BLOCK / COMPLETE`

**已实现组件：**

* `InternalRpcServer`

  * 负责监听 TCP（例如 `0.0.0.0:19000`），accept 连接；
  * 每个连接一个线程，循环：

    * 读取帧头 + payload；
    * 反序列化成对应 Request 类型；
    * 调用 `IInternalGatewayService` 的相应方法；
    * 序列化 Response，按同样帧格式写回。
* `InternalRpcClient`

  * 在 gwctl 中使用；
  * 封装了 `CreateFile / GetFileInfo / ... / Complete` 一组同步调用；
  * 内部实现与 Server 一致的 length-prefix + methodId + protobuf 协议。

---

## 3. 服务层：InternalGatewayServiceImpl（核心逻辑）

### 3.1 依赖注入

`InternalGatewayServiceImpl` 主要依赖两个组件：

* `CephFsAdapter`（对 CephFS 的封装）
* `IMetadataStore`（具体为 `XattrMetadataStore`）

构造时注入：

```cpp
InternalGatewayServiceImpl(std::shared_ptr<CephFsAdapter> ceph,
                           std::shared_ptr<IMetadataStore> meta_store);
```

### 3.2 FileMeta + XattrMetadataStore

**FileMeta 结构：**

* `block_size`：块大小（默认 128MB）
* `replication`：副本数（Stage 0 一直是 1）
* `length`：文件逻辑长度（字节）

**Xattr 存储格式：**

* key：`user.hdfs.meta`
* value 示例：`bs=134217728;rep=1;len=12345`

这样：

* `getfattr -n user.hdfs.meta ...` 能直接看 meta；
* `XattrMetadataStore` 提供：

  * `load_file_meta(path, meta)`：解析上述文本；
  * `save_file_meta(path, meta)`：格式化并写入 xattr。

### 3.3 已实现的 RPC 逻辑

**(1) CreateFile**

* 校验 path 必须是绝对路径；
* 递归创建父目录 `mkdir -p`；
* 通过 `CephFsAdapter::create(path, mode, overwrite=true)` 新建文件；
* 初始化 `FileMeta`：

  * `block_size` 来自请求（默认 128MB）；
  * `replication` 来自请求（默认 1）；
  * `length = 0`；
* 写入 xattr：`user.hdfs.meta`；
* 返回 `FileHandle{ file_id = hash(path), path = path }`。

**(2) GetFileInfo**

* 使用 `CephFsAdapter::stat(path, CephStat)` 获取：

  * 是否目录、size、mode、mtime/atime；
* 尝试 `meta_store_->load_file_meta(path, meta)`：

  * 如果没有 meta（比如普通 CephFS 文件），block_size/replication 使用默认值；
* 填充 `FileStatusProto` 返回。

**(3) ListStatus**

* `CephFsAdapter::readdir` 列出目录 entries；
* 对每个子项调用 `stat`，再尝试加载 meta；
* 生成 `entries[]`：

  * `path / is_dir / length / block_size / replication / mode / owner/group(hdfs)`。

**(4) DeletePath**

* `stat(path)` 判断是文件还是目录；
* 文件：

  * 调 `CephFsAdapter::unlink(path)` 删除；
* 目录：

  * 若 `recursive=true`，递归遍历子项，逐个 DeletePath；
  * 最后删除目录本身（目前示例用 unlink，后续改成 `rmdir` 更合理）。

**(5) AllocateBlock**

* 读取 `FileMeta`：

  * 若不存在，从 `stat` 初始化 `length` 等；
* 计算下一个 block index：

  * `index = meta.length / meta.block_size`；
* 保存 meta（暂不修改 length，写数据时更新）；
* 返回：

  * `BlockHandle{ file_id=hash(path), index=index, path=path }`；
  * `block_size`。

**(6) GetBlockLocations**

* 通过 `stat` 拿文件 size；
* 从 meta 中获取 `block_size`（未找到则使用默认）；
* 以 `block_size` 为单位切分整个文件：

  * 为每个 block 填 `BlockLocationProto`：

    * `block`: (file_id, index, path)
    * `offset`: 相对文件偏移
    * `length`: 本 block 有效数据长度
    * `datanodes`: Stage 0 固定为 `["127.0.0.1:50010"]`
* 目前不解析请求的 offset/length，视为“全文件”。

**(7) WriteBlock**

* 从 `BlockHandle` 中取 path + index；
* 从 meta 中取 `block_size`（没有就默认并 length=0）；
* 计算写入偏移：

  * `base_offset = index * block_size`
  * `file_offset = base_offset + offset_in_block`
* 使用 `CephFsAdapter::open` + `pwrite` 写入 CephFS 文件；
* 更新 `FileMeta.length`：

  * `length = max(length, file_offset + bytes_written)`，写回 xattr；
* 返回实际写入长度。

**(8) ReadBlock**

* 同样从 meta + index 算出 `file_offset`；
* 用 `CephFsAdapter::open` + `pread` 读取数据，放入 `ReadBlockResponse.data`；
* 不做额外校验（offset/length超界报错即可）。

**(9) Complete**

* 当前只记录日志，不做额外收尾动作（后续可以在此挂 Lease 完成、统计等）。

---

## 5. FS 层：CephFsAdapter 已覆盖的能力

在 Stage 0 中，CephFsAdapter 已经具备：

* 会话管理：

  * `init(CephFsConfig)`：`ceph_create` + `ceph_conf_set` + `ceph_mount`；
  * `shutdown()`：`ceph_unmount` + `ceph_release`；
* 文件操作：

  * `create(path, mode, overwrite)` → `ceph_open` + `O_CREAT/O_TRUNC` 等；
  * `open(path, flags, mode, out_fd)` / `close(fd)`；
  * `pread(fd, buf, len, offset)` / `pwrite(fd, buf, len, offset)`；
* 目录操作：

  * `mkdir(path, mode, recursive)`；
  * `readdir(path, std::vector<std::string>&)`;
  * （删除目录目前是 DeletePath 中用 `unlink`，后续可以在这里加 `rmdir`）。
* xattr 操作：

  * `get_xattr(path, name, value)`；
  * `set_xattr(path, name, value)`；
* stat：

  * `stat(path, CephStat&)`：基于 `ceph_stat/ceph_lstat` 取 size/type/mode/mtime/atime。

---

## 6. 工具层：gwctl 已支持的命令

**gwctl** 是 Stage 0 的测试和运维小工具，使用 `InternalRpcClient` 和同一套 proto：

* `gwctl create <path>`

  * 调 `CreateFile`；
* `gwctl stat <path>`

  * 调 `GetFileInfo`，打印 FileStatusProto；
* `gwctl ls <path>`

  * 调 `ListStatus`，打印目录内容；
* `gwctl rm <path>`

  * 调 `Delete`，非递归删除（可扩展 `-r`）；
* `gwctl put <local_file> <remote_path>`

  * 本地读文件 → 内存
  * `CreateFile` → `AllocateBlock`（单 block）
  * 循环小 chunk 调 `WriteBlock`（offset_in_block 累加）
  * 最后 `Complete`
* `gwctl cat <remote_path>`

  * `GetBlockLocations`（目前假设一个 block，但代码已支持多 block 循环）
  * 按 block / chunk 调 `ReadBlock`，写 stdout

现在已经可以做到：

```bash
./hdfs_ceph_gateway   # 启动 internal RPC + CephFS 会话
./gwctl put local.bin /user/test/a.bin
./gwctl cat /user/test/a.bin > out.bin
diff local.bin out.bin
./gwctl stat /user/test/a.bin
./gwctl ls /user/test
./gwctl rm /user/test/a.bin
```

---

## 7. Stage 0 的假设与限制

当前实现有一些**刻意简化**的地方，后续 Stage 1/2 会加强：

1. **不兼容真实 HDFS 协议**

   * 只支持 Internal RPC（自定义二进制协议），Hadoop 客户端暂时不能直接连。
2. **单节点 + 无副本**

   * replication 字段只存在 meta 中，不参与任何实际副本管理；
   * DataNode 概念还没真正引入，所有 IO 都直接打到 CephFS 上。
3. **无 lease / safemode / 恢复机制**

   * 只是简单地通过 `Complete` 标志写完，没有租约管理、超时回收。
4. **block 元数据简化**

   * 没有单独的 BlockMap 结构，直接用 `FileMeta.length` + `block_size` 推导 block 布局。
5. **错误和权限模型简化**

   * 统一用 `RpcStatus` + errno 风格 code；
   * owner/group 固定为 `"hdfs"`，无 ACL、无用户隔离。

---
